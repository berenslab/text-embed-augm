{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import gc\n",
    "\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from openTSNE import TSNE, affinity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import time\n",
    "import memory_profiler\n",
    "\n",
    "%load_ext memory_profiler\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1+cu111'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from text_embeddings_src.model_stuff import train_loop\n",
    "from text_embeddings_src.data_stuff import SentencePairDataset\n",
    "from text_embeddings_src.metrics import knn_accuracy\n",
    "from text_embeddings_src.embeddings import generate_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import black\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(line_length=79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_path = Path(\"../results/variables\")\n",
    "figures_path = Path(\"../results/figures\")\n",
    "data_path = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"matplotlib_style.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 183 ms, sys: 1.9 ms, total: 185 ms\n",
      "Wall time: 247 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "compression_opts = dict(method=\"zip\", archive_name=\"iclr.pickle.csv\")\n",
    "\n",
    "iclr = pd.read_pickle(\n",
    "    data_path / \"iclr.pickle.zip\",\n",
    "    # index_col=False,\n",
    "    compression=compression_opts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>decision</th>\n",
       "      <th>scores</th>\n",
       "      <th>keywords</th>\n",
       "      <th>gender-first</th>\n",
       "      <th>gender-last</th>\n",
       "      <th>t-SNE x</th>\n",
       "      <th>t-SNE y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>ryBnUWb0b</td>\n",
       "      <td>Predicting Floor-Level for 911 Calls with Neur...</td>\n",
       "      <td>In cities with tall buildings, emergency respo...</td>\n",
       "      <td>William Falcon, Henning Schulzrinne</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "      <td>[7, 6, 6]</td>\n",
       "      <td>[recurrent neural networks, rnn, lstm, mobile ...</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>2.536470</td>\n",
       "      <td>0.739367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>Skk3Jm96W</td>\n",
       "      <td>Some Considerations on Learning to Explore via...</td>\n",
       "      <td>We consider the problem of exploration in meta...</td>\n",
       "      <td>Bradly Stadie, Ge Yang, Rein Houthooft, Xi Che...</td>\n",
       "      <td>Invite to Workshop Track</td>\n",
       "      <td>[7, 4, 6]</td>\n",
       "      <td>[reinforcement learning, rl, exploration, meta...</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>49.831927</td>\n",
       "      <td>-29.813831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>r1RQdCg0W</td>\n",
       "      <td>MACH: Embarrassingly parallel $K$-class classi...</td>\n",
       "      <td>We present Merged-Averaged Classifiers via Has...</td>\n",
       "      <td>Qixuan Huang, Anshumali Shrivastava, Yiqiu Wang</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[6, 6, 6]</td>\n",
       "      <td>[extreme classification, large-scale learning,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-22.502752</td>\n",
       "      <td>9.577367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>rJ3fy0k0Z</td>\n",
       "      <td>Deterministic Policy Imitation Gradient Algorithm</td>\n",
       "      <td>The goal of imitation learning (IL) is to enab...</td>\n",
       "      <td>Fumihiro Sasaki, Atsuo Kawaguchi</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[6, 5, 5]</td>\n",
       "      <td>[imitation learning]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>40.437523</td>\n",
       "      <td>-47.690889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>SkBYYyZRZ</td>\n",
       "      <td>Searching for Activation Functions</td>\n",
       "      <td>The choice of activation functions in deep net...</td>\n",
       "      <td>Prajit Ramachandran, Barret Zoph, Quoc V. Le</td>\n",
       "      <td>Invite to Workshop Track</td>\n",
       "      <td>[5, 4, 7]</td>\n",
       "      <td>[meta learning, activation functions]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-33.260086</td>\n",
       "      <td>-4.038115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16531</th>\n",
       "      <td>2023</td>\n",
       "      <td>w4eQcMZsJa</td>\n",
       "      <td>Text-Driven Generative Domain Adaptation with ...</td>\n",
       "      <td>Combined with the generative prior of pre-trai...</td>\n",
       "      <td>Zhenhuan Liu, Liang Li, Jiayu Xiao, Zhengjun Z...</td>\n",
       "      <td>Desk rejected</td>\n",
       "      <td>[]</td>\n",
       "      <td>[gan, stylegan, clip, domain adaptation, style...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>59.296526</td>\n",
       "      <td>5.206691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16532</th>\n",
       "      <td>2023</td>\n",
       "      <td>SDHSQuBpf2</td>\n",
       "      <td>Laziness, Barren Plateau, and Noises in Machin...</td>\n",
       "      <td>We define \\emph{laziness} to describe a large ...</td>\n",
       "      <td>Zexi Lin, Liang Jiang</td>\n",
       "      <td>Desk rejected</td>\n",
       "      <td>[]</td>\n",
       "      <td>[theoretical issues in deep learning, learning...</td>\n",
       "      <td>None</td>\n",
       "      <td>male</td>\n",
       "      <td>-29.178083</td>\n",
       "      <td>-21.810583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16533</th>\n",
       "      <td>2023</td>\n",
       "      <td>HyIY8u5LVDr</td>\n",
       "      <td>Discovering the Representation Bottleneck of G...</td>\n",
       "      <td>Most graph neural networks (GNNs) rely on the ...</td>\n",
       "      <td>Fang Wu, Siyuan Li, Lirong Wu, Dragomir Radev,...</td>\n",
       "      <td>Desk rejected</td>\n",
       "      <td>[]</td>\n",
       "      <td>[gnn bottleneck, graph rewiring, representatio...</td>\n",
       "      <td>None</td>\n",
       "      <td>male</td>\n",
       "      <td>-7.573978</td>\n",
       "      <td>68.386671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16534</th>\n",
       "      <td>2023</td>\n",
       "      <td>470wZ5Qk4ur</td>\n",
       "      <td>Results for Perfect Classification for Graph A...</td>\n",
       "      <td>We study the ability of one layer Graph Attent...</td>\n",
       "      <td>Kimon Fountoulakis, Amit Levi</td>\n",
       "      <td>Desk rejected</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>male</td>\n",
       "      <td>-7.753593</td>\n",
       "      <td>60.764583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16535</th>\n",
       "      <td>2023</td>\n",
       "      <td>3GDft6lexE</td>\n",
       "      <td>Cooperate or Compete: A New Perspective on Tra...</td>\n",
       "      <td>GANs have two competing modules: the generator...</td>\n",
       "      <td>Sobhan Babu</td>\n",
       "      <td>Desk rejected</td>\n",
       "      <td>[]</td>\n",
       "      <td>[generative adversarial networks, nash equilib...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>38.470326</td>\n",
       "      <td>-1.929707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16536 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year           id                                              title  \\\n",
       "0      2018    ryBnUWb0b  Predicting Floor-Level for 911 Calls with Neur...   \n",
       "1      2018    Skk3Jm96W  Some Considerations on Learning to Explore via...   \n",
       "2      2018    r1RQdCg0W  MACH: Embarrassingly parallel $K$-class classi...   \n",
       "3      2018    rJ3fy0k0Z  Deterministic Policy Imitation Gradient Algorithm   \n",
       "4      2018    SkBYYyZRZ                 Searching for Activation Functions   \n",
       "...     ...          ...                                                ...   \n",
       "16531  2023   w4eQcMZsJa  Text-Driven Generative Domain Adaptation with ...   \n",
       "16532  2023   SDHSQuBpf2  Laziness, Barren Plateau, and Noises in Machin...   \n",
       "16533  2023  HyIY8u5LVDr  Discovering the Representation Bottleneck of G...   \n",
       "16534  2023  470wZ5Qk4ur  Results for Perfect Classification for Graph A...   \n",
       "16535  2023   3GDft6lexE  Cooperate or Compete: A New Perspective on Tra...   \n",
       "\n",
       "                                                abstract  \\\n",
       "0      In cities with tall buildings, emergency respo...   \n",
       "1      We consider the problem of exploration in meta...   \n",
       "2      We present Merged-Averaged Classifiers via Has...   \n",
       "3      The goal of imitation learning (IL) is to enab...   \n",
       "4      The choice of activation functions in deep net...   \n",
       "...                                                  ...   \n",
       "16531  Combined with the generative prior of pre-trai...   \n",
       "16532  We define \\emph{laziness} to describe a large ...   \n",
       "16533  Most graph neural networks (GNNs) rely on the ...   \n",
       "16534  We study the ability of one layer Graph Attent...   \n",
       "16535  GANs have two competing modules: the generator...   \n",
       "\n",
       "                                                 authors  \\\n",
       "0                    William Falcon, Henning Schulzrinne   \n",
       "1      Bradly Stadie, Ge Yang, Rein Houthooft, Xi Che...   \n",
       "2        Qixuan Huang, Anshumali Shrivastava, Yiqiu Wang   \n",
       "3                       Fumihiro Sasaki, Atsuo Kawaguchi   \n",
       "4           Prajit Ramachandran, Barret Zoph, Quoc V. Le   \n",
       "...                                                  ...   \n",
       "16531  Zhenhuan Liu, Liang Li, Jiayu Xiao, Zhengjun Z...   \n",
       "16532                              Zexi Lin, Liang Jiang   \n",
       "16533  Fang Wu, Siyuan Li, Lirong Wu, Dragomir Radev,...   \n",
       "16534                      Kimon Fountoulakis, Amit Levi   \n",
       "16535                                        Sobhan Babu   \n",
       "\n",
       "                       decision     scores  \\\n",
       "0               Accept (Poster)  [7, 6, 6]   \n",
       "1      Invite to Workshop Track  [7, 4, 6]   \n",
       "2                        Reject  [6, 6, 6]   \n",
       "3                        Reject  [6, 5, 5]   \n",
       "4      Invite to Workshop Track  [5, 4, 7]   \n",
       "...                         ...        ...   \n",
       "16531             Desk rejected         []   \n",
       "16532             Desk rejected         []   \n",
       "16533             Desk rejected         []   \n",
       "16534             Desk rejected         []   \n",
       "16535             Desk rejected         []   \n",
       "\n",
       "                                                keywords gender-first  \\\n",
       "0      [recurrent neural networks, rnn, lstm, mobile ...         male   \n",
       "1      [reinforcement learning, rl, exploration, meta...         male   \n",
       "2      [extreme classification, large-scale learning,...         None   \n",
       "3                                   [imitation learning]         None   \n",
       "4                  [meta learning, activation functions]         None   \n",
       "...                                                  ...          ...   \n",
       "16531  [gan, stylegan, clip, domain adaptation, style...         None   \n",
       "16532  [theoretical issues in deep learning, learning...         None   \n",
       "16533  [gnn bottleneck, graph rewiring, representatio...         None   \n",
       "16534                                                 []         None   \n",
       "16535  [generative adversarial networks, nash equilib...         None   \n",
       "\n",
       "      gender-last    t-SNE x    t-SNE y  \n",
       "0            None   2.536470   0.739367  \n",
       "1            male  49.831927 -29.813831  \n",
       "2            None -22.502752   9.577367  \n",
       "3            None  40.437523 -47.690889  \n",
       "4            None -33.260086  -4.038115  \n",
       "...           ...        ...        ...  \n",
       "16531        None  59.296526   5.206691  \n",
       "16532        male -29.178083 -21.810583  \n",
       "16533        male  -7.573978  68.386671  \n",
       "16534        male  -7.753593  60.764583  \n",
       "16535        None  38.470326  -1.929707  \n",
       "\n",
       "[16536 rows x 12 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iclr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_abstracts_together = [\n",
    "    iclr.title[i] + \" \" + iclr.abstract[i] for i in range(len(iclr))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16536\n"
     ]
    }
   ],
   "source": [
    "print(len(titles_abstracts_together))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16536\n",
      "8964\n",
      "6849\n"
     ]
    }
   ],
   "source": [
    "# iclr = pd.read_pickle(\"iclr.pickle.zip\")\n",
    "\n",
    "keywords = [\n",
    "    \"network\",\n",
    "    \"graph\",\n",
    "    \"reinforcement\",\n",
    "    \"language\",\n",
    "    \"adversarial\",\n",
    "    \"federated\",\n",
    "    \"contrastive\",\n",
    "    \"domain\",\n",
    "    \"diffusion\",\n",
    "    \"out-of-dis\",\n",
    "    \"continual\",\n",
    "    \"distillation\",\n",
    "    \"architecture\",\n",
    "    \"privacy\",\n",
    "    \"protein\",\n",
    "    \"fair\",\n",
    "    \"attention\",\n",
    "    \"video\",\n",
    "    \"meta-learning\",\n",
    "    \"generative adv\",\n",
    "    \"autoencoder\",\n",
    "    \"game\",\n",
    "    \"semi-sup\",\n",
    "    \"pruning\",\n",
    "    \"physics\",\n",
    "    \"3d\",\n",
    "    \"translation\",\n",
    "    \"optimization\",\n",
    "    \"recurrent\",\n",
    "    \"word\",\n",
    "    \"bayesian\",\n",
    "]\n",
    "keywords = np.array(keywords)\n",
    "\n",
    "y = np.zeros(iclr.shape[0]) * np.nan\n",
    "\n",
    "for num, keyword in enumerate(keywords):\n",
    "    mask = [keyword.lower() in t.lower() for t in iclr.title]\n",
    "    y[mask & ~np.isnan(y)] = -1\n",
    "    y[mask & np.isnan(y)] = num\n",
    "\n",
    "print(y.size)\n",
    "print(np.sum(~np.isnan(y)))\n",
    "print(np.sum(y >= 0))\n",
    "\n",
    "labeled = y >= 0\n",
    "\n",
    "iclr_labeled = iclr[labeled].reset_index(drop=True)\n",
    "y_labeled = y[labeled].astype(int)\n",
    "iclr_labeled[\"y\"] = y_labeled\n",
    "iclr_labeled[\"label\"] = keywords[y_labeled]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility\n",
    "Are the batches always the same if I fix the random seed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"BERT\",\n",
    "    \"SBERT\",\n",
    "    \"SPECTER\",\n",
    "    \"SciNCL\",\n",
    "]\n",
    "\n",
    "# rep = [\"av\", \"sep\", \"cls\"]\n",
    "\n",
    "model_paths = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"allenai/specter\",\n",
    "    \"malteos/scincl\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "i = 0\n",
    "\n",
    "# random_state = random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on device: {}\".format(device))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_paths[i])\n",
    "model = AutoModel.from_pretrained(model_paths[i])\n",
    "print(model_paths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m     \n",
      "\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext_pair\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext_pair_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtruncation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTruncationStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_overflowing_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_offsets_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           BertTokenizerFast\n",
      "\u001b[0;31mString form:\u001b[0m    BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fa <...> oken': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n",
      "\u001b[0;31mLength:\u001b[0m         30522\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.8/site-packages/transformers/models/bert/tokenization_bert_fast.py\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Construct a \"fast\" BERT tokenizer (backed by HuggingFace's *tokenizers* library). Based on WordPiece.\n",
      "\n",
      "This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n",
      "refer to this superclass for more information regarding those methods.\n",
      "\n",
      "Args:\n",
      "    vocab_file (`str`):\n",
      "        File containing the vocabulary.\n",
      "    do_lower_case (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to lowercase the input when tokenizing.\n",
      "    unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n",
      "        The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
      "        token instead.\n",
      "    sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n",
      "        The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n",
      "        sequence classification or for a text and a question for question answering. It is also used as the last\n",
      "        token of a sequence built with special tokens.\n",
      "    pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n",
      "        The token used for padding, for example when batching sequences of different lengths.\n",
      "    cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n",
      "        The classifier token which is used when doing sequence classification (classification of the whole sequence\n",
      "        instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
      "    mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n",
      "        The token used for masking values. This is the token used when training this model with masked language\n",
      "        modeling. This is the token which the model will try to predict.\n",
      "    clean_text (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to clean the text before tokenization by removing any control characters and replacing all\n",
      "        whitespaces by the classic one.\n",
      "    tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see [this\n",
      "        issue](https://github.com/huggingface/transformers/issues/328)).\n",
      "    strip_accents (`bool`, *optional*):\n",
      "        Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n",
      "        value for `lowercase` (as in the original BERT).\n",
      "    wordpieces_prefix (`str`, *optional*, defaults to `\"##\"`):\n",
      "        The prefix for subwords.\n",
      "\u001b[0;31mCall docstring:\u001b[0m\n",
      "Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
      "sequences.\n",
      "\n",
      "Args:\n",
      "    text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      "        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      "        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "    text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      "        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      "        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "    text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      "        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      "        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "    text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      "        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      "        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "\n",
      "    add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to encode the sequences with the special tokens relative to their model.\n",
      "    padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      "        Activates and controls padding. Accepts the following values:\n",
      "\n",
      "        - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      "          sequence if provided).\n",
      "        - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      "          acceptable input length for the model if that argument is not provided.\n",
      "        - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      "          lengths).\n",
      "    truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      "        Activates and controls truncation. Accepts the following values:\n",
      "\n",
      "        - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      "          to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      "          truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      "          sequences (or a batch of pairs) is provided.\n",
      "        - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "          truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "        - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "          truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "        - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      "          greater than the model maximum admissible input size).\n",
      "    max_length (`int`, *optional*):\n",
      "        Controls the maximum length to use by one of the truncation/padding parameters.\n",
      "\n",
      "        If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      "        is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      "        length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      "    stride (`int`, *optional*, defaults to 0):\n",
      "        If set to a number along with `max_length`, the overflowing tokens returned when\n",
      "        `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      "        returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      "        argument defines the number of overlapping tokens.\n",
      "    is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      "        tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      "        which it will tokenize. This is useful for NER or token classification.\n",
      "    pad_to_multiple_of (`int`, *optional*):\n",
      "        If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
      "        This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      "        `>= 7.5` (Volta).\n",
      "    return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      "        If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      "\n",
      "        - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      "        - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      "        - `'np'`: Return Numpy `np.ndarray` objects.\n",
      "\n",
      "    return_token_type_ids (`bool`, *optional*):\n",
      "        Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      "        the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      "\n",
      "        [What are token type IDs?](../glossary#token-type-ids)\n",
      "    return_attention_mask (`bool`, *optional*):\n",
      "        Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      "        to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      "\n",
      "        [What are attention masks?](../glossary#attention-mask)\n",
      "    return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      "        of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      "        of returning overflowing tokens.\n",
      "    return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return special tokens mask information.\n",
      "    return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return `(char_start, char_end)` for each token.\n",
      "\n",
      "        This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      "        Python's tokenizer, this method will raise `NotImplementedError`.\n",
      "    return_length  (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return the lengths of the encoded inputs.\n",
      "    verbose (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to print more information and warnings.\n",
      "    **kwargs: passed to the `self.tokenize()` method\n",
      "\n",
      "Return:\n",
      "    [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      "\n",
      "    - **input_ids** -- List of token ids to be fed to a model.\n",
      "\n",
      "      [What are input IDs?](../glossary#input-ids)\n",
      "\n",
      "    - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      "      if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      "\n",
      "      [What are token type IDs?](../glossary#token-type-ids)\n",
      "\n",
      "    - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      "      `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      "\n",
      "      [What are attention masks?](../glossary#attention-mask)\n",
      "\n",
      "    - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      "      `return_overflowing_tokens=True`).\n",
      "    - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      "      `return_overflowing_tokens=True`).\n",
      "    - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      "      regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      "    - **length** -- The length of the inputs (when `return_length=True`)\n"
     ]
    }
   ],
   "source": [
    "?tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "# Set the random seed for PyTorch (see https://pytorch.org/docs/stable/notes/randomness.html)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertTokenizerFast' object has no attribute 'set_seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/gpfs01/berens/user/rgonzalesmarquez/phd/text-embeddings/scripts/06-rgm-data-exploration.ipynb Cell 48\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgber7/gpfs01/berens/user/rgonzalesmarquez/phd/text-embeddings/scripts/06-rgm-data-exploration.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m training_dataset \u001b[39m=\u001b[39m SentencePairDataset(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgber7/gpfs01/berens/user/rgonzalesmarquez/phd/text-embeddings/scripts/06-rgm-data-exploration.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     iclr\u001b[39m.\u001b[39;49mabstract, tokenizer, device, seed\u001b[39m=\u001b[39;49mseed\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgber7/gpfs01/berens/user/rgonzalesmarquez/phd/text-embeddings/scripts/06-rgm-data-exploration.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m )\n",
      "File \u001b[0;32m~/phd/text-embeddings/text_embeddings_src/data_stuff.py:109\u001b[0m, in \u001b[0;36mSentencePairDataset.__init__\u001b[0;34m(self, abstracts, tokenizer, device, tokenizer_kwargs, seed)\u001b[0m\n\u001b[1;32m     98\u001b[0m     tokenizer_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m     99\u001b[0m         max_length\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m,\n\u001b[1;32m    100\u001b[0m         padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    101\u001b[0m         truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    102\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    103\u001b[0m     )\n\u001b[1;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentences_tok \u001b[39m=\u001b[39m tokenizer(\n\u001b[1;32m    106\u001b[0m     \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentences_map\u001b[39m.\u001b[39mkeys()), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenizer_kwargs\n\u001b[1;32m    107\u001b[0m )\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 109\u001b[0m tokenizer\u001b[39m.\u001b[39;49mset_seed(seed)\n\u001b[1;32m    111\u001b[0m \u001b[39m# we group the flat sentences by the original abstract they\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m# come from.  Then we can check whether we have enough\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m# sentences and append the abstracts with at least two\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39m# sentences to our list.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m sentences_and_toks \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\n\u001b[1;32m    116\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentences_map\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m    117\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentences_tok[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentences_tok[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    119\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertTokenizerFast' object has no attribute 'set_seed'"
     ]
    }
   ],
   "source": [
    "training_dataset = SentencePairDataset(\n",
    "    iclr.abstract, tokenizer, device, seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(\n",
    "    training_dataset, batch_size=128, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d6fa37efa34dc1b8608c87ad0a672d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "loop = tqdm(training_loader, leave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([  101,  2007,  2023,  6614,  1010,  1037,  2028,  1011,  2915,  3565,\n",
      "         7159,  2003,  2788, 21155,  2094,  2004,  1037,  2836,  9345,  7630,\n",
      "         8844,  2000,  4635,  1996,  2836,  1032, 23277,  2102,  1066,  2367,\n",
      "         9381,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i_batch, batch in enumerate(training_loader):\n",
    "    print(len(batch[0]))\n",
    "    print(batch[0][0][0])\n",
    "    break\n",
    "    # prepare batches and more all to the active device\n",
    "    anchor_ids = batch[0][0].to(device)\n",
    "    anchor_mask = batch[0][1].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_2 = SentencePairDataset(\n",
    "    iclr.abstract,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader_2 = torch.utils.data.DataLoader(\n",
    "    training_dataset_2, batch_size=128, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d9d70adae541ac954f59fa6dc9d8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "loop_2 = tqdm(training_loader_2, leave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([  101,  4866,  7885,  2006,  3746,  7159,  1010, 25022, 14971,  1011,\n",
      "         2184,  1010,  8292,  2571,  3676,  1998,  5796, 25033,  2951, 13462,\n",
      "         2031, 20119,  2256, 19113, 13599,  2000,  2060,  2110,  1011,  1997,\n",
      "         1011,  1996,  1011,  2396, 26163,  2015,  1012,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i_batch, batch in enumerate(loop_2):\n",
    "    print(len(batch[0]))\n",
    "    print(batch[0][0][0])\n",
    "    break\n",
    "    # prepare batches and more all to the active device\n",
    "    anchor_ids = batch[0][0].to(device)\n",
    "    anchor_mask = batch[0][1].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"BERT\",\n",
    "    \"SBERT\",\n",
    "    \"SPECTER\",\n",
    "    \"SciNCL\",\n",
    "]\n",
    "\n",
    "# rep = [\"av\", \"sep\", \"cls\"]\n",
    "\n",
    "model_paths = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"allenai/specter\",\n",
    "    \"malteos/scincl\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "i = 0\n",
    "\n",
    "random_state = random.seed(42)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Running on device: {}\".format(device))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_paths[i])\n",
    "model = AutoModel.from_pretrained(model_paths[i])\n",
    "print(model_paths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = SentencePairDataset(\n",
    "    iclr.abstract, tokenizer, device, seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(\n",
    "    training_dataset, batch_size=128, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([  101,  2007,  2023,  6614,  1010,  1037,  2028,  1011,  2915,  3565,\n",
      "         7159,  2003,  2788, 21155,  2094,  2004,  1037,  2836,  9345,  7630,\n",
      "         8844,  2000,  4635,  1996,  2836,  1032, 23277,  2102,  1066,  2367,\n",
      "         9381,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "for i_batch, batch in enumerate(training_loader):\n",
    "    print(len(batch[0]))\n",
    "    print(batch[0][0][0])\n",
    "    break\n",
    "    # prepare batches and more all to the active device\n",
    "    anchor_ids = batch[0][0].to(device)\n",
    "    anchor_mask = batch[0][1].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_2 = SentencePairDataset(\n",
    "    iclr.abstract,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed)\n",
    "\n",
    "training_dataset_2 = SentencePairDataset(\n",
    "    iclr.abstract,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = torch.Generator(device)\n",
    "gen.manual_seed(seed)\n",
    "training_loader_2 = torch.utils.data.DataLoader(\n",
    "    training_dataset_2, batch_size=128, shuffle=True, generator=gen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([  101,  1999,  2023,  3259,  1010,  2057,  2817,  1996,  3291,  1997,\n",
      "         4531,  3816, 10594, 14442,  2005,  2812,  1011,  2492,  2048,  1011,\n",
      "         2447,  5717,  1011,  7680,  2399,  1012,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "for i_batch, batch in enumerate(training_loader_2):\n",
    "    print(len(batch[0]))\n",
    "    print(batch[0][0][0])\n",
    "    break\n",
    "    # prepare batches and more all to the active device\n",
    "    anchor_ids = batch[0][0].to(device)\n",
    "    anchor_mask = batch[0][1].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([  101,  1999,  3655,  2007,  4206,  3121,  1010,  5057,  6869,  2545,\n",
       "           2342,  2019,  8321,  2723,  2504,  3295,  2000,  2424, 19989, 20587,\n",
       "           2015,  2855,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])),\n",
       " (tensor([  101,  4406, 17727, 22648, 14656,  3025,  8107,  1010,  2256,  2291,\n",
       "           2003,  1996,  2034,  2008,  2515,  2025,  5478,  1996,  2224,  1997,\n",
       "          14400,  2015,  1010,  3188,  3716,  1997,  1996,  2311,  6502,  1010,\n",
       "           2030,  3716,  1997,  5310,  5248,  1012,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset_3 = SentencePairDataset(\n",
    "    iclr.abstract,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    seed=seed,\n",
    ")\n",
    "training_dataset_3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9923,  566])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(42)\n",
    "np.random.choice(len(iclr.abstract), size=2, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5585, 6675])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(len(iclr.abstract), size=2, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New: This way we ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"BERT\",\n",
    "    \"SBERT\",\n",
    "    \"SPECTER\",\n",
    "    \"SciNCL\",\n",
    "]\n",
    "\n",
    "# rep = [\"av\", \"sep\", \"cls\"]\n",
    "\n",
    "model_paths = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"allenai/specter\",\n",
    "    \"malteos/scincl\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set the random seed\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "i = 0\n",
    "\n",
    "# random_state = random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on device: {}\".format(device))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_paths[i])\n",
    "model = AutoModel.from_pretrained(model_paths[i])\n",
    "print(model_paths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_4 = SentencePairDataset(\n",
    "    iclr.abstract,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = torch.Generator()\n",
    "gen.manual_seed(seed)\n",
    "training_loader_4 = torch.utils.data.DataLoader(\n",
    "    training_dataset_4, batch_size=128, shuffle=True, generator=gen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([  101,  2004,  7976,  4454,  2004,  1037,  2326, 12154,  6217,  1010,\n",
      "         8650,  2092,  1011,  4738,  4275,  2004,  7789,  3200,  2003,  3352,\n",
      "         6233,  2590,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i_batch, batch in enumerate(training_loader_4):\n",
    "    print(len(batch[0]))\n",
    "    print(batch[0][0][0])\n",
    "    break\n",
    "    # prepare batches and more all to the active device\n",
    "    anchor_ids = batch[0][0].to(device)\n",
    "    anchor_mask = batch[0][1].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([  101,  7885,  2006,  3115, 10629,  5579,  2951, 13462,  2015, 10580,\n",
      "         1996, 12353,  1997,  1996,  3818,  3921,  1999,  7831,  2007,  2110,\n",
      "         1011,  1997,  1011,  1996,  1011,  2396,  4725,  1012,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i_batch, batch in enumerate(training_loader_4):\n",
    "    print(len(batch[0]))\n",
    "    print(batch[0][0][0])\n",
    "    break\n",
    "    # prepare batches and more all to the active device\n",
    "    anchor_ids = batch[0][0].to(device)\n",
    "    anchor_mask = batch[0][1].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
