{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import gc\n",
    "\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from openTSNE import TSNE, affinity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import time\n",
    "import memory_profiler\n",
    "\n",
    "%load_ext memory_profiler\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1+cu111'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from text_embeddings_src.model_stuff import train_loop\n",
    "from text_embeddings_src.data_stuff import (\n",
    "    SentencePairDataset,\n",
    "    MultSentencesPairDataset,\n",
    "    MultOverlappingSentencesPairDataset,\n",
    "    MultOverlappingSentencesLabelPairDataset,\n",
    ")\n",
    "from text_embeddings_src.metrics import knn_accuracy\n",
    "from text_embeddings_src.embeddings import generate_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import black\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(line_length=79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_path = Path(\"../results/variables\")\n",
    "figures_path = Path(\"../results/figures\")\n",
    "data_path = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"matplotlib_style.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 186 ms, sys: 27.7 ms, total: 214 ms\n",
      "Wall time: 295 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "compression_opts = dict(method=\"zip\", archive_name=\"iclr.pickle.csv\")\n",
    "\n",
    "iclr = pd.read_pickle(\n",
    "    data_path / \"iclr.pickle.zip\",\n",
    "    # index_col=False,\n",
    "    compression=compression_opts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>decision</th>\n",
       "      <th>scores</th>\n",
       "      <th>keywords</th>\n",
       "      <th>gender-first</th>\n",
       "      <th>gender-last</th>\n",
       "      <th>t-SNE x</th>\n",
       "      <th>t-SNE y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>ryBnUWb0b</td>\n",
       "      <td>Predicting Floor-Level for 911 Calls with Neur...</td>\n",
       "      <td>In cities with tall buildings, emergency respo...</td>\n",
       "      <td>William Falcon, Henning Schulzrinne</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "      <td>[7, 6, 6]</td>\n",
       "      <td>[recurrent neural networks, rnn, lstm, mobile ...</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>2.536470</td>\n",
       "      <td>0.739367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>Skk3Jm96W</td>\n",
       "      <td>Some Considerations on Learning to Explore via...</td>\n",
       "      <td>We consider the problem of exploration in meta...</td>\n",
       "      <td>Bradly Stadie, Ge Yang, Rein Houthooft, Xi Che...</td>\n",
       "      <td>Invite to Workshop Track</td>\n",
       "      <td>[7, 4, 6]</td>\n",
       "      <td>[reinforcement learning, rl, exploration, meta...</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>49.831927</td>\n",
       "      <td>-29.813831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>r1RQdCg0W</td>\n",
       "      <td>MACH: Embarrassingly parallel $K$-class classi...</td>\n",
       "      <td>We present Merged-Averaged Classifiers via Has...</td>\n",
       "      <td>Qixuan Huang, Anshumali Shrivastava, Yiqiu Wang</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[6, 6, 6]</td>\n",
       "      <td>[extreme classification, large-scale learning,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-22.502752</td>\n",
       "      <td>9.577367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>rJ3fy0k0Z</td>\n",
       "      <td>Deterministic Policy Imitation Gradient Algorithm</td>\n",
       "      <td>The goal of imitation learning (IL) is to enab...</td>\n",
       "      <td>Fumihiro Sasaki, Atsuo Kawaguchi</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[6, 5, 5]</td>\n",
       "      <td>[imitation learning]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>40.437523</td>\n",
       "      <td>-47.690889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>SkBYYyZRZ</td>\n",
       "      <td>Searching for Activation Functions</td>\n",
       "      <td>The choice of activation functions in deep net...</td>\n",
       "      <td>Prajit Ramachandran, Barret Zoph, Quoc V. Le</td>\n",
       "      <td>Invite to Workshop Track</td>\n",
       "      <td>[5, 4, 7]</td>\n",
       "      <td>[meta learning, activation functions]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-33.260086</td>\n",
       "      <td>-4.038115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16531</th>\n",
       "      <td>2023</td>\n",
       "      <td>w4eQcMZsJa</td>\n",
       "      <td>Text-Driven Generative Domain Adaptation with ...</td>\n",
       "      <td>Combined with the generative prior of pre-trai...</td>\n",
       "      <td>Zhenhuan Liu, Liang Li, Jiayu Xiao, Zhengjun Z...</td>\n",
       "      <td>Desk rejected</td>\n",
       "      <td>[]</td>\n",
       "      <td>[gan, stylegan, clip, domain adaptation, style...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>59.296526</td>\n",
       "      <td>5.206691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16532</th>\n",
       "      <td>2023</td>\n",
       "      <td>SDHSQuBpf2</td>\n",
       "      <td>Laziness, Barren Plateau, and Noises in Machin...</td>\n",
       "      <td>We define \\emph{laziness} to describe a large ...</td>\n",
       "      <td>Zexi Lin, Liang Jiang</td>\n",
       "      <td>Desk rejected</td>\n",
       "      <td>[]</td>\n",
       "      <td>[theoretical issues in deep learning, learning...</td>\n",
       "      <td>None</td>\n",
       "      <td>male</td>\n",
       "      <td>-29.178083</td>\n",
       "      <td>-21.810583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16533</th>\n",
       "      <td>2023</td>\n",
       "      <td>HyIY8u5LVDr</td>\n",
       "      <td>Discovering the Representation Bottleneck of G...</td>\n",
       "      <td>Most graph neural networks (GNNs) rely on the ...</td>\n",
       "      <td>Fang Wu, Siyuan Li, Lirong Wu, Dragomir Radev,...</td>\n",
       "      <td>Desk rejected</td>\n",
       "      <td>[]</td>\n",
       "      <td>[gnn bottleneck, graph rewiring, representatio...</td>\n",
       "      <td>None</td>\n",
       "      <td>male</td>\n",
       "      <td>-7.573978</td>\n",
       "      <td>68.386671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16534</th>\n",
       "      <td>2023</td>\n",
       "      <td>470wZ5Qk4ur</td>\n",
       "      <td>Results for Perfect Classification for Graph A...</td>\n",
       "      <td>We study the ability of one layer Graph Attent...</td>\n",
       "      <td>Kimon Fountoulakis, Amit Levi</td>\n",
       "      <td>Desk rejected</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>male</td>\n",
       "      <td>-7.753593</td>\n",
       "      <td>60.764583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16535</th>\n",
       "      <td>2023</td>\n",
       "      <td>3GDft6lexE</td>\n",
       "      <td>Cooperate or Compete: A New Perspective on Tra...</td>\n",
       "      <td>GANs have two competing modules: the generator...</td>\n",
       "      <td>Sobhan Babu</td>\n",
       "      <td>Desk rejected</td>\n",
       "      <td>[]</td>\n",
       "      <td>[generative adversarial networks, nash equilib...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>38.470326</td>\n",
       "      <td>-1.929707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16536 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year           id                                              title  \\\n",
       "0      2018    ryBnUWb0b  Predicting Floor-Level for 911 Calls with Neur...   \n",
       "1      2018    Skk3Jm96W  Some Considerations on Learning to Explore via...   \n",
       "2      2018    r1RQdCg0W  MACH: Embarrassingly parallel $K$-class classi...   \n",
       "3      2018    rJ3fy0k0Z  Deterministic Policy Imitation Gradient Algorithm   \n",
       "4      2018    SkBYYyZRZ                 Searching for Activation Functions   \n",
       "...     ...          ...                                                ...   \n",
       "16531  2023   w4eQcMZsJa  Text-Driven Generative Domain Adaptation with ...   \n",
       "16532  2023   SDHSQuBpf2  Laziness, Barren Plateau, and Noises in Machin...   \n",
       "16533  2023  HyIY8u5LVDr  Discovering the Representation Bottleneck of G...   \n",
       "16534  2023  470wZ5Qk4ur  Results for Perfect Classification for Graph A...   \n",
       "16535  2023   3GDft6lexE  Cooperate or Compete: A New Perspective on Tra...   \n",
       "\n",
       "                                                abstract  \\\n",
       "0      In cities with tall buildings, emergency respo...   \n",
       "1      We consider the problem of exploration in meta...   \n",
       "2      We present Merged-Averaged Classifiers via Has...   \n",
       "3      The goal of imitation learning (IL) is to enab...   \n",
       "4      The choice of activation functions in deep net...   \n",
       "...                                                  ...   \n",
       "16531  Combined with the generative prior of pre-trai...   \n",
       "16532  We define \\emph{laziness} to describe a large ...   \n",
       "16533  Most graph neural networks (GNNs) rely on the ...   \n",
       "16534  We study the ability of one layer Graph Attent...   \n",
       "16535  GANs have two competing modules: the generator...   \n",
       "\n",
       "                                                 authors  \\\n",
       "0                    William Falcon, Henning Schulzrinne   \n",
       "1      Bradly Stadie, Ge Yang, Rein Houthooft, Xi Che...   \n",
       "2        Qixuan Huang, Anshumali Shrivastava, Yiqiu Wang   \n",
       "3                       Fumihiro Sasaki, Atsuo Kawaguchi   \n",
       "4           Prajit Ramachandran, Barret Zoph, Quoc V. Le   \n",
       "...                                                  ...   \n",
       "16531  Zhenhuan Liu, Liang Li, Jiayu Xiao, Zhengjun Z...   \n",
       "16532                              Zexi Lin, Liang Jiang   \n",
       "16533  Fang Wu, Siyuan Li, Lirong Wu, Dragomir Radev,...   \n",
       "16534                      Kimon Fountoulakis, Amit Levi   \n",
       "16535                                        Sobhan Babu   \n",
       "\n",
       "                       decision     scores  \\\n",
       "0               Accept (Poster)  [7, 6, 6]   \n",
       "1      Invite to Workshop Track  [7, 4, 6]   \n",
       "2                        Reject  [6, 6, 6]   \n",
       "3                        Reject  [6, 5, 5]   \n",
       "4      Invite to Workshop Track  [5, 4, 7]   \n",
       "...                         ...        ...   \n",
       "16531             Desk rejected         []   \n",
       "16532             Desk rejected         []   \n",
       "16533             Desk rejected         []   \n",
       "16534             Desk rejected         []   \n",
       "16535             Desk rejected         []   \n",
       "\n",
       "                                                keywords gender-first  \\\n",
       "0      [recurrent neural networks, rnn, lstm, mobile ...         male   \n",
       "1      [reinforcement learning, rl, exploration, meta...         male   \n",
       "2      [extreme classification, large-scale learning,...         None   \n",
       "3                                   [imitation learning]         None   \n",
       "4                  [meta learning, activation functions]         None   \n",
       "...                                                  ...          ...   \n",
       "16531  [gan, stylegan, clip, domain adaptation, style...         None   \n",
       "16532  [theoretical issues in deep learning, learning...         None   \n",
       "16533  [gnn bottleneck, graph rewiring, representatio...         None   \n",
       "16534                                                 []         None   \n",
       "16535  [generative adversarial networks, nash equilib...         None   \n",
       "\n",
       "      gender-last    t-SNE x    t-SNE y  \n",
       "0            None   2.536470   0.739367  \n",
       "1            male  49.831927 -29.813831  \n",
       "2            None -22.502752   9.577367  \n",
       "3            None  40.437523 -47.690889  \n",
       "4            None -33.260086  -4.038115  \n",
       "...           ...        ...        ...  \n",
       "16531        None  59.296526   5.206691  \n",
       "16532        male -29.178083 -21.810583  \n",
       "16533        male  -7.573978  68.386671  \n",
       "16534        male  -7.753593  60.764583  \n",
       "16535        None  38.470326  -1.929707  \n",
       "\n",
       "[16536 rows x 12 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iclr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_abstracts_together = [\n",
    "    iclr.title[i] + \" \" + iclr.abstract[i] for i in range(len(iclr))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16536\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(titles_abstracts_together))\n",
    "print(type(titles_abstracts_together))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16536\n",
      "8964\n",
      "6849\n"
     ]
    }
   ],
   "source": [
    "# iclr = pd.read_pickle(\"iclr.pickle.zip\")\n",
    "\n",
    "keywords = [\n",
    "    \"network\",\n",
    "    \"graph\",\n",
    "    \"reinforcement\",\n",
    "    \"language\",\n",
    "    \"adversarial\",\n",
    "    \"federated\",\n",
    "    \"contrastive\",\n",
    "    \"domain\",\n",
    "    \"diffusion\",\n",
    "    \"out-of-dis\",\n",
    "    \"continual\",\n",
    "    \"distillation\",\n",
    "    \"architecture\",\n",
    "    \"privacy\",\n",
    "    \"protein\",\n",
    "    \"fair\",\n",
    "    \"attention\",\n",
    "    \"video\",\n",
    "    \"meta-learning\",\n",
    "    \"generative adv\",\n",
    "    \"autoencoder\",\n",
    "    \"game\",\n",
    "    \"semi-sup\",\n",
    "    \"pruning\",\n",
    "    \"physics\",\n",
    "    \"3d\",\n",
    "    \"translation\",\n",
    "    \"optimization\",\n",
    "    \"recurrent\",\n",
    "    \"word\",\n",
    "    \"bayesian\",\n",
    "]\n",
    "keywords = np.array(keywords)\n",
    "\n",
    "y = np.zeros(iclr.shape[0]) * np.nan\n",
    "\n",
    "for num, keyword in enumerate(keywords):\n",
    "    mask = [keyword.lower() in t.lower() for t in iclr.title]\n",
    "    y[mask & ~np.isnan(y)] = -1\n",
    "    y[mask & np.isnan(y)] = num\n",
    "\n",
    "print(y.size)\n",
    "print(np.sum(~np.isnan(y)))\n",
    "print(np.sum(y >= 0))\n",
    "\n",
    "labeled = y >= 0\n",
    "\n",
    "iclr_labeled = iclr[labeled].reset_index(drop=True)\n",
    "y_labeled = y[labeled].astype(int)\n",
    "iclr_labeled[\"y\"] = y_labeled\n",
    "iclr_labeled[\"label\"] = keywords[y_labeled]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"BERT\",\n",
    "    \"MPNet\",\n",
    "    \"SBERT\",\n",
    "    \"SciBERT\",\n",
    "    \"SPECTER\",\n",
    "    \"SciNCL\",\n",
    "]\n",
    "\n",
    "\n",
    "model_paths = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"microsoft/mpnet-base\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    \"allenai/specter\",\n",
    "    \"malteos/scincl\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- do t-SNE using the representations of ater 1 epoch and after 10 epochs\n",
    "- kNN recall in which the \"original high-dim\" representation is the space trained after 10 epochs and the \"low-dim\" one is after one epoch\n",
    "- \"abstract-accuracy\": use as dataset now instead of abtsracts, sentences of abstracts and as labels the abstract PMID. train + test sets!\n",
    "- Only train the model on the train set and evaluate after each epoch the test set by computing the loss.  train + test sets!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First two points together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tsne_simple(Z, k=10, rs=42, verbose=False):\n",
    "    A = affinity.Uniform(\n",
    "        Z,\n",
    "        verbose=verbose,\n",
    "        method=\"exact\",\n",
    "        random_state=rs,\n",
    "        k_neighbors=k,\n",
    "    )\n",
    "\n",
    "    X = TSNE(\n",
    "        verbose=True, initialization=\"spectral\", random_state=42\n",
    "    ).fit(affinities=A)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_tsne_and_knn_rec(model, loader, device, titles_abstracts_together, tokenizer, optimized_rep= \"av\", n_epochs=1, lr=2e-5):\n",
    "\n",
    "    assert optimized_rep in [\"av\", \"cls\", \"sep\", \"7th\"], \"Not valid `optimized_rep`. Choose from ['av', 'cls', 'sep', '7th'].\"\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # define layers to be used in multiple-negatives-ranking\n",
    "    cos_sim = torch.nn.CosineSimilarity()\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    scale = 20.0  # we multiply similarity score by this scale value\n",
    "    # move layers to device\n",
    "    cos_sim.to(device)\n",
    "    loss_func.to(device)\n",
    "\n",
    "    # initialize Adam optimizer\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # setup warmup for first ~10% of steps\n",
    "    total_steps = len(loader) * n_epochs \n",
    "    warmup_steps = int(0.1 * len(loader))\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optim,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps - warmup_steps,\n",
    "    )\n",
    "\n",
    "    losses = np.empty((n_epochs, len(loader)))\n",
    "    high_d_reps= []\n",
    "    tsne_embeddings= []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()  # make sure model is in training mode\n",
    "        # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
    "        loop = tqdm(loader, leave=True)\n",
    "        for i_batch, batch in enumerate(loop):\n",
    "            # zero all gradients on each new step\n",
    "            optim.zero_grad()\n",
    "            # prepare batches and more all to the active device\n",
    "            anchor_ids = batch[0][0].to(device)     \n",
    "            anchor_mask = batch[0][1].to(device)\n",
    "            pos_ids = batch[1][0].to(device)\n",
    "            pos_mask = batch[1][1].to(device)\n",
    "            # extract token embeddings from BERT\n",
    "            a = model(anchor_ids, attention_mask=anchor_mask)[0]  # all token embeddings\n",
    "            p = model(pos_ids, attention_mask=pos_mask)[0]\n",
    "            \n",
    "            # get the mean pooled vectors\n",
    "            if optimized_rep == \"av\":\n",
    "                a = mean_pool(a, anchor_mask)\n",
    "                p = mean_pool(p, pos_mask)\n",
    "                \n",
    "            elif optimized_rep == \"cls\":\n",
    "                a = cls_pool(a, anchor_mask)\n",
    "                p = cls_pool(p, pos_mask)\n",
    "                \n",
    "            elif optimized_rep == \"sep\":\n",
    "                a = sep_pool(a, anchor_mask)\n",
    "                p = sep_pool(p, pos_mask)\n",
    "                \n",
    "            elif optimized_rep == \"7th\":\n",
    "                a = seventh_pool(a, anchor_mask)\n",
    "                p = seventh_pool(p, pos_mask)\n",
    "                \n",
    "            # calculate the cosine similarities\n",
    "            scores = torch.stack(\n",
    "                [cos_sim(a_i.reshape(1, a_i.shape[0]), p) for a_i in a]\n",
    "            )\n",
    "            # get label(s) - we could define this before if confident\n",
    "            # of consistent batch sizes\n",
    "            labels = torch.tensor(\n",
    "                range(len(scores)), dtype=torch.long, device=scores.device\n",
    "            )\n",
    "            # and now calculate the loss\n",
    "            loss = loss_func(scores * scale, labels)   # Nik does not know what the labels nor the scale are\n",
    "            losses[epoch, i_batch] = loss.item()\n",
    "\n",
    "            # using loss, calculate gradients and then optimize\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            # update learning rate scheduler\n",
    "            scheduler.step()\n",
    "            # update the TDQM progress bar\n",
    "            loop.set_description(f\"Epoch {epoch}\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "        ## get high-dim and low-dim representations\n",
    "        if (epoch == 0) | (epoch == n_epochs-1):\n",
    "\n",
    "            embedding_cls, embedding_sep, embedding_av = generate_embeddings(\n",
    "                titles_abstracts_together, tokenizer, model, device, batch_size=256, return_seventh=False\n",
    "            )\n",
    "\n",
    "            if optimized_rep == \"av\":\n",
    "                high_d_reps.append(embedding_av)\n",
    "                tsne_result = run_tsne_simple(embedding_av)\n",
    "                tsne_embeddings.append(tsne_result)\n",
    "                \n",
    "            elif optimized_rep == \"cls\":\n",
    "                high_d_reps.append(embedding_cls)\n",
    "                tsne_result = run_tsne_simple(embedding_cls)\n",
    "                tsne_embeddings.append(tsne_result)\n",
    "                \n",
    "            elif optimized_rep == \"sep\":\n",
    "                high_d_reps.append(embedding_sep)\n",
    "                tsne_result = run_tsne_simple(embedding_sep)\n",
    "                tsne_embeddings.append(tsne_result)\n",
    "\n",
    "\n",
    "    # knn recall\n",
    "    knn_recall_result = knn_recall(high_d_reps[1], [high_d_reps[0]])\n",
    "        \n",
    "    return losses, tsne_embeddings, knn_recall_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second two points together\n",
    "- Only train the model on the train set and evaluate after each epoch the test set by computing the loss.  \n",
    "- \"abstract-accuracy\": use as dataset now instead of abtsracts, sentences of abstracts and as labels the abstract PMID. \n",
    "train + test sets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_eval_test_loss(model, train_loader, test_loader, device, tokenizer, train_dataset, test_dataset, optimized_rep= \"av\", n_epochs=1, lr=2e-5):\n",
    "    \n",
    "    assert optimized_rep in [\"av\", \"cls\", \"sep\", \"7th\"], \"Not valid `optimized_rep`. Choose from ['av', 'cls', 'sep', '7th'].\"\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # define layers to be used in multiple-negatives-ranking\n",
    "    cos_sim = torch.nn.CosineSimilarity()\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    scale = 20.0  # we multiply similarity score by this scale value\n",
    "    # move layers to device\n",
    "    cos_sim.to(device)\n",
    "    loss_func.to(device)\n",
    "\n",
    "    # initialize Adam optimizer\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # setup warmup for first ~10% of steps\n",
    "    total_steps = len(train_loader) * n_epochs \n",
    "    warmup_steps = int(0.1 * len(train_loader))\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optim,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps - warmup_steps,\n",
    "    )\n",
    "\n",
    "    losses_train = np.empty((n_epochs, len(train_loader)))\n",
    "    losses_test = np.empty((n_epochs, len(test_loader)))\n",
    "    knn_accuracies_train =  np.empty((n_epochs, 3))\n",
    "    knn_accuracies_test =  np.empty((n_epochs, 3))\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()  # make sure model is in training mode\n",
    "\n",
    "        # TRAIN LOADER -- training model with train loader\n",
    "        # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
    "        train_loop = tqdm(train_loader, leave=True)\n",
    "        for i_batch, batch in enumerate(train_loop):\n",
    "            # zero all gradients on each new step\n",
    "            optim.zero_grad()\n",
    "\n",
    "            # prepare batches and move all to the active device\n",
    "            anchor_ids = batch[0][0].to(device)     \n",
    "            anchor_mask = batch[0][1].to(device)\n",
    "            pos_ids = batch[1][0].to(device)\n",
    "            pos_mask = batch[1][1].to(device)\n",
    "            # extract token embeddings from model\n",
    "            a = model(anchor_ids, attention_mask=anchor_mask)[0]  # all token embeddings\n",
    "            p = model(pos_ids, attention_mask=pos_mask)[0]\n",
    "            \n",
    "            # get the pooled vectors\n",
    "            if optimized_rep == \"av\":\n",
    "                a = mean_pool(a, anchor_mask)\n",
    "                p = mean_pool(p, pos_mask)\n",
    "                \n",
    "            elif optimized_rep == \"cls\":\n",
    "                a = cls_pool(a, anchor_mask)\n",
    "                p = cls_pool(p, pos_mask)\n",
    "                \n",
    "            elif optimized_rep == \"sep\":\n",
    "                a = sep_pool(a, anchor_mask)\n",
    "                p = sep_pool(p, pos_mask)\n",
    "                \n",
    "            elif optimized_rep == \"7th\":\n",
    "                a = seventh_pool(a, anchor_mask)\n",
    "                p = seventh_pool(p, pos_mask)\n",
    "                \n",
    "            # calculate the cosine similarities\n",
    "            scores = torch.stack(\n",
    "                [cos_sim(a_i.reshape(1, a_i.shape[0]), p) for a_i in a]\n",
    "            )\n",
    "            # get label(s)\n",
    "            labels = torch.tensor(\n",
    "                range(len(scores)), dtype=torch.long, device=scores.device\n",
    "            )\n",
    "            # and now calculate the loss\n",
    "            loss = loss_func(scores * scale, labels) \n",
    "            losses_train[epoch, i_batch] = loss.item()\n",
    "\n",
    "            # using loss, calculate gradients and then optimize\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            # update learning rate scheduler\n",
    "            scheduler.step()\n",
    "            # update the TDQM progress bar\n",
    "            train_loop.set_description(f\"Epoch {epoch}\")\n",
    "            train_loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        # TEST LOADER -- no training model, only evaluating loss for the test loader\n",
    "        # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
    "        test_loop = tqdm(test_loader, leave=True)\n",
    "        for i_batch, batch in enumerate(test_loop):\n",
    "            # prepare batches and move all to the active device\n",
    "            anchor_ids = batch[0][0].to(device)     \n",
    "            anchor_mask = batch[0][1].to(device)\n",
    "            pos_ids = batch[1][0].to(device)\n",
    "            pos_mask = batch[1][1].to(device)\n",
    "            # extract token embeddings from model\n",
    "            a = model(anchor_ids, attention_mask=anchor_mask)[0]  # all token embeddings\n",
    "            p = model(pos_ids, attention_mask=pos_mask)[0]\n",
    "            \n",
    "            # get the pooled vectors\n",
    "            if optimized_rep == \"av\":\n",
    "                a = mean_pool(a, anchor_mask)\n",
    "                p = mean_pool(p, pos_mask)\n",
    "                \n",
    "            elif optimized_rep == \"cls\":\n",
    "                a = cls_pool(a, anchor_mask)\n",
    "                p = cls_pool(p, pos_mask)\n",
    "                \n",
    "            elif optimized_rep == \"sep\":\n",
    "                a = sep_pool(a, anchor_mask)\n",
    "                p = sep_pool(p, pos_mask)\n",
    "                \n",
    "            elif optimized_rep == \"7th\":\n",
    "                a = seventh_pool(a, anchor_mask)\n",
    "                p = seventh_pool(p, pos_mask)\n",
    "                \n",
    "            # calculate the cosine similarities\n",
    "            scores = torch.stack(\n",
    "                [cos_sim(a_i.reshape(1, a_i.shape[0]), p) for a_i in a]\n",
    "            )\n",
    "            # get label(s)\n",
    "            labels = torch.tensor(\n",
    "                range(len(scores)), dtype=torch.long, device=scores.device\n",
    "            )\n",
    "            # and now calculate the loss\n",
    "            loss = loss_func(scores * scale, labels)  \n",
    "            losses_test[epoch, i_batch] = loss.item()\n",
    "\n",
    "            # update the TDQM progress bar\n",
    "            test_loop.set_description(f\"Epoch {epoch}\")\n",
    "            test_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "        ## evaluation \n",
    "        # TRAINING DATASET -- evaluation of abstract accuracy\n",
    "        X_train = list(np.vstack(train_dataset.abs_sentences)[:, 0])\n",
    "        y_train = list(np.vstack(train_dataset.abs_sentences)[:, 1])\n",
    "\n",
    "        # knn accuracy\n",
    "        embd_cls_train, embd_sep_train, embd_av_train = generate_embeddings(\n",
    "            X_train, tokenizer, model, device, batch_size=256\n",
    "        )\n",
    "        knn_acc_train = knn_accuracy([embd_av_train, embd_cls_train, embd_sep_train], y_train)\n",
    "        knn_accuracies_train[epoch,:]= knn_acc_train\n",
    "\n",
    "\n",
    "        # TEST DATASET -- evaluation of abstract accuracy\n",
    "        X_test = list(np.vstack(test_dataset.abs_sentences)[:, 0])\n",
    "        y_test = list(np.vstack(test_dataset.abs_sentences)[:, 1])\n",
    "\n",
    "        # knn accuracy\n",
    "        embd_cls_test, embd_sep_test, embd_av_test = generate_embeddings(\n",
    "            X_test, tokenizer, model, device, batch_size=256\n",
    "        )\n",
    "        knn_acc_test = knn_accuracy([embd_av_test, embd_cls_test, embd_sep_test], y_test)\n",
    "        knn_accuracies_test[epoch,:]= knn_acc_test\n",
    "\n",
    "        \n",
    "    return losses_train, losses_test, knn_accuracies_train, knn_accuracies_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  BERT\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n",
      "CPU times: user 48.1 s, sys: 20.2 s, total: 1min 8s\n",
      "Wall time: 8.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# model\n",
    "i = 0\n",
    "model_name = model_names[i]\n",
    "\n",
    "## fix random seeds\n",
    "seed = 42\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed)\n",
    "# Set the random seed\n",
    "random.seed(seed)\n",
    "\n",
    "## set up model\n",
    "print(\"Model: \", model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on device: {}\".format(device))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_paths[i])\n",
    "model = AutoModel.from_pretrained(model_paths[i])\n",
    "print(model_paths[i])\n",
    "\n",
    "## data\n",
    "# split into train and test\n",
    "X_train, X_test = train_test_split(\n",
    "    iclr.abstract, test_size=0.1, random_state=np.random.seed(seed)\n",
    ")\n",
    "\n",
    "# do the train and test datasets with the class\n",
    "n_cons_sntcs = 2\n",
    "\n",
    "train_dataset = MultOverlappingSentencesPairDataset(\n",
    "    X_train, tokenizer, device, n_cons_sntcs=n_cons_sntcs, seed=42\n",
    ")\n",
    "test_dataset = MultOverlappingSentencesPairDataset(\n",
    "    X_test, tokenizer, device, n_cons_sntcs=n_cons_sntcs, seed=42\n",
    ")\n",
    "\n",
    "gen = torch.Generator()\n",
    "gen.manual_seed(seed)\n",
    "\n",
    "# do the loader with the train and test datasets\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True, generator=gen\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=True, generator=gen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14345\n",
      "1586\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exploration batches class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"BERT\",\n",
    "    \"MPNet\",\n",
    "    \"SBERT\",\n",
    "    # \"SciBERT\",\n",
    "    # \"SPECTER\",\n",
    "    # \"SciNCL\",\n",
    "]\n",
    "\n",
    "\n",
    "model_paths = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"microsoft/mpnet-base\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    # \"allenai/scibert_scivocab_uncased\",\n",
    "    # \"allenai/specter\",\n",
    "    # \"malteos/scincl\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  BERT\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "i = 0\n",
    "model_name = model_names[i]\n",
    "\n",
    "## fix random seeds\n",
    "seed = 42\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed)\n",
    "# Set the random seed\n",
    "random.seed(seed)\n",
    "\n",
    "# set up model\n",
    "print(\"Model: \", model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on device: {}\".format(device))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_paths[i])\n",
    "model = AutoModel.from_pretrained(model_paths[i])\n",
    "print(model_paths[i])\n",
    "\n",
    "# data\n",
    "training_dataset = MultOverlappingSentencesPairDataset(\n",
    "    iclr.abstract, tokenizer, device, n_cons_sntcs=2, seed=42\n",
    ")\n",
    "\n",
    "gen = torch.Generator()\n",
    "gen.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(\n",
    "    training_dataset, batch_size=64, shuffle=True, generator=gen\n",
    ")\n",
    "\n",
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n"
     ]
    }
   ],
   "source": [
    "print(len(training_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  2004,  3698,  4083,  4275,  2024,  6233,  2108,  4846,  2000,\n",
      "         2191,  9530,  3366, 15417,  4818,  6567,  1999,  2613,  1011,  2088,\n",
      "        10906,  1010,  2009,  4150,  4187,  2000,  5676,  2008,  3633,  2040,\n",
      "         2024, 15316,  2135, 19209,  1006,  1041,  1012,  2096,  2195,  8107,\n",
      "         2031,  2042,  3818,  2000,  9570, 28667, 22957,  2229,  2005,  5360,\n",
      "         3633,  1010,  1996, 28667, 22957,  2229,  6434,  2011,  2122,  4725,\n",
      "         2593,  6162,  2659,  5366,  1006,  1045,  1012,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0], device='cuda:0')\n",
      "172\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(training_loader):\n",
    "    if i == 0:\n",
    "        # print(batch)\n",
    "        # print(len(batch))\n",
    "        # print(batch[0])\n",
    "        # print(len(batch[0]))\n",
    "        print(batch[0][0][0])\n",
    "        print(len(batch[0][0][0]))\n",
    "        print(batch[0][1])\n",
    "        print(len(batch[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"In cities with tall buildings, emergency responders need an accurate floor level location to find 911 callers quickly. We introduce a system to estimate a victim's floor level via their mobile device's sensor data in a two-step process. \", 0)\n",
      "(\"We introduce a system to estimate a victim's floor level via their mobile device's sensor data in a two-step process. First, we train a neural network to determine when a smartphone enters or exits a building via GPS signal changes. \", 0)\n",
      "(\"First, we train a neural network to determine when a smartphone enters or exits a building via GPS signal changes. Second, we use a barometer equipped smartphone to measure the change in barometric pressure from the entrance of the building to the victim's indoor location. \", 0)\n",
      "(\"Second, we use a barometer equipped smartphone to measure the change in barometric pressure from the entrance of the building to the victim's indoor location. Unlike impractical previous approaches, our system is the first that does not require the use of beacons, prior knowledge of the building infrastructure, or knowledge of user behavior. \", 0)\n",
      "('Unlike impractical previous approaches, our system is the first that does not require the use of beacons, prior knowledge of the building infrastructure, or knowledge of user behavior. We demonstrate real-world feasibility through 63 experiments across five different tall buildings throughout New York City where our system predicted the correct floor level with 100% accuracy. ', 0)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for elem in training_dataset.abs_sentences[0]:\n",
    "    print(elem)\n",
    "print(len(training_dataset.abs_sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "5\n",
      "6\n",
      "11\n",
      "(11, 2)\n",
      "['0' '0' '0' '0' '0' '2' '2' '2' '2' '2' '2']\n",
      "[\"In cities with tall buildings, emergency responders need an accurate floor level location to find 911 callers quickly. We introduce a system to estimate a victim's floor level via their mobile device's sensor data in a two-step process. \", \"We introduce a system to estimate a victim's floor level via their mobile device's sensor data in a two-step process. First, we train a neural network to determine when a smartphone enters or exits a building via GPS signal changes. \", \"First, we train a neural network to determine when a smartphone enters or exits a building via GPS signal changes. Second, we use a barometer equipped smartphone to measure the change in barometric pressure from the entrance of the building to the victim's indoor location. \", \"Second, we use a barometer equipped smartphone to measure the change in barometric pressure from the entrance of the building to the victim's indoor location. Unlike impractical previous approaches, our system is the first that does not require the use of beacons, prior knowledge of the building infrastructure, or knowledge of user behavior. \", 'Unlike impractical previous approaches, our system is the first that does not require the use of beacons, prior knowledge of the building infrastructure, or knowledge of user behavior. We demonstrate real-world feasibility through 63 experiments across five different tall buildings throughout New York City where our system predicted the correct floor level with 100% accuracy. ', 'Compared to traditional one-vs-all classifiers that require $O(Kd)$ memory and inference cost, MACH only need $O(d\\\\log{K})$ memory while only requiring $O(K\\\\log{K} + d\\\\log{K})$ operation for inference. MACH is the first generic $K$-classification algorithm, with provably theoretical guarantees, which requires $O(\\\\log{K})$ memory without any assumption on the relationship between classes. ', 'MACH is the first generic $K$-classification algorithm, with provably theoretical guarantees, which requires $O(\\\\log{K})$ memory without any assumption on the relationship between classes. MACH uses universal hashing to reduce classification with a large number of classes to few independent classification task with very small (constant) number of classes. ', 'MACH uses universal hashing to reduce classification with a large number of classes to few independent classification task with very small (constant) number of classes. We provide theoretical quantification of accuracy-memory tradeoff by showing the first connection between extreme classification and heavy hitters. ', 'We provide theoretical quantification of accuracy-memory tradeoff by showing the first connection between extreme classification and heavy hitters. With MACH we can train ODP dataset with 100,000 classes and 400,000 features on a single Titan X GPU (12GB), with the classification accuracy of 19. ', 'With MACH we can train ODP dataset with 100,000 classes and 400,000 features on a single Titan X GPU (12GB), with the classification accuracy of 19. Before this work, the best performing baseline is a one-vs-all classifier that requires 40 billion parameters (320 GB model size) and achieves 9\\\\% accuracy. ', 'Before this work, the best performing baseline is a one-vs-all classifier that requires 40 billion parameters (320 GB model size) and achieves 9\\\\% accuracy. With MACH, we also demonstrate complete training of fine-grained imagenet dataset (compressed size 104GB), with 21,000 classes, on a single GPU. ']\n"
     ]
    }
   ],
   "source": [
    "subset_abs_sentences = training_dataset.abs_sentences[0:2]\n",
    "print(len(subset_abs_sentences))\n",
    "print(len(subset_abs_sentences[0]))\n",
    "print(len(subset_abs_sentences[1]))\n",
    "print(len(np.vstack(subset_abs_sentences)))\n",
    "print(np.vstack(subset_abs_sentences).shape)\n",
    "print(np.vstack(subset_abs_sentences)[:, 1])\n",
    "print(list(np.vstack(subset_abs_sentences)[:, 0]))\n",
    "# print(np.vstack(training_dataset.abs_sentences)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline abstract accuracy\n",
    "As a sanity check and using this new dataset and kNN abstract accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"BERT\",\n",
    "    \"MPNet\",\n",
    "    \"SBERT\",\n",
    "    # \"SciBERT\",\n",
    "    # \"SPECTER\",\n",
    "    # \"SciNCL\",\n",
    "]\n",
    "\n",
    "\n",
    "model_paths = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"microsoft/mpnet-base\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    # \"allenai/scibert_scivocab_uncased\",\n",
    "    # \"allenai/specter\",\n",
    "    # \"malteos/scincl\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  BERT\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n",
      "Train set:  (14882,)\n",
      "Test set:  (1654,)\n",
      "Train dataset:  14345\n",
      "Test dataset:  1586\n",
      "CPU times: user 53.4 s, sys: 21.3 s, total: 1min 14s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "model_name = model_names[i]\n",
    "\n",
    "## fix random seeds\n",
    "seed = 42\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed)\n",
    "# Set the random seed\n",
    "random.seed(seed)\n",
    "\n",
    "## set up model\n",
    "print(\"Model: \", model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on device: {}\".format(device))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_paths[i])\n",
    "model = AutoModel.from_pretrained(model_paths[i])\n",
    "model.to(device)\n",
    "print(model_paths[i])\n",
    "\n",
    "## data\n",
    "# split into train and test\n",
    "X_train, X_test = train_test_split(\n",
    "    iclr.abstract, test_size=0.1, random_state=np.random.seed(seed)\n",
    ")\n",
    "print(\"Train set: \", X_train.shape)\n",
    "print(\"Test set: \", X_test.shape)\n",
    "\n",
    "# do the train and test datasets with the class\n",
    "n_cons_sntcs = 2\n",
    "\n",
    "train_dataset = MultOverlappingSentencesPairDataset(\n",
    "    X_train, tokenizer, device, n_cons_sntcs=n_cons_sntcs, seed=42\n",
    ")\n",
    "test_dataset = MultOverlappingSentencesPairDataset(\n",
    "    X_test, tokenizer, device, n_cons_sntcs=n_cons_sntcs, seed=42\n",
    ")\n",
    "\n",
    "print(\"Train dataset: \", len(train_dataset))\n",
    "print(\"Test dataset: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 70175\n",
      "Number of classes: 14345\n",
      "Ratio: 0.20441752760954757\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e540b17b65542dd94c7d6368529d792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[0.29652323 0.09988601 0.11157025]\n",
      "CPU times: user 21min 59s, sys: 1h, total: 1h 22min\n",
      "Wall time: 3min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = list(np.vstack(train_dataset.abs_sentences)[:, 0])\n",
    "y_train = list(np.vstack(train_dataset.abs_sentences)[:, 1])\n",
    "print(\"Number of sentences:\", len(X_train))\n",
    "print(\"Number of classes:\", len(np.unique(y_train)))\n",
    "print(\"Ratio:\", len(np.unique(y_train)) / len(X_train))\n",
    "\n",
    "# knn accuracy\n",
    "embd_cls_train, embd_sep_train, embd_av_train = generate_embeddings(\n",
    "    X_train, tokenizer, model, device, batch_size=256\n",
    ")\n",
    "knn_acc_train = knn_accuracy(\n",
    "    [embd_av_train, embd_cls_train, embd_sep_train], y_train\n",
    ")\n",
    "print(knn_acc_train)\n",
    "\n",
    "# save\n",
    "saving_path = Path(\"embeddings_\" + model_name.lower())\n",
    "(variables_path / saving_path).mkdir(exist_ok=True)\n",
    "np.save(\n",
    "    variables_path / saving_path / \"knn_accuracies_abstract_train_baseline\",\n",
    "    knn_acc_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network. Following recent work in simplifying these networks with model pruning and a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs. \n",
      "Following recent work in simplifying these networks with model pruning and a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs. We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout. \n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 7820\n",
      "Number of classes: 1586\n",
      "Ratio: 0.20281329923273658\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aae7df560a64f1697e7c3263d0d1e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[0.40920716 0.17647059 0.15984655]\n",
      "CPU times: user 51.2 s, sys: 2min 15s, total: 3min 6s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TEST DATASET -- evaluation of abstract accuracy\n",
    "X_test = list(np.vstack(test_dataset.abs_sentences)[:, 0])\n",
    "y_test = list(np.vstack(test_dataset.abs_sentences)[:, 1])\n",
    "print(\"Number of sentences:\", len(X_test))\n",
    "print(\"Number of classes:\", len(np.unique(y_test)))\n",
    "print(\"Ratio:\", len(np.unique(y_test)) / len(X_test))\n",
    "\n",
    "# knn accuracy\n",
    "embd_cls_test, embd_sep_test, embd_av_test = generate_embeddings(\n",
    "    X_test, tokenizer, model, device, batch_size=256\n",
    ")\n",
    "knn_acc_test = knn_accuracy(\n",
    "    [embd_av_test, embd_cls_test, embd_sep_test], y_test\n",
    ")\n",
    "print(knn_acc_test)\n",
    "\n",
    "saving_path = Path(\"embeddings_\" + model_name.lower())\n",
    "(variables_path / saving_path).mkdir(exist_ok=True)\n",
    "np.save(\n",
    "    variables_path / saving_path / \"knn_accuracies_abstract_test_baseline\",\n",
    "    knn_acc_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.297 0.1   0.112]\n",
      "[0.409 0.176 0.16 ]\n"
     ]
    }
   ],
   "source": [
    "print(np.round(knn_acc_train, 3))\n",
    "print(np.round(knn_acc_test, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "def knn_acc_loocv(X, y, n_neighbors=10):\n",
    "    neigh = NearestNeighbors(n_neighbors=n_neighbors).fit(X)\n",
    "    knn = neigh.kneighbors(return_distance=False)\n",
    "    yhat = mode(y[knn], axis=1).mode.flatten()\n",
    "    return np.mean(yhat == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42762148337595907\n",
      "CPU times: user 13.6 s, sys: 38.4 s, total: 52.1 s\n",
      "Wall time: 2.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "knn_acc_loo_test = knn_acc_loocv(embd_av_test, np.array(y_test))\n",
    "print(knn_acc_loo_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  7839\n",
      "Number of classes:  1593\n",
      "Ratio:  0.20321469575200918\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a658ed60ad9249a389908db027223840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[0.37627551 0.19387755 0.13137755]\n",
      "CPU times: user 1min 11s, sys: 3min 9s, total: 4min 20s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(12)\n",
    "subset = np.random.choice(\n",
    "    len(iclr.abstract), size=int(len(iclr.abstract) * 0.1), replace=False\n",
    ")\n",
    "\n",
    "train_subset_dataset = MultOverlappingSentencesPairDataset(\n",
    "    iclr.abstract[subset],\n",
    "    tokenizer,\n",
    "    device,\n",
    "    n_cons_sntcs=n_cons_sntcs,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "X_train_subset = list(np.vstack(train_subset_dataset.abs_sentences)[:, 0])\n",
    "y_train_subset = list(np.vstack(train_subset_dataset.abs_sentences)[:, 1])\n",
    "print(\"Number of sentences: \", len(X_train_subset))\n",
    "print(\"Number of classes: \", len(np.unique(y_train_subset)))\n",
    "print(\"Ratio: \", len(np.unique(y_train_subset)) / len(X_train_subset))\n",
    "\n",
    "\n",
    "# knn accuracy\n",
    "(\n",
    "    embd_cls_train_subset,\n",
    "    embd_sep_train_subset,\n",
    "    embd_av_train_subset,\n",
    ") = generate_embeddings(\n",
    "    X_train_subset, tokenizer, model, device, batch_size=256\n",
    ")\n",
    "knn_acc_train_subset_3 = knn_accuracy(\n",
    "    [embd_av_train_subset, embd_cls_train_subset, embd_sep_train_subset],\n",
    "    y_train_subset,\n",
    ")\n",
    "print(knn_acc_train_subset_3)\n",
    "\n",
    "# # save\n",
    "# saving_path = Path(\"embeddings_\" + model_name.lower())\n",
    "# (variables_path / saving_path).mkdir(exist_ok=True)\n",
    "# np.save(\n",
    "#     variables_path / saving_path / \"knn_accuracies_abstract_train_baseline\",\n",
    "#     knn_acc_train,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7820\n",
      "6107\n",
      "0.7809462915601023\n",
      "[0.01918159 0.01662404 0.01150895]\n",
      "CPU times: user 52.6 s, sys: 2min 11s, total: 3min 3s\n",
      "Wall time: 3.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# subset of train set of size=test set\n",
    "# for rs_i in range(5):\n",
    "np.random.seed(42)\n",
    "subset = np.random.choice(len(X_train), size=len(X_test), replace=False)\n",
    "print(len(X_test))\n",
    "print(len(np.unique(np.array(y_train)[subset])))\n",
    "print(len(np.unique(np.array(y_train)[subset])) / len(X_test))\n",
    "\n",
    "knn_acc_train_subset = knn_accuracy(\n",
    "    [\n",
    "        embd_av_train[subset],\n",
    "        embd_cls_train[subset],\n",
    "        embd_sep_train[subset],\n",
    "    ],\n",
    "    np.array(y_train)[subset],\n",
    ")\n",
    "print(knn_acc_train_subset)\n",
    "# # save\n",
    "# saving_path = Path(\"embeddings_\" + model_name.lower())\n",
    "# (variables_path / saving_path).mkdir(exist_ok=True)\n",
    "# np.save(\n",
    "#     variables_path\n",
    "#     / saving_path\n",
    "#     / \"knn_accuracies_abstract_train_subset_baseline\",\n",
    "#     knn_acc_train_subset,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1605\n",
      "0.20524296675191817\n",
      "[0.34015345 0.16368286 0.15984655]\n",
      "CPU times: user 1min 1s, sys: 3min 1s, total: 4min 3s\n",
      "Wall time: 5.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# subset of train set of size=test set\n",
    "# knn accuracy\n",
    "# print(np.array(y_train)[subset].shape)\n",
    "n = 7820\n",
    "knn_acc_train_subset_2 = knn_accuracy(\n",
    "    [\n",
    "        embd_av_train[:n],\n",
    "        embd_cls_train[:n],\n",
    "        embd_sep_train[:n],\n",
    "    ],\n",
    "    np.array(y_train)[:n],\n",
    ")\n",
    "print(len(np.unique(np.array(y_train)[:n])))\n",
    "print(len(np.unique(np.array(y_train)[:n])) / n)\n",
    "print(knn_acc_train_subset_2)\n",
    "# # save\n",
    "# saving_path = Path(\"embeddings_\" + model_name.lower())\n",
    "# (variables_path / saving_path).mkdir(exist_ok=True)\n",
    "# np.save(\n",
    "#     variables_path\n",
    "#     / saving_path\n",
    "#     / \"knn_accuracies_abstract_train_subset_baseline\",\n",
    "#     knn_acc_train_subset,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset size      number of classes       ratio           accuracy\n",
      "100                   100               1.000           0.000\n",
      "1000                   972               0.972           0.000\n",
      "10000                   7308               0.731           0.032\n",
      "50000                   14165               0.283           0.220\n",
      "CPU times: user 9min 28s, sys: 19min 19s, total: 28min 48s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# subset of train set of size=test set\n",
    "print(\"Subset size      number of classes       ratio           accuracy\")\n",
    "for subset_size in np.hstack((np.logspace(2, 4, 3), np.array([50000]))):\n",
    "    np.random.seed(12)\n",
    "    subset = np.random.choice(\n",
    "        len(X_train), size=int(subset_size), replace=False\n",
    "    )\n",
    "    # knn accuracy\n",
    "    # print(np.array(y_train)[subset].shape)\n",
    "\n",
    "    knn_acc_loo_train_subset = knn_acc_loocv(\n",
    "        embd_av_train[subset], np.array(y_train)[subset]\n",
    "    )\n",
    "    print(\n",
    "        f\"{int(subset_size)}\",\n",
    "        f\"                  {len(np.unique(np.array(y_train)[subset]))}\",\n",
    "        f\"              {len(np.unique(np.array(y_train)[subset])) / subset_size:.3f}\",\n",
    "        f\"          {knn_acc_loo_train_subset:.3f}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set ratio:  0.2044\n",
      "Test set ratio:  0.2028\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set ratio: \", np.round(len(np.unique(y_train)) / len(X_train), 4))\n",
    "print(\"Test set ratio: \", np.round(len(np.unique(y_test)) / len(X_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  70175 70175\n",
      "Test set:  7820 7820\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set: \", len(X_train), len(y_train))\n",
    "print(\"Test set: \", len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check for another random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  BERT\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n",
      "TRAIN SET, random seed 0\n",
      "Number of sentences: 70246\n",
      "Number of classes: 14335\n",
      "Ratio: 0.2040685590638613\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400e5bd23d1747aca26e9e33fd151a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[0.30462633 0.11160142 0.10064057]\n",
      "TEST SET, random seed 0\n",
      "Number of sentences: 7749\n",
      "Number of classes: 1596\n",
      "Ratio: 0.20596205962059622\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2d953b4418424abc9997dbe0653599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[0.35741935 0.1483871  0.15483871]\n",
      "--------------------------------\n",
      "TRAIN SET, random seed 1\n",
      "Number of sentences: 70194\n",
      "Number of classes: 14347\n",
      "Ratio: 0.20439068866284868\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c34ab02ba34431db39d65899943828f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[0.2965812  0.1037037  0.10797721]\n",
      "TEST SET, random seed 1\n",
      "Number of sentences: 7801\n",
      "Number of classes: 1584\n",
      "Ratio: 0.20305089091142162\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd59139595224e9aa435f8b2e402db9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[0.37900128 0.1574904  0.13060179]\n",
      "--------------------------------\n",
      "TRAIN SET, random seed 2\n",
      "Number of sentences: 70106\n",
      "Number of classes: 14332\n",
      "Ratio: 0.2044332867372265\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d2df38e21440bcbaedbe9534ae9c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[0.29681928 0.100271   0.10711739]\n",
      "TEST SET, random seed 2\n",
      "Number of sentences: 7889\n",
      "Number of classes: 1599\n",
      "Ratio: 0.20268728609456205\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70777eff5754eaea5adb5c2cfc43f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[0.37515843 0.16603295 0.15335868]\n",
      "--------------------------------\n",
      "TRAIN SET, random seed 3\n",
      "Number of sentences: 70326\n",
      "Number of classes: 14350\n",
      "Ratio: 0.20404971134431077\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b336c1426be431f835f74d1cb1a949c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[0.29745486 0.09853548 0.10507607]\n",
      "TEST SET, random seed 3\n",
      "Number of sentences: 7669\n",
      "Number of classes: 1581\n",
      "Ratio: 0.2061546485852132\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0576b2a9678446e83553ff7cee6624d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[0.37027379 0.17079531 0.12516297]\n",
      "--------------------------------\n",
      "TRAIN SET, random seed 4\n",
      "Number of sentences: 70248\n",
      "Number of classes: 14339\n",
      "Ratio: 0.20411969024029153\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce7d717cc55499e874ea51f9c806d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[0.29907473 0.10021352 0.10035587]\n",
      "TEST SET, random seed 4\n",
      "Number of sentences: 7747\n",
      "Number of classes: 1592\n",
      "Ratio: 0.2054989028010843\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3252096d7ba34ae4962a8d98007224fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[0.37419355 0.18064516 0.14193548]\n",
      "--------------------------------\n",
      "CPU times: user 1h 58min 36s, sys: 5h 10min 2s, total: 7h 8min 39s\n",
      "Wall time: 20min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "model_name = model_names[i]\n",
    "\n",
    "## fix random seeds\n",
    "seed = 42\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed)\n",
    "# Set the random seed\n",
    "random.seed(seed)\n",
    "\n",
    "## set up model\n",
    "print(\"Model: \", model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on device: {}\".format(device))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_paths[i])\n",
    "model = AutoModel.from_pretrained(model_paths[i])\n",
    "model.to(device)\n",
    "print(model_paths[i])\n",
    "\n",
    "## data\n",
    "# split into train and test\n",
    "for rs_i in range(5):\n",
    "    X_train, X_test = train_test_split(\n",
    "        iclr.abstract, test_size=0.1, random_state=np.random.seed(rs_i)\n",
    "    )\n",
    "    # print(\"Train set: \", X_train.shape)\n",
    "    # print(\"Test set: \", X_test.shape)\n",
    "\n",
    "    # do the train and test datasets with the class\n",
    "    n_cons_sntcs = 2\n",
    "\n",
    "    train_dataset = MultOverlappingSentencesPairDataset(\n",
    "        X_train, tokenizer, device, n_cons_sntcs=n_cons_sntcs, seed=42\n",
    "    )\n",
    "    test_dataset = MultOverlappingSentencesPairDataset(\n",
    "        X_test, tokenizer, device, n_cons_sntcs=n_cons_sntcs, seed=42\n",
    "    )\n",
    "    print(f\"TRAIN SET, random seed {rs_i}\")\n",
    "    X_train = list(np.vstack(train_dataset.abs_sentences)[:, 0])\n",
    "    y_train = list(np.vstack(train_dataset.abs_sentences)[:, 1])\n",
    "    print(\"Number of sentences:\", len(X_train))\n",
    "    print(\"Number of classes:\", len(np.unique(y_train)))\n",
    "    print(\"Ratio:\", len(np.unique(y_train)) / len(X_train))\n",
    "\n",
    "    # knn accuracy\n",
    "    embd_cls_train, embd_sep_train, embd_av_train = generate_embeddings(\n",
    "        X_train, tokenizer, model, device, batch_size=256\n",
    "    )\n",
    "    knn_acc_train = knn_accuracy(\n",
    "        [embd_av_train, embd_cls_train, embd_sep_train], y_train\n",
    "    )\n",
    "    print(knn_acc_train)\n",
    "\n",
    "    print(f\"TEST SET, random seed {rs_i}\")\n",
    "    X_test = list(np.vstack(test_dataset.abs_sentences)[:, 0])\n",
    "    y_test = list(np.vstack(test_dataset.abs_sentences)[:, 1])\n",
    "    print(\"Number of sentences:\", len(X_test))\n",
    "    print(\"Number of classes:\", len(np.unique(y_test)))\n",
    "    print(\"Ratio:\", len(np.unique(y_test)) / len(X_test))\n",
    "\n",
    "    # knn accuracy\n",
    "    embd_cls_test, embd_sep_test, embd_av_test = generate_embeddings(\n",
    "        X_test, tokenizer, model, device, batch_size=256\n",
    "    )\n",
    "    knn_acc_test = knn_accuracy(\n",
    "        [embd_av_test, embd_cls_test, embd_sep_test], y_test\n",
    "    )\n",
    "    print(knn_acc_test)\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  BERT\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n",
      "TEST SET, random seed 0\n",
      "Number of sentences: 7749\n",
      "Number of classes: 1596\n",
      "Ratio: 0.20596205962059622\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa31bf4945d48bcbf278063b4f8a6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "knn acc loo:  0.4165698799845141\n",
      "--------------------------------\n",
      "TEST SET, random seed 1\n",
      "Number of sentences: 7801\n",
      "Number of classes: 1584\n",
      "Ratio: 0.20305089091142162\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6198eea810e545618ba249f39b256c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "knn acc loo:  0.41212665042943214\n",
      "--------------------------------\n",
      "TEST SET, random seed 2\n",
      "Number of sentences: 7889\n",
      "Number of classes: 1599\n",
      "Ratio: 0.20268728609456205\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d010010585ed45829e2786ca5839c076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "knn acc loo:  0.41424768665230066\n",
      "--------------------------------\n",
      "TEST SET, random seed 3\n",
      "Number of sentences: 7669\n",
      "Number of classes: 1581\n",
      "Ratio: 0.2061546485852132\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4848f57a0ebf4421b0b76aa1eea4bdf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "knn acc loo:  0.4246968313991394\n",
      "--------------------------------\n",
      "TEST SET, random seed 4\n",
      "Number of sentences: 7747\n",
      "Number of classes: 1592\n",
      "Ratio: 0.2054989028010843\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa4ae1f27f543b79161946384c65c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "knn acc loo:  0.4082870788692397\n",
      "--------------------------------\n",
      "CPU times: user 2min 34s, sys: 5min 7s, total: 7min 42s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "model_name = model_names[i]\n",
    "\n",
    "## fix random seeds\n",
    "seed = 42\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed)\n",
    "# Set the random seed\n",
    "random.seed(seed)\n",
    "\n",
    "## set up model\n",
    "print(\"Model: \", model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on device: {}\".format(device))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_paths[i])\n",
    "model = AutoModel.from_pretrained(model_paths[i])\n",
    "model.to(device)\n",
    "print(model_paths[i])\n",
    "\n",
    "## data\n",
    "# split into train and test\n",
    "for rs_i in range(5):\n",
    "    X_train, X_test = train_test_split(\n",
    "        iclr.abstract, test_size=0.1, random_state=np.random.seed(rs_i)\n",
    "    )\n",
    "\n",
    "    # do the train and test datasets with the class\n",
    "    n_cons_sntcs = 2\n",
    "\n",
    "    test_dataset = MultOverlappingSentencesPairDataset(\n",
    "        X_test, tokenizer, device, n_cons_sntcs=n_cons_sntcs, seed=42\n",
    "    )\n",
    "\n",
    "    print(f\"TEST SET, random seed {rs_i}\")\n",
    "    X_test = list(np.vstack(test_dataset.abs_sentences)[:, 0])\n",
    "    y_test = list(np.vstack(test_dataset.abs_sentences)[:, 1])\n",
    "    print(\"Number of sentences:\", len(X_test))\n",
    "    print(\"Number of classes:\", len(np.unique(y_test)))\n",
    "    print(\"Ratio:\", len(np.unique(y_test)) / len(X_test))\n",
    "\n",
    "    # knn accuracy\n",
    "    embd_cls_test, embd_sep_test, embd_av_test = generate_embeddings(\n",
    "        X_test, tokenizer, model, device, batch_size=256\n",
    "    )\n",
    "    knn_acc_loo_test = knn_acc_loocv(embd_av_test, np.array(y_test))\n",
    "    print(\"knn acc loo: \", knn_acc_loo_test)\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                           Size           number of classes                              AV                CLS               SEP               \n",
    "Train set         70175       14345   [0.297           0.1                0.112]               (evaluated on 7018 points)\n",
    "Test set           7820         1586   [0.409           0.176           0.16 ]                (evaluated on 782 points)\n",
    "\n",
    "   accuracy \n",
    "                         0.297\n",
    "                             0.409"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
