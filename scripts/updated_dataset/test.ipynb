{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    wrapped_model, \n",
    "    loader,  # training data loader\n",
    "    device,\n",
    "    eval_train_data,\n",
    "    eval_train_labels,\n",
    "    eval_test_data = None,  # None when eval is on MTEB or no train/test split\n",
    "    eval_test_labels =None,\n",
    "    eval_every_epochs =  True, #bool, {True, False} \n",
    "    eval_every_batches = 0, # int, 0 would be like none\n",
    "    eval_function = KNNEval,\n",
    "    pooler = mean_pool,\n",
    "    eval_rep=None, # representation to evaluate, if None it is the same used by pooler\n",
    "    dist_metric = \"euclidean\",\n",
    "    mteb_saving_path = None,\n",
    "    mteb_tasks = None, \n",
    "    n_epochs=1,\n",
    "    lr=2e-5,\n",
    "    scale = 20.0,  # we multiply similarity score by this scale value, it is the inverse of the temperature\n",
    "):\n",
    "    \n",
    "    ## training set up\n",
    "    ...\n",
    "\n",
    "    # initialize eval list\n",
    "    if (eval_every_epochs != 0) | (eval_every_batches != 0):\n",
    "        training_eval_results = defaultdict(list)\n",
    "\n",
    "    ## training\n",
    "    for epoch in range(n_epochs):\n",
    "        wrapped_model.model.train()  # make sure model is in training mode\n",
    "        # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
    "        loop = tqdm(loader, leave=True)\n",
    "        for i_batch, batch in enumerate(loop):\n",
    "            ## train -- finished\n",
    "            # zero all gradients on each new step\n",
    "            optim.zero_grad()\n",
    "            # prepare batches and move all to the active device\n",
    "            anchor_ids = batch[0][0].to(device)     # this are all anchor abstracts from the batch,len(anchor_ids)= len(batch)\n",
    "            anchor_mask = batch[0][1].to(device)\n",
    "            pos_ids = batch[1][0].to(device)       # this each positive pair from each anchor, all in one array, also len(batch)\n",
    "            pos_mask = batch[1][1].to(device)\n",
    "\n",
    "            # get hidden state\n",
    "            a = wrapped_model.get_outputs(input_ids=anchor_ids, attention_mask=anchor_mask)\n",
    "            p = wrapped_model.get_outputs(input_ids = pos_ids, attention_mask=pos_mask)\n",
    "            \n",
    "            # get the mean pooled vectors  \n",
    "            a = pooler(a, anchor_mask)\n",
    "            p = pooler(p, pos_mask)\n",
    "\n",
    "            # loss\n",
    "            ...\n",
    "\n",
    "            ## evaluation every batches\n",
    "            if eval_every_batches != 0:\n",
    "                eval_results = eval_function()\n",
    "                [training_eval_results[k].append(v) for k, v in eval_results.items()]\n",
    "\n",
    "        if eval_every_epochs != 0:\n",
    "            eval_results = eval_function()\n",
    "            [training_eval_results[k].append(v) for k, v in eval_results.items()]\n",
    "\n",
    "\n",
    "    if (eval_every_epochs != 0) | (eval_every_batches != 0):\n",
    "        ...\n",
    "        return losses, df_training_eval_results\n",
    "    else:\n",
    "        return losses"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
