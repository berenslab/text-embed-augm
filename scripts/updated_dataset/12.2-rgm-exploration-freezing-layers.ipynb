{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import gc\n",
    "\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import memory_profiler\n",
    "\n",
    "%load_ext memory_profiler\n",
    "\n",
    "from pathlib import Path\n",
    "import distro\n",
    "\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from text_embeddings_src.model_stuff import train_loop\n",
    "from text_embeddings_src.data_stuff import (\n",
    "    MultOverlappingSentencesPairDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(line_length=79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_path = Path(\"../../results/variables\")\n",
    "figures_path = Path(\"../../results/figures/updated_dataset\")\n",
    "data_path = Path(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"../matplotlib_style.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Rita González-Márquez\n",
      "\n",
      "Last updated: 2024-01-26 11:48:07CET\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.5\n",
      "IPython version      : 8.18.1\n",
      "\n",
      "openTSNE: 1.0.0\n",
      "\n",
      "Compiler    : GCC 11.2.0\n",
      "OS          : Linux\n",
      "Release     : 3.10.0-1160.el7.x86_64\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 64\n",
      "Architecture: 64bit\n",
      "\n",
      "Hostname: rgonzalesmarquez_GPU0-llm_gber7\n",
      "\n",
      "memory_profiler: 0.61.0\n",
      "scipy          : 1.11.4\n",
      "jupyter_black  : 0.3.4\n",
      "distro         : 1.8.0\n",
      "numpy          : 1.26.2\n",
      "torch          : 2.1.1\n",
      "pandas         : 2.1.3\n",
      "matplotlib     : 3.8.2\n",
      "\n",
      "Watermark: 2.4.3\n",
      "\n",
      "Ubuntu 22.04.3 LTS\n"
     ]
    }
   ],
   "source": [
    "%watermark -a 'Rita González-Márquez' -t -d -tz -u -v -iv -w -m -h -p transformers -p openTSNE\n",
    "print(distro.name(pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 285 ms, sys: 109 ms, total: 394 ms\n",
      "Wall time: 305 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iclr2024 = pd.read_parquet(\n",
    "    data_path / \"iclr2024.parquet.gzip\",\n",
    "    # index=False,\n",
    "    engine=\"pyarrow\",\n",
    "    # compression=\"gzip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iclr2024.keywords = iclr2024.keywords.transform(lambda x: list(x))\n",
    "iclr2024.scores = iclr2024.scores.transform(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>decision</th>\n",
       "      <th>scores</th>\n",
       "      <th>keywords</th>\n",
       "      <th>gender-first</th>\n",
       "      <th>gender-last</th>\n",
       "      <th>t-SNE x</th>\n",
       "      <th>t-SNE y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>S1VaB4cex</td>\n",
       "      <td>FractalNet: Ultra-Deep Neural Networks without...</td>\n",
       "      <td>We introduce a design strategy for neural netw...</td>\n",
       "      <td>Gustav Larsson, Michael Maire, Gregory Shakhna...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "      <td>[5, 7, 6, 6]</td>\n",
       "      <td>[]</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>-28.117955</td>\n",
       "      <td>-20.418127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>H1W1UN9gg</td>\n",
       "      <td>Deep Information Propagation</td>\n",
       "      <td>We study the behavior of untrained neural netw...</td>\n",
       "      <td>Samuel S. Schoenholz, Justin Gilmer, Surya Gan...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "      <td>[8, 9, 8]</td>\n",
       "      <td>[theory, deep learning]</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>-32.466820</td>\n",
       "      <td>-10.791123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>r1GKzP5xx</td>\n",
       "      <td>Recurrent Normalization Propagation</td>\n",
       "      <td>We propose a LSTM parametrization  that preser...</td>\n",
       "      <td>César Laurent, Nicolas Ballas, Pascal Vincent</td>\n",
       "      <td>Invite to Workshop Track</td>\n",
       "      <td>[4, 6, 6]</td>\n",
       "      <td>[deep learning, optimization]</td>\n",
       "      <td>None</td>\n",
       "      <td>male</td>\n",
       "      <td>3.504240</td>\n",
       "      <td>19.946053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2017</td>\n",
       "      <td>S1J0E-71l</td>\n",
       "      <td>Surprisal-Driven Feedback in Recurrent Networks</td>\n",
       "      <td>Recurrent neural nets are widely used for pred...</td>\n",
       "      <td>K, a, m, i, l,  , R, o, c, k, i</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[3, 4, 3]</td>\n",
       "      <td>[unsupervised learning, applications, deep lea...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4.553473</td>\n",
       "      <td>16.037763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>SJGCiw5gl</td>\n",
       "      <td>Pruning Convolutional Neural Networks for Reso...</td>\n",
       "      <td>We propose a new formulation for pruning convo...</td>\n",
       "      <td>Pavlo Molchanov, Stephen Tyree, Tero Karras, T...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "      <td>[6, 7, 9]</td>\n",
       "      <td>[deep learning, transfer learning]</td>\n",
       "      <td>None</td>\n",
       "      <td>male</td>\n",
       "      <td>-25.827705</td>\n",
       "      <td>-37.891772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24342</th>\n",
       "      <td>7299</td>\n",
       "      <td>2024</td>\n",
       "      <td>1bbPQShCT2</td>\n",
       "      <td>I-PHYRE: Interactive Physical Reasoning</td>\n",
       "      <td>Current evaluation protocols predominantly ass...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[intuitive physics, physical reasoning]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>43.137120</td>\n",
       "      <td>44.316133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24343</th>\n",
       "      <td>7300</td>\n",
       "      <td>2024</td>\n",
       "      <td>Ny150AblPu</td>\n",
       "      <td>EXPOSING TEXT-IMAGE INCONSISTENCY USING DIFFUS...</td>\n",
       "      <td>In the battle against widespread online misinf...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[mis-contextualization, media forensic]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>59.742172</td>\n",
       "      <td>-22.673627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24344</th>\n",
       "      <td>7301</td>\n",
       "      <td>2024</td>\n",
       "      <td>ZGBOfAQrMl</td>\n",
       "      <td>Video Super-Resolution Transformer with Masked...</td>\n",
       "      <td>Recently, Vision Transformer has achieved grea...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[video super-resolution, adaptive, memory and ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>57.933273</td>\n",
       "      <td>-3.932825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24345</th>\n",
       "      <td>7302</td>\n",
       "      <td>2024</td>\n",
       "      <td>J2kRjUAOLh</td>\n",
       "      <td>Contrastive Predict-and-Search for Mixed Integ...</td>\n",
       "      <td>Mixed integer linear programs  (MILP) are flex...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[mixed integer programs; contrastive learning]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-11.437999</td>\n",
       "      <td>21.289523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24346</th>\n",
       "      <td>7303</td>\n",
       "      <td>2024</td>\n",
       "      <td>U0P622bfUN</td>\n",
       "      <td>Federated Generative Learning with Foundation ...</td>\n",
       "      <td>Existing federated learning solutions focus on...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[federated learning, non-iid data]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-65.112587</td>\n",
       "      <td>18.746354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24347 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  year          id  \\\n",
       "0          0  2017   S1VaB4cex   \n",
       "1          1  2017   H1W1UN9gg   \n",
       "2          2  2017   r1GKzP5xx   \n",
       "3          3  2017   S1J0E-71l   \n",
       "4          4  2017   SJGCiw5gl   \n",
       "...      ...   ...         ...   \n",
       "24342   7299  2024  1bbPQShCT2   \n",
       "24343   7300  2024  Ny150AblPu   \n",
       "24344   7301  2024  ZGBOfAQrMl   \n",
       "24345   7302  2024  J2kRjUAOLh   \n",
       "24346   7303  2024  U0P622bfUN   \n",
       "\n",
       "                                                   title  \\\n",
       "0      FractalNet: Ultra-Deep Neural Networks without...   \n",
       "1                           Deep Information Propagation   \n",
       "2                    Recurrent Normalization Propagation   \n",
       "3        Surprisal-Driven Feedback in Recurrent Networks   \n",
       "4      Pruning Convolutional Neural Networks for Reso...   \n",
       "...                                                  ...   \n",
       "24342            I-PHYRE: Interactive Physical Reasoning   \n",
       "24343  EXPOSING TEXT-IMAGE INCONSISTENCY USING DIFFUS...   \n",
       "24344  Video Super-Resolution Transformer with Masked...   \n",
       "24345  Contrastive Predict-and-Search for Mixed Integ...   \n",
       "24346  Federated Generative Learning with Foundation ...   \n",
       "\n",
       "                                                abstract  \\\n",
       "0      We introduce a design strategy for neural netw...   \n",
       "1      We study the behavior of untrained neural netw...   \n",
       "2      We propose a LSTM parametrization  that preser...   \n",
       "3      Recurrent neural nets are widely used for pred...   \n",
       "4      We propose a new formulation for pruning convo...   \n",
       "...                                                  ...   \n",
       "24342  Current evaluation protocols predominantly ass...   \n",
       "24343  In the battle against widespread online misinf...   \n",
       "24344  Recently, Vision Transformer has achieved grea...   \n",
       "24345  Mixed integer linear programs  (MILP) are flex...   \n",
       "24346  Existing federated learning solutions focus on...   \n",
       "\n",
       "                                                 authors  \\\n",
       "0      Gustav Larsson, Michael Maire, Gregory Shakhna...   \n",
       "1      Samuel S. Schoenholz, Justin Gilmer, Surya Gan...   \n",
       "2          César Laurent, Nicolas Ballas, Pascal Vincent   \n",
       "3                        K, a, m, i, l,  , R, o, c, k, i   \n",
       "4      Pavlo Molchanov, Stephen Tyree, Tero Karras, T...   \n",
       "...                                                  ...   \n",
       "24342                                                      \n",
       "24343                                                      \n",
       "24344                                                      \n",
       "24345                                                      \n",
       "24346                                                      \n",
       "\n",
       "                       decision        scores  \\\n",
       "0               Accept (Poster)  [5, 7, 6, 6]   \n",
       "1               Accept (Poster)     [8, 9, 8]   \n",
       "2      Invite to Workshop Track     [4, 6, 6]   \n",
       "3                        Reject     [3, 4, 3]   \n",
       "4               Accept (Poster)     [6, 7, 9]   \n",
       "...                         ...           ...   \n",
       "24342                                      []   \n",
       "24343                                      []   \n",
       "24344                                      []   \n",
       "24345                                      []   \n",
       "24346                                      []   \n",
       "\n",
       "                                                keywords gender-first  \\\n",
       "0                                                     []         male   \n",
       "1                                [theory, deep learning]         male   \n",
       "2                          [deep learning, optimization]         None   \n",
       "3      [unsupervised learning, applications, deep lea...         None   \n",
       "4                     [deep learning, transfer learning]         None   \n",
       "...                                                  ...          ...   \n",
       "24342            [intuitive physics, physical reasoning]         None   \n",
       "24343            [mis-contextualization, media forensic]         None   \n",
       "24344  [video super-resolution, adaptive, memory and ...         None   \n",
       "24345     [mixed integer programs; contrastive learning]         None   \n",
       "24346                 [federated learning, non-iid data]         None   \n",
       "\n",
       "      gender-last    t-SNE x    t-SNE y  \n",
       "0            male -28.117955 -20.418127  \n",
       "1            None -32.466820 -10.791123  \n",
       "2            male   3.504240  19.946053  \n",
       "3            None   4.553473  16.037763  \n",
       "4            male -25.827705 -37.891772  \n",
       "...           ...        ...        ...  \n",
       "24342        None  43.137120  44.316133  \n",
       "24343        None  59.742172 -22.673627  \n",
       "24344        None  57.933273  -3.932825  \n",
       "24345        None -11.437999  21.289523  \n",
       "24346        None -65.112587  18.746354  \n",
       "\n",
       "[24347 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iclr2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_iclr = np.load(variables_path / \"updated_dataset\" / \"labels_iclr.npy\")\n",
    "colors_iclr = np.load(variables_path / \"updated_dataset\" / \"colors_iclr.npy\")\n",
    "\n",
    "pickle_in = open(\n",
    "    variables_path / \"updated_dataset\" / \"dict_label_to_color.pkl\", \"rb\"\n",
    ")\n",
    "dict_label_to_color = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['unlabeled', 'unlabeled', 'optimization', ..., 'unlabeled',\n",
       "       'unlabeled', 'federated learning'], dtype='<U34')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "print(len(np.unique(labels_iclr)))\n",
    "labels_iclr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"BERT\",\n",
    "    \"MPNet\",\n",
    "    # \"SBERT\",\n",
    "    # \"SciBERT\",\n",
    "    # \"SPECTER\",\n",
    "    # \"SciNCL\",\n",
    "]\n",
    "\n",
    "\n",
    "model_paths = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"microsoft/mpnet-base\",\n",
    "    # \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    # \"allenai/scibert_scivocab_uncased\",\n",
    "    # \"allenai/specter\",\n",
    "    # \"malteos/scincl\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set the random seed\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.bias', 'mpnet.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/mpnet-base\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "i = 1\n",
    "\n",
    "# random_state = random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on device: {}\".format(device))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_paths[i])\n",
    "model = AutoModel.from_pretrained(model_paths[i])\n",
    "print(model_paths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight: True\n",
      "embeddings.position_embeddings.weight: True\n",
      "embeddings.LayerNorm.weight: True\n",
      "embeddings.LayerNorm.bias: True\n",
      "encoder.layer.0.attention.attn.q.weight: True\n",
      "encoder.layer.0.attention.attn.q.bias: True\n",
      "encoder.layer.0.attention.attn.k.weight: True\n",
      "encoder.layer.0.attention.attn.k.bias: True\n",
      "encoder.layer.0.attention.attn.v.weight: True\n",
      "encoder.layer.0.attention.attn.v.bias: True\n",
      "encoder.layer.0.attention.attn.o.weight: True\n",
      "encoder.layer.0.attention.attn.o.bias: True\n",
      "encoder.layer.0.attention.LayerNorm.weight: True\n",
      "encoder.layer.0.attention.LayerNorm.bias: True\n",
      "encoder.layer.0.intermediate.dense.weight: True\n",
      "encoder.layer.0.intermediate.dense.bias: True\n",
      "encoder.layer.0.output.dense.weight: True\n",
      "encoder.layer.0.output.dense.bias: True\n",
      "encoder.layer.0.output.LayerNorm.weight: True\n",
      "encoder.layer.0.output.LayerNorm.bias: True\n",
      "encoder.layer.1.attention.attn.q.weight: True\n",
      "encoder.layer.1.attention.attn.q.bias: True\n",
      "encoder.layer.1.attention.attn.k.weight: True\n",
      "encoder.layer.1.attention.attn.k.bias: True\n",
      "encoder.layer.1.attention.attn.v.weight: True\n",
      "encoder.layer.1.attention.attn.v.bias: True\n",
      "encoder.layer.1.attention.attn.o.weight: True\n",
      "encoder.layer.1.attention.attn.o.bias: True\n",
      "encoder.layer.1.attention.LayerNorm.weight: True\n",
      "encoder.layer.1.attention.LayerNorm.bias: True\n",
      "encoder.layer.1.intermediate.dense.weight: True\n",
      "encoder.layer.1.intermediate.dense.bias: True\n",
      "encoder.layer.1.output.dense.weight: True\n",
      "encoder.layer.1.output.dense.bias: True\n",
      "encoder.layer.1.output.LayerNorm.weight: True\n",
      "encoder.layer.1.output.LayerNorm.bias: True\n",
      "encoder.layer.2.attention.attn.q.weight: True\n",
      "encoder.layer.2.attention.attn.q.bias: True\n",
      "encoder.layer.2.attention.attn.k.weight: True\n",
      "encoder.layer.2.attention.attn.k.bias: True\n",
      "encoder.layer.2.attention.attn.v.weight: True\n",
      "encoder.layer.2.attention.attn.v.bias: True\n",
      "encoder.layer.2.attention.attn.o.weight: True\n",
      "encoder.layer.2.attention.attn.o.bias: True\n",
      "encoder.layer.2.attention.LayerNorm.weight: True\n",
      "encoder.layer.2.attention.LayerNorm.bias: True\n",
      "encoder.layer.2.intermediate.dense.weight: True\n",
      "encoder.layer.2.intermediate.dense.bias: True\n",
      "encoder.layer.2.output.dense.weight: True\n",
      "encoder.layer.2.output.dense.bias: True\n",
      "encoder.layer.2.output.LayerNorm.weight: True\n",
      "encoder.layer.2.output.LayerNorm.bias: True\n",
      "encoder.layer.3.attention.attn.q.weight: True\n",
      "encoder.layer.3.attention.attn.q.bias: True\n",
      "encoder.layer.3.attention.attn.k.weight: True\n",
      "encoder.layer.3.attention.attn.k.bias: True\n",
      "encoder.layer.3.attention.attn.v.weight: True\n",
      "encoder.layer.3.attention.attn.v.bias: True\n",
      "encoder.layer.3.attention.attn.o.weight: True\n",
      "encoder.layer.3.attention.attn.o.bias: True\n",
      "encoder.layer.3.attention.LayerNorm.weight: True\n",
      "encoder.layer.3.attention.LayerNorm.bias: True\n",
      "encoder.layer.3.intermediate.dense.weight: True\n",
      "encoder.layer.3.intermediate.dense.bias: True\n",
      "encoder.layer.3.output.dense.weight: True\n",
      "encoder.layer.3.output.dense.bias: True\n",
      "encoder.layer.3.output.LayerNorm.weight: True\n",
      "encoder.layer.3.output.LayerNorm.bias: True\n",
      "encoder.layer.4.attention.attn.q.weight: True\n",
      "encoder.layer.4.attention.attn.q.bias: True\n",
      "encoder.layer.4.attention.attn.k.weight: True\n",
      "encoder.layer.4.attention.attn.k.bias: True\n",
      "encoder.layer.4.attention.attn.v.weight: True\n",
      "encoder.layer.4.attention.attn.v.bias: True\n",
      "encoder.layer.4.attention.attn.o.weight: True\n",
      "encoder.layer.4.attention.attn.o.bias: True\n",
      "encoder.layer.4.attention.LayerNorm.weight: True\n",
      "encoder.layer.4.attention.LayerNorm.bias: True\n",
      "encoder.layer.4.intermediate.dense.weight: True\n",
      "encoder.layer.4.intermediate.dense.bias: True\n",
      "encoder.layer.4.output.dense.weight: True\n",
      "encoder.layer.4.output.dense.bias: True\n",
      "encoder.layer.4.output.LayerNorm.weight: True\n",
      "encoder.layer.4.output.LayerNorm.bias: True\n",
      "encoder.layer.5.attention.attn.q.weight: True\n",
      "encoder.layer.5.attention.attn.q.bias: True\n",
      "encoder.layer.5.attention.attn.k.weight: True\n",
      "encoder.layer.5.attention.attn.k.bias: True\n",
      "encoder.layer.5.attention.attn.v.weight: True\n",
      "encoder.layer.5.attention.attn.v.bias: True\n",
      "encoder.layer.5.attention.attn.o.weight: True\n",
      "encoder.layer.5.attention.attn.o.bias: True\n",
      "encoder.layer.5.attention.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.attn.q.weight: True\n",
      "encoder.layer.6.attention.attn.q.bias: True\n",
      "encoder.layer.6.attention.attn.k.weight: True\n",
      "encoder.layer.6.attention.attn.k.bias: True\n",
      "encoder.layer.6.attention.attn.v.weight: True\n",
      "encoder.layer.6.attention.attn.v.bias: True\n",
      "encoder.layer.6.attention.attn.o.weight: True\n",
      "encoder.layer.6.attention.attn.o.bias: True\n",
      "encoder.layer.6.attention.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.attn.q.weight: True\n",
      "encoder.layer.7.attention.attn.q.bias: True\n",
      "encoder.layer.7.attention.attn.k.weight: True\n",
      "encoder.layer.7.attention.attn.k.bias: True\n",
      "encoder.layer.7.attention.attn.v.weight: True\n",
      "encoder.layer.7.attention.attn.v.bias: True\n",
      "encoder.layer.7.attention.attn.o.weight: True\n",
      "encoder.layer.7.attention.attn.o.bias: True\n",
      "encoder.layer.7.attention.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.attn.q.weight: True\n",
      "encoder.layer.8.attention.attn.q.bias: True\n",
      "encoder.layer.8.attention.attn.k.weight: True\n",
      "encoder.layer.8.attention.attn.k.bias: True\n",
      "encoder.layer.8.attention.attn.v.weight: True\n",
      "encoder.layer.8.attention.attn.v.bias: True\n",
      "encoder.layer.8.attention.attn.o.weight: True\n",
      "encoder.layer.8.attention.attn.o.bias: True\n",
      "encoder.layer.8.attention.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.attn.q.weight: True\n",
      "encoder.layer.9.attention.attn.q.bias: True\n",
      "encoder.layer.9.attention.attn.k.weight: True\n",
      "encoder.layer.9.attention.attn.k.bias: True\n",
      "encoder.layer.9.attention.attn.v.weight: True\n",
      "encoder.layer.9.attention.attn.v.bias: True\n",
      "encoder.layer.9.attention.attn.o.weight: True\n",
      "encoder.layer.9.attention.attn.o.bias: True\n",
      "encoder.layer.9.attention.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.attn.q.weight: True\n",
      "encoder.layer.10.attention.attn.q.bias: True\n",
      "encoder.layer.10.attention.attn.k.weight: True\n",
      "encoder.layer.10.attention.attn.k.bias: True\n",
      "encoder.layer.10.attention.attn.v.weight: True\n",
      "encoder.layer.10.attention.attn.v.bias: True\n",
      "encoder.layer.10.attention.attn.o.weight: True\n",
      "encoder.layer.10.attention.attn.o.bias: True\n",
      "encoder.layer.10.attention.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.attn.q.weight: True\n",
      "encoder.layer.11.attention.attn.q.bias: True\n",
      "encoder.layer.11.attention.attn.k.weight: True\n",
      "encoder.layer.11.attention.attn.k.bias: True\n",
      "encoder.layer.11.attention.attn.v.weight: True\n",
      "encoder.layer.11.attention.attn.v.bias: True\n",
      "encoder.layer.11.attention.attn.o.weight: True\n",
      "encoder.layer.11.attention.attn.o.bias: True\n",
      "encoder.layer.11.attention.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "encoder.relative_attention_bias.weight: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training -------------------\n",
      "embeddings.word_embeddings.weight: False\n",
      "embeddings.position_embeddings.weight: False\n",
      "embeddings.LayerNorm.weight: False\n",
      "embeddings.LayerNorm.bias: False\n",
      "encoder.layer.0.attention.attn.q.weight: False\n",
      "encoder.layer.0.attention.attn.q.bias: False\n",
      "encoder.layer.0.attention.attn.k.weight: False\n",
      "encoder.layer.0.attention.attn.k.bias: False\n",
      "encoder.layer.0.attention.attn.v.weight: False\n",
      "encoder.layer.0.attention.attn.v.bias: False\n",
      "encoder.layer.0.attention.attn.o.weight: False\n",
      "encoder.layer.0.attention.attn.o.bias: False\n",
      "encoder.layer.0.attention.LayerNorm.weight: False\n",
      "encoder.layer.0.attention.LayerNorm.bias: False\n",
      "encoder.layer.0.intermediate.dense.weight: False\n",
      "encoder.layer.0.intermediate.dense.bias: False\n",
      "encoder.layer.0.output.dense.weight: False\n",
      "encoder.layer.0.output.dense.bias: False\n",
      "encoder.layer.0.output.LayerNorm.weight: False\n",
      "encoder.layer.0.output.LayerNorm.bias: False\n",
      "encoder.layer.1.attention.attn.q.weight: False\n",
      "encoder.layer.1.attention.attn.q.bias: False\n",
      "encoder.layer.1.attention.attn.k.weight: False\n",
      "encoder.layer.1.attention.attn.k.bias: False\n",
      "encoder.layer.1.attention.attn.v.weight: False\n",
      "encoder.layer.1.attention.attn.v.bias: False\n",
      "encoder.layer.1.attention.attn.o.weight: False\n",
      "encoder.layer.1.attention.attn.o.bias: False\n",
      "encoder.layer.1.attention.LayerNorm.weight: False\n",
      "encoder.layer.1.attention.LayerNorm.bias: False\n",
      "encoder.layer.1.intermediate.dense.weight: False\n",
      "encoder.layer.1.intermediate.dense.bias: False\n",
      "encoder.layer.1.output.dense.weight: False\n",
      "encoder.layer.1.output.dense.bias: False\n",
      "encoder.layer.1.output.LayerNorm.weight: False\n",
      "encoder.layer.1.output.LayerNorm.bias: False\n",
      "encoder.layer.2.attention.attn.q.weight: False\n",
      "encoder.layer.2.attention.attn.q.bias: False\n",
      "encoder.layer.2.attention.attn.k.weight: False\n",
      "encoder.layer.2.attention.attn.k.bias: False\n",
      "encoder.layer.2.attention.attn.v.weight: False\n",
      "encoder.layer.2.attention.attn.v.bias: False\n",
      "encoder.layer.2.attention.attn.o.weight: False\n",
      "encoder.layer.2.attention.attn.o.bias: False\n",
      "encoder.layer.2.attention.LayerNorm.weight: False\n",
      "encoder.layer.2.attention.LayerNorm.bias: False\n",
      "encoder.layer.2.intermediate.dense.weight: False\n",
      "encoder.layer.2.intermediate.dense.bias: False\n",
      "encoder.layer.2.output.dense.weight: False\n",
      "encoder.layer.2.output.dense.bias: False\n",
      "encoder.layer.2.output.LayerNorm.weight: False\n",
      "encoder.layer.2.output.LayerNorm.bias: False\n",
      "encoder.layer.3.attention.attn.q.weight: False\n",
      "encoder.layer.3.attention.attn.q.bias: False\n",
      "encoder.layer.3.attention.attn.k.weight: False\n",
      "encoder.layer.3.attention.attn.k.bias: False\n",
      "encoder.layer.3.attention.attn.v.weight: False\n",
      "encoder.layer.3.attention.attn.v.bias: False\n",
      "encoder.layer.3.attention.attn.o.weight: False\n",
      "encoder.layer.3.attention.attn.o.bias: False\n",
      "encoder.layer.3.attention.LayerNorm.weight: False\n",
      "encoder.layer.3.attention.LayerNorm.bias: False\n",
      "encoder.layer.3.intermediate.dense.weight: False\n",
      "encoder.layer.3.intermediate.dense.bias: False\n",
      "encoder.layer.3.output.dense.weight: False\n",
      "encoder.layer.3.output.dense.bias: False\n",
      "encoder.layer.3.output.LayerNorm.weight: False\n",
      "encoder.layer.3.output.LayerNorm.bias: False\n",
      "encoder.layer.4.attention.attn.q.weight: False\n",
      "encoder.layer.4.attention.attn.q.bias: False\n",
      "encoder.layer.4.attention.attn.k.weight: False\n",
      "encoder.layer.4.attention.attn.k.bias: False\n",
      "encoder.layer.4.attention.attn.v.weight: False\n",
      "encoder.layer.4.attention.attn.v.bias: False\n",
      "encoder.layer.4.attention.attn.o.weight: False\n",
      "encoder.layer.4.attention.attn.o.bias: False\n",
      "encoder.layer.4.attention.LayerNorm.weight: False\n",
      "encoder.layer.4.attention.LayerNorm.bias: False\n",
      "encoder.layer.4.intermediate.dense.weight: False\n",
      "encoder.layer.4.intermediate.dense.bias: False\n",
      "encoder.layer.4.output.dense.weight: False\n",
      "encoder.layer.4.output.dense.bias: False\n",
      "encoder.layer.4.output.LayerNorm.weight: False\n",
      "encoder.layer.4.output.LayerNorm.bias: False\n",
      "encoder.layer.5.attention.attn.q.weight: False\n",
      "encoder.layer.5.attention.attn.q.bias: False\n",
      "encoder.layer.5.attention.attn.k.weight: False\n",
      "encoder.layer.5.attention.attn.k.bias: False\n",
      "encoder.layer.5.attention.attn.v.weight: False\n",
      "encoder.layer.5.attention.attn.v.bias: False\n",
      "encoder.layer.5.attention.attn.o.weight: False\n",
      "encoder.layer.5.attention.attn.o.bias: False\n",
      "encoder.layer.5.attention.LayerNorm.weight: False\n",
      "encoder.layer.5.attention.LayerNorm.bias: False\n",
      "encoder.layer.5.intermediate.dense.weight: False\n",
      "encoder.layer.5.intermediate.dense.bias: False\n",
      "encoder.layer.5.output.dense.weight: False\n",
      "encoder.layer.5.output.dense.bias: False\n",
      "encoder.layer.5.output.LayerNorm.weight: False\n",
      "encoder.layer.5.output.LayerNorm.bias: False\n",
      "encoder.layer.6.attention.attn.q.weight: False\n",
      "encoder.layer.6.attention.attn.q.bias: False\n",
      "encoder.layer.6.attention.attn.k.weight: False\n",
      "encoder.layer.6.attention.attn.k.bias: False\n",
      "encoder.layer.6.attention.attn.v.weight: False\n",
      "encoder.layer.6.attention.attn.v.bias: False\n",
      "encoder.layer.6.attention.attn.o.weight: False\n",
      "encoder.layer.6.attention.attn.o.bias: False\n",
      "encoder.layer.6.attention.LayerNorm.weight: False\n",
      "encoder.layer.6.attention.LayerNorm.bias: False\n",
      "encoder.layer.6.intermediate.dense.weight: False\n",
      "encoder.layer.6.intermediate.dense.bias: False\n",
      "encoder.layer.6.output.dense.weight: False\n",
      "encoder.layer.6.output.dense.bias: False\n",
      "encoder.layer.6.output.LayerNorm.weight: False\n",
      "encoder.layer.6.output.LayerNorm.bias: False\n",
      "encoder.layer.7.attention.attn.q.weight: False\n",
      "encoder.layer.7.attention.attn.q.bias: False\n",
      "encoder.layer.7.attention.attn.k.weight: False\n",
      "encoder.layer.7.attention.attn.k.bias: False\n",
      "encoder.layer.7.attention.attn.v.weight: False\n",
      "encoder.layer.7.attention.attn.v.bias: False\n",
      "encoder.layer.7.attention.attn.o.weight: False\n",
      "encoder.layer.7.attention.attn.o.bias: False\n",
      "encoder.layer.7.attention.LayerNorm.weight: False\n",
      "encoder.layer.7.attention.LayerNorm.bias: False\n",
      "encoder.layer.7.intermediate.dense.weight: False\n",
      "encoder.layer.7.intermediate.dense.bias: False\n",
      "encoder.layer.7.output.dense.weight: False\n",
      "encoder.layer.7.output.dense.bias: False\n",
      "encoder.layer.7.output.LayerNorm.weight: False\n",
      "encoder.layer.7.output.LayerNorm.bias: False\n",
      "encoder.layer.8.attention.attn.q.weight: False\n",
      "encoder.layer.8.attention.attn.q.bias: False\n",
      "encoder.layer.8.attention.attn.k.weight: False\n",
      "encoder.layer.8.attention.attn.k.bias: False\n",
      "encoder.layer.8.attention.attn.v.weight: False\n",
      "encoder.layer.8.attention.attn.v.bias: False\n",
      "encoder.layer.8.attention.attn.o.weight: False\n",
      "encoder.layer.8.attention.attn.o.bias: False\n",
      "encoder.layer.8.attention.LayerNorm.weight: False\n",
      "encoder.layer.8.attention.LayerNorm.bias: False\n",
      "encoder.layer.8.intermediate.dense.weight: False\n",
      "encoder.layer.8.intermediate.dense.bias: False\n",
      "encoder.layer.8.output.dense.weight: False\n",
      "encoder.layer.8.output.dense.bias: False\n",
      "encoder.layer.8.output.LayerNorm.weight: False\n",
      "encoder.layer.8.output.LayerNorm.bias: False\n",
      "encoder.layer.9.attention.attn.q.weight: False\n",
      "encoder.layer.9.attention.attn.q.bias: False\n",
      "encoder.layer.9.attention.attn.k.weight: False\n",
      "encoder.layer.9.attention.attn.k.bias: False\n",
      "encoder.layer.9.attention.attn.v.weight: False\n",
      "encoder.layer.9.attention.attn.v.bias: False\n",
      "encoder.layer.9.attention.attn.o.weight: False\n",
      "encoder.layer.9.attention.attn.o.bias: False\n",
      "encoder.layer.9.attention.LayerNorm.weight: False\n",
      "encoder.layer.9.attention.LayerNorm.bias: False\n",
      "encoder.layer.9.intermediate.dense.weight: False\n",
      "encoder.layer.9.intermediate.dense.bias: False\n",
      "encoder.layer.9.output.dense.weight: False\n",
      "encoder.layer.9.output.dense.bias: False\n",
      "encoder.layer.9.output.LayerNorm.weight: False\n",
      "encoder.layer.9.output.LayerNorm.bias: False\n",
      "encoder.layer.10.attention.attn.q.weight: False\n",
      "encoder.layer.10.attention.attn.q.bias: False\n",
      "encoder.layer.10.attention.attn.k.weight: False\n",
      "encoder.layer.10.attention.attn.k.bias: False\n",
      "encoder.layer.10.attention.attn.v.weight: False\n",
      "encoder.layer.10.attention.attn.v.bias: False\n",
      "encoder.layer.10.attention.attn.o.weight: False\n",
      "encoder.layer.10.attention.attn.o.bias: False\n",
      "encoder.layer.10.attention.LayerNorm.weight: False\n",
      "encoder.layer.10.attention.LayerNorm.bias: False\n",
      "encoder.layer.10.intermediate.dense.weight: False\n",
      "encoder.layer.10.intermediate.dense.bias: False\n",
      "encoder.layer.10.output.dense.weight: False\n",
      "encoder.layer.10.output.dense.bias: False\n",
      "encoder.layer.10.output.LayerNorm.weight: False\n",
      "encoder.layer.10.output.LayerNorm.bias: False\n",
      "encoder.layer.11.attention.attn.q.weight: False\n",
      "encoder.layer.11.attention.attn.q.bias: False\n",
      "encoder.layer.11.attention.attn.k.weight: False\n",
      "encoder.layer.11.attention.attn.k.bias: False\n",
      "encoder.layer.11.attention.attn.v.weight: False\n",
      "encoder.layer.11.attention.attn.v.bias: False\n",
      "encoder.layer.11.attention.attn.o.weight: False\n",
      "encoder.layer.11.attention.attn.o.bias: False\n",
      "encoder.layer.11.attention.LayerNorm.weight: False\n",
      "encoder.layer.11.attention.LayerNorm.bias: False\n",
      "encoder.layer.11.intermediate.dense.weight: False\n",
      "encoder.layer.11.intermediate.dense.bias: False\n",
      "encoder.layer.11.output.dense.weight: False\n",
      "encoder.layer.11.output.dense.bias: False\n",
      "encoder.layer.11.output.LayerNorm.weight: False\n",
      "encoder.layer.11.output.LayerNorm.bias: False\n",
      "encoder.relative_attention_bias.weight: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n"
     ]
    }
   ],
   "source": [
    "# CHECK\n",
    "# for last_frozen_layer in np.arange(\n",
    "#     2\n",
    "# ):  # layers start at 0, so layer 11 in the model output corresponds to 12, and layer 11 of the code model won't be frozen in this case\n",
    "first_unfrozen_layer = 12\n",
    "modules = [\n",
    "    model.embeddings,\n",
    "    *model.encoder.layer[:first_unfrozen_layer],\n",
    "]\n",
    "for module in modules:\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "# check that you actually froze the layer\n",
    "print(\"Before training -------------------\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New freezing experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"BERT\",\n",
    "    \"MPNet\",\n",
    "    # \"SBERT\",\n",
    "    # \"SciBERT\",\n",
    "    # \"SPECTER\",\n",
    "    # \"SciNCL\",\n",
    "]\n",
    "\n",
    "\n",
    "model_paths = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"microsoft/mpnet-base\",\n",
    "    # \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    # \"allenai/scibert_scivocab_uncased\",\n",
    "    # \"allenai/specter\",\n",
    "    # \"malteos/scincl\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First unfrozen layer: 0\n",
      "Model :  MPNet\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/mpnet-base\n",
      "Before training -------------------\n",
      "embeddings.word_embeddings.weight: False\n",
      "embeddings.position_embeddings.weight: False\n",
      "embeddings.LayerNorm.weight: False\n",
      "embeddings.LayerNorm.bias: False\n",
      "encoder.layer.0.attention.attn.q.weight: True\n",
      "encoder.layer.0.attention.attn.q.bias: True\n",
      "encoder.layer.0.attention.attn.k.weight: True\n",
      "encoder.layer.0.attention.attn.k.bias: True\n",
      "encoder.layer.0.attention.attn.v.weight: True\n",
      "encoder.layer.0.attention.attn.v.bias: True\n",
      "encoder.layer.0.attention.attn.o.weight: True\n",
      "encoder.layer.0.attention.attn.o.bias: True\n",
      "encoder.layer.0.attention.LayerNorm.weight: True\n",
      "encoder.layer.0.attention.LayerNorm.bias: True\n",
      "encoder.layer.0.intermediate.dense.weight: True\n",
      "encoder.layer.0.intermediate.dense.bias: True\n",
      "encoder.layer.0.output.dense.weight: True\n",
      "encoder.layer.0.output.dense.bias: True\n",
      "encoder.layer.0.output.LayerNorm.weight: True\n",
      "encoder.layer.0.output.LayerNorm.bias: True\n",
      "encoder.layer.1.attention.attn.q.weight: True\n",
      "encoder.layer.1.attention.attn.q.bias: True\n",
      "encoder.layer.1.attention.attn.k.weight: True\n",
      "encoder.layer.1.attention.attn.k.bias: True\n",
      "encoder.layer.1.attention.attn.v.weight: True\n",
      "encoder.layer.1.attention.attn.v.bias: True\n",
      "encoder.layer.1.attention.attn.o.weight: True\n",
      "encoder.layer.1.attention.attn.o.bias: True\n",
      "encoder.layer.1.attention.LayerNorm.weight: True\n",
      "encoder.layer.1.attention.LayerNorm.bias: True\n",
      "encoder.layer.1.intermediate.dense.weight: True\n",
      "encoder.layer.1.intermediate.dense.bias: True\n",
      "encoder.layer.1.output.dense.weight: True\n",
      "encoder.layer.1.output.dense.bias: True\n",
      "encoder.layer.1.output.LayerNorm.weight: True\n",
      "encoder.layer.1.output.LayerNorm.bias: True\n",
      "encoder.layer.2.attention.attn.q.weight: True\n",
      "encoder.layer.2.attention.attn.q.bias: True\n",
      "encoder.layer.2.attention.attn.k.weight: True\n",
      "encoder.layer.2.attention.attn.k.bias: True\n",
      "encoder.layer.2.attention.attn.v.weight: True\n",
      "encoder.layer.2.attention.attn.v.bias: True\n",
      "encoder.layer.2.attention.attn.o.weight: True\n",
      "encoder.layer.2.attention.attn.o.bias: True\n",
      "encoder.layer.2.attention.LayerNorm.weight: True\n",
      "encoder.layer.2.attention.LayerNorm.bias: True\n",
      "encoder.layer.2.intermediate.dense.weight: True\n",
      "encoder.layer.2.intermediate.dense.bias: True\n",
      "encoder.layer.2.output.dense.weight: True\n",
      "encoder.layer.2.output.dense.bias: True\n",
      "encoder.layer.2.output.LayerNorm.weight: True\n",
      "encoder.layer.2.output.LayerNorm.bias: True\n",
      "encoder.layer.3.attention.attn.q.weight: True\n",
      "encoder.layer.3.attention.attn.q.bias: True\n",
      "encoder.layer.3.attention.attn.k.weight: True\n",
      "encoder.layer.3.attention.attn.k.bias: True\n",
      "encoder.layer.3.attention.attn.v.weight: True\n",
      "encoder.layer.3.attention.attn.v.bias: True\n",
      "encoder.layer.3.attention.attn.o.weight: True\n",
      "encoder.layer.3.attention.attn.o.bias: True\n",
      "encoder.layer.3.attention.LayerNorm.weight: True\n",
      "encoder.layer.3.attention.LayerNorm.bias: True\n",
      "encoder.layer.3.intermediate.dense.weight: True\n",
      "encoder.layer.3.intermediate.dense.bias: True\n",
      "encoder.layer.3.output.dense.weight: True\n",
      "encoder.layer.3.output.dense.bias: True\n",
      "encoder.layer.3.output.LayerNorm.weight: True\n",
      "encoder.layer.3.output.LayerNorm.bias: True\n",
      "encoder.layer.4.attention.attn.q.weight: True\n",
      "encoder.layer.4.attention.attn.q.bias: True\n",
      "encoder.layer.4.attention.attn.k.weight: True\n",
      "encoder.layer.4.attention.attn.k.bias: True\n",
      "encoder.layer.4.attention.attn.v.weight: True\n",
      "encoder.layer.4.attention.attn.v.bias: True\n",
      "encoder.layer.4.attention.attn.o.weight: True\n",
      "encoder.layer.4.attention.attn.o.bias: True\n",
      "encoder.layer.4.attention.LayerNorm.weight: True\n",
      "encoder.layer.4.attention.LayerNorm.bias: True\n",
      "encoder.layer.4.intermediate.dense.weight: True\n",
      "encoder.layer.4.intermediate.dense.bias: True\n",
      "encoder.layer.4.output.dense.weight: True\n",
      "encoder.layer.4.output.dense.bias: True\n",
      "encoder.layer.4.output.LayerNorm.weight: True\n",
      "encoder.layer.4.output.LayerNorm.bias: True\n",
      "encoder.layer.5.attention.attn.q.weight: True\n",
      "encoder.layer.5.attention.attn.q.bias: True\n",
      "encoder.layer.5.attention.attn.k.weight: True\n",
      "encoder.layer.5.attention.attn.k.bias: True\n",
      "encoder.layer.5.attention.attn.v.weight: True\n",
      "encoder.layer.5.attention.attn.v.bias: True\n",
      "encoder.layer.5.attention.attn.o.weight: True\n",
      "encoder.layer.5.attention.attn.o.bias: True\n",
      "encoder.layer.5.attention.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.attn.q.weight: True\n",
      "encoder.layer.6.attention.attn.q.bias: True\n",
      "encoder.layer.6.attention.attn.k.weight: True\n",
      "encoder.layer.6.attention.attn.k.bias: True\n",
      "encoder.layer.6.attention.attn.v.weight: True\n",
      "encoder.layer.6.attention.attn.v.bias: True\n",
      "encoder.layer.6.attention.attn.o.weight: True\n",
      "encoder.layer.6.attention.attn.o.bias: True\n",
      "encoder.layer.6.attention.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.attn.q.weight: True\n",
      "encoder.layer.7.attention.attn.q.bias: True\n",
      "encoder.layer.7.attention.attn.k.weight: True\n",
      "encoder.layer.7.attention.attn.k.bias: True\n",
      "encoder.layer.7.attention.attn.v.weight: True\n",
      "encoder.layer.7.attention.attn.v.bias: True\n",
      "encoder.layer.7.attention.attn.o.weight: True\n",
      "encoder.layer.7.attention.attn.o.bias: True\n",
      "encoder.layer.7.attention.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.attn.q.weight: True\n",
      "encoder.layer.8.attention.attn.q.bias: True\n",
      "encoder.layer.8.attention.attn.k.weight: True\n",
      "encoder.layer.8.attention.attn.k.bias: True\n",
      "encoder.layer.8.attention.attn.v.weight: True\n",
      "encoder.layer.8.attention.attn.v.bias: True\n",
      "encoder.layer.8.attention.attn.o.weight: True\n",
      "encoder.layer.8.attention.attn.o.bias: True\n",
      "encoder.layer.8.attention.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.attn.q.weight: True\n",
      "encoder.layer.9.attention.attn.q.bias: True\n",
      "encoder.layer.9.attention.attn.k.weight: True\n",
      "encoder.layer.9.attention.attn.k.bias: True\n",
      "encoder.layer.9.attention.attn.v.weight: True\n",
      "encoder.layer.9.attention.attn.v.bias: True\n",
      "encoder.layer.9.attention.attn.o.weight: True\n",
      "encoder.layer.9.attention.attn.o.bias: True\n",
      "encoder.layer.9.attention.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.attn.q.weight: True\n",
      "encoder.layer.10.attention.attn.q.bias: True\n",
      "encoder.layer.10.attention.attn.k.weight: True\n",
      "encoder.layer.10.attention.attn.k.bias: True\n",
      "encoder.layer.10.attention.attn.v.weight: True\n",
      "encoder.layer.10.attention.attn.v.bias: True\n",
      "encoder.layer.10.attention.attn.o.weight: True\n",
      "encoder.layer.10.attention.attn.o.bias: True\n",
      "encoder.layer.10.attention.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.attn.q.weight: True\n",
      "encoder.layer.11.attention.attn.q.bias: True\n",
      "encoder.layer.11.attention.attn.k.weight: True\n",
      "encoder.layer.11.attention.attn.k.bias: True\n",
      "encoder.layer.11.attention.attn.v.weight: True\n",
      "encoder.layer.11.attention.attn.v.bias: True\n",
      "encoder.layer.11.attention.attn.o.weight: True\n",
      "encoder.layer.11.attention.attn.o.bias: True\n",
      "encoder.layer.11.attention.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "encoder.relative_attention_bias.weight: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b805342a5a24a9187258fe9512a1f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700654cf912443139b94a769f736e8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First unfrozen layer: 1\n",
      "Model :  MPNet\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/mpnet-base\n",
      "Before training -------------------\n",
      "embeddings.word_embeddings.weight: False\n",
      "embeddings.position_embeddings.weight: False\n",
      "embeddings.LayerNorm.weight: False\n",
      "embeddings.LayerNorm.bias: False\n",
      "encoder.layer.0.attention.attn.q.weight: False\n",
      "encoder.layer.0.attention.attn.q.bias: False\n",
      "encoder.layer.0.attention.attn.k.weight: False\n",
      "encoder.layer.0.attention.attn.k.bias: False\n",
      "encoder.layer.0.attention.attn.v.weight: False\n",
      "encoder.layer.0.attention.attn.v.bias: False\n",
      "encoder.layer.0.attention.attn.o.weight: False\n",
      "encoder.layer.0.attention.attn.o.bias: False\n",
      "encoder.layer.0.attention.LayerNorm.weight: False\n",
      "encoder.layer.0.attention.LayerNorm.bias: False\n",
      "encoder.layer.0.intermediate.dense.weight: False\n",
      "encoder.layer.0.intermediate.dense.bias: False\n",
      "encoder.layer.0.output.dense.weight: False\n",
      "encoder.layer.0.output.dense.bias: False\n",
      "encoder.layer.0.output.LayerNorm.weight: False\n",
      "encoder.layer.0.output.LayerNorm.bias: False\n",
      "encoder.layer.1.attention.attn.q.weight: True\n",
      "encoder.layer.1.attention.attn.q.bias: True\n",
      "encoder.layer.1.attention.attn.k.weight: True\n",
      "encoder.layer.1.attention.attn.k.bias: True\n",
      "encoder.layer.1.attention.attn.v.weight: True\n",
      "encoder.layer.1.attention.attn.v.bias: True\n",
      "encoder.layer.1.attention.attn.o.weight: True\n",
      "encoder.layer.1.attention.attn.o.bias: True\n",
      "encoder.layer.1.attention.LayerNorm.weight: True\n",
      "encoder.layer.1.attention.LayerNorm.bias: True\n",
      "encoder.layer.1.intermediate.dense.weight: True\n",
      "encoder.layer.1.intermediate.dense.bias: True\n",
      "encoder.layer.1.output.dense.weight: True\n",
      "encoder.layer.1.output.dense.bias: True\n",
      "encoder.layer.1.output.LayerNorm.weight: True\n",
      "encoder.layer.1.output.LayerNorm.bias: True\n",
      "encoder.layer.2.attention.attn.q.weight: True\n",
      "encoder.layer.2.attention.attn.q.bias: True\n",
      "encoder.layer.2.attention.attn.k.weight: True\n",
      "encoder.layer.2.attention.attn.k.bias: True\n",
      "encoder.layer.2.attention.attn.v.weight: True\n",
      "encoder.layer.2.attention.attn.v.bias: True\n",
      "encoder.layer.2.attention.attn.o.weight: True\n",
      "encoder.layer.2.attention.attn.o.bias: True\n",
      "encoder.layer.2.attention.LayerNorm.weight: True\n",
      "encoder.layer.2.attention.LayerNorm.bias: True\n",
      "encoder.layer.2.intermediate.dense.weight: True\n",
      "encoder.layer.2.intermediate.dense.bias: True\n",
      "encoder.layer.2.output.dense.weight: True\n",
      "encoder.layer.2.output.dense.bias: True\n",
      "encoder.layer.2.output.LayerNorm.weight: True\n",
      "encoder.layer.2.output.LayerNorm.bias: True\n",
      "encoder.layer.3.attention.attn.q.weight: True\n",
      "encoder.layer.3.attention.attn.q.bias: True\n",
      "encoder.layer.3.attention.attn.k.weight: True\n",
      "encoder.layer.3.attention.attn.k.bias: True\n",
      "encoder.layer.3.attention.attn.v.weight: True\n",
      "encoder.layer.3.attention.attn.v.bias: True\n",
      "encoder.layer.3.attention.attn.o.weight: True\n",
      "encoder.layer.3.attention.attn.o.bias: True\n",
      "encoder.layer.3.attention.LayerNorm.weight: True\n",
      "encoder.layer.3.attention.LayerNorm.bias: True\n",
      "encoder.layer.3.intermediate.dense.weight: True\n",
      "encoder.layer.3.intermediate.dense.bias: True\n",
      "encoder.layer.3.output.dense.weight: True\n",
      "encoder.layer.3.output.dense.bias: True\n",
      "encoder.layer.3.output.LayerNorm.weight: True\n",
      "encoder.layer.3.output.LayerNorm.bias: True\n",
      "encoder.layer.4.attention.attn.q.weight: True\n",
      "encoder.layer.4.attention.attn.q.bias: True\n",
      "encoder.layer.4.attention.attn.k.weight: True\n",
      "encoder.layer.4.attention.attn.k.bias: True\n",
      "encoder.layer.4.attention.attn.v.weight: True\n",
      "encoder.layer.4.attention.attn.v.bias: True\n",
      "encoder.layer.4.attention.attn.o.weight: True\n",
      "encoder.layer.4.attention.attn.o.bias: True\n",
      "encoder.layer.4.attention.LayerNorm.weight: True\n",
      "encoder.layer.4.attention.LayerNorm.bias: True\n",
      "encoder.layer.4.intermediate.dense.weight: True\n",
      "encoder.layer.4.intermediate.dense.bias: True\n",
      "encoder.layer.4.output.dense.weight: True\n",
      "encoder.layer.4.output.dense.bias: True\n",
      "encoder.layer.4.output.LayerNorm.weight: True\n",
      "encoder.layer.4.output.LayerNorm.bias: True\n",
      "encoder.layer.5.attention.attn.q.weight: True\n",
      "encoder.layer.5.attention.attn.q.bias: True\n",
      "encoder.layer.5.attention.attn.k.weight: True\n",
      "encoder.layer.5.attention.attn.k.bias: True\n",
      "encoder.layer.5.attention.attn.v.weight: True\n",
      "encoder.layer.5.attention.attn.v.bias: True\n",
      "encoder.layer.5.attention.attn.o.weight: True\n",
      "encoder.layer.5.attention.attn.o.bias: True\n",
      "encoder.layer.5.attention.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.attn.q.weight: True\n",
      "encoder.layer.6.attention.attn.q.bias: True\n",
      "encoder.layer.6.attention.attn.k.weight: True\n",
      "encoder.layer.6.attention.attn.k.bias: True\n",
      "encoder.layer.6.attention.attn.v.weight: True\n",
      "encoder.layer.6.attention.attn.v.bias: True\n",
      "encoder.layer.6.attention.attn.o.weight: True\n",
      "encoder.layer.6.attention.attn.o.bias: True\n",
      "encoder.layer.6.attention.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.attn.q.weight: True\n",
      "encoder.layer.7.attention.attn.q.bias: True\n",
      "encoder.layer.7.attention.attn.k.weight: True\n",
      "encoder.layer.7.attention.attn.k.bias: True\n",
      "encoder.layer.7.attention.attn.v.weight: True\n",
      "encoder.layer.7.attention.attn.v.bias: True\n",
      "encoder.layer.7.attention.attn.o.weight: True\n",
      "encoder.layer.7.attention.attn.o.bias: True\n",
      "encoder.layer.7.attention.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.attn.q.weight: True\n",
      "encoder.layer.8.attention.attn.q.bias: True\n",
      "encoder.layer.8.attention.attn.k.weight: True\n",
      "encoder.layer.8.attention.attn.k.bias: True\n",
      "encoder.layer.8.attention.attn.v.weight: True\n",
      "encoder.layer.8.attention.attn.v.bias: True\n",
      "encoder.layer.8.attention.attn.o.weight: True\n",
      "encoder.layer.8.attention.attn.o.bias: True\n",
      "encoder.layer.8.attention.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.attn.q.weight: True\n",
      "encoder.layer.9.attention.attn.q.bias: True\n",
      "encoder.layer.9.attention.attn.k.weight: True\n",
      "encoder.layer.9.attention.attn.k.bias: True\n",
      "encoder.layer.9.attention.attn.v.weight: True\n",
      "encoder.layer.9.attention.attn.v.bias: True\n",
      "encoder.layer.9.attention.attn.o.weight: True\n",
      "encoder.layer.9.attention.attn.o.bias: True\n",
      "encoder.layer.9.attention.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.attn.q.weight: True\n",
      "encoder.layer.10.attention.attn.q.bias: True\n",
      "encoder.layer.10.attention.attn.k.weight: True\n",
      "encoder.layer.10.attention.attn.k.bias: True\n",
      "encoder.layer.10.attention.attn.v.weight: True\n",
      "encoder.layer.10.attention.attn.v.bias: True\n",
      "encoder.layer.10.attention.attn.o.weight: True\n",
      "encoder.layer.10.attention.attn.o.bias: True\n",
      "encoder.layer.10.attention.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.attn.q.weight: True\n",
      "encoder.layer.11.attention.attn.q.bias: True\n",
      "encoder.layer.11.attention.attn.k.weight: True\n",
      "encoder.layer.11.attention.attn.k.bias: True\n",
      "encoder.layer.11.attention.attn.v.weight: True\n",
      "encoder.layer.11.attention.attn.v.bias: True\n",
      "encoder.layer.11.attention.attn.o.weight: True\n",
      "encoder.layer.11.attention.attn.o.bias: True\n",
      "encoder.layer.11.attention.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "encoder.relative_attention_bias.weight: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f34a6ffbc554442bba49a58442fe08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645913e808a8467494701b145ca74c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First unfrozen layer: 2\n",
      "Model :  MPNet\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/mpnet-base\n",
      "Before training -------------------\n",
      "embeddings.word_embeddings.weight: False\n",
      "embeddings.position_embeddings.weight: False\n",
      "embeddings.LayerNorm.weight: False\n",
      "embeddings.LayerNorm.bias: False\n",
      "encoder.layer.0.attention.attn.q.weight: False\n",
      "encoder.layer.0.attention.attn.q.bias: False\n",
      "encoder.layer.0.attention.attn.k.weight: False\n",
      "encoder.layer.0.attention.attn.k.bias: False\n",
      "encoder.layer.0.attention.attn.v.weight: False\n",
      "encoder.layer.0.attention.attn.v.bias: False\n",
      "encoder.layer.0.attention.attn.o.weight: False\n",
      "encoder.layer.0.attention.attn.o.bias: False\n",
      "encoder.layer.0.attention.LayerNorm.weight: False\n",
      "encoder.layer.0.attention.LayerNorm.bias: False\n",
      "encoder.layer.0.intermediate.dense.weight: False\n",
      "encoder.layer.0.intermediate.dense.bias: False\n",
      "encoder.layer.0.output.dense.weight: False\n",
      "encoder.layer.0.output.dense.bias: False\n",
      "encoder.layer.0.output.LayerNorm.weight: False\n",
      "encoder.layer.0.output.LayerNorm.bias: False\n",
      "encoder.layer.1.attention.attn.q.weight: False\n",
      "encoder.layer.1.attention.attn.q.bias: False\n",
      "encoder.layer.1.attention.attn.k.weight: False\n",
      "encoder.layer.1.attention.attn.k.bias: False\n",
      "encoder.layer.1.attention.attn.v.weight: False\n",
      "encoder.layer.1.attention.attn.v.bias: False\n",
      "encoder.layer.1.attention.attn.o.weight: False\n",
      "encoder.layer.1.attention.attn.o.bias: False\n",
      "encoder.layer.1.attention.LayerNorm.weight: False\n",
      "encoder.layer.1.attention.LayerNorm.bias: False\n",
      "encoder.layer.1.intermediate.dense.weight: False\n",
      "encoder.layer.1.intermediate.dense.bias: False\n",
      "encoder.layer.1.output.dense.weight: False\n",
      "encoder.layer.1.output.dense.bias: False\n",
      "encoder.layer.1.output.LayerNorm.weight: False\n",
      "encoder.layer.1.output.LayerNorm.bias: False\n",
      "encoder.layer.2.attention.attn.q.weight: True\n",
      "encoder.layer.2.attention.attn.q.bias: True\n",
      "encoder.layer.2.attention.attn.k.weight: True\n",
      "encoder.layer.2.attention.attn.k.bias: True\n",
      "encoder.layer.2.attention.attn.v.weight: True\n",
      "encoder.layer.2.attention.attn.v.bias: True\n",
      "encoder.layer.2.attention.attn.o.weight: True\n",
      "encoder.layer.2.attention.attn.o.bias: True\n",
      "encoder.layer.2.attention.LayerNorm.weight: True\n",
      "encoder.layer.2.attention.LayerNorm.bias: True\n",
      "encoder.layer.2.intermediate.dense.weight: True\n",
      "encoder.layer.2.intermediate.dense.bias: True\n",
      "encoder.layer.2.output.dense.weight: True\n",
      "encoder.layer.2.output.dense.bias: True\n",
      "encoder.layer.2.output.LayerNorm.weight: True\n",
      "encoder.layer.2.output.LayerNorm.bias: True\n",
      "encoder.layer.3.attention.attn.q.weight: True\n",
      "encoder.layer.3.attention.attn.q.bias: True\n",
      "encoder.layer.3.attention.attn.k.weight: True\n",
      "encoder.layer.3.attention.attn.k.bias: True\n",
      "encoder.layer.3.attention.attn.v.weight: True\n",
      "encoder.layer.3.attention.attn.v.bias: True\n",
      "encoder.layer.3.attention.attn.o.weight: True\n",
      "encoder.layer.3.attention.attn.o.bias: True\n",
      "encoder.layer.3.attention.LayerNorm.weight: True\n",
      "encoder.layer.3.attention.LayerNorm.bias: True\n",
      "encoder.layer.3.intermediate.dense.weight: True\n",
      "encoder.layer.3.intermediate.dense.bias: True\n",
      "encoder.layer.3.output.dense.weight: True\n",
      "encoder.layer.3.output.dense.bias: True\n",
      "encoder.layer.3.output.LayerNorm.weight: True\n",
      "encoder.layer.3.output.LayerNorm.bias: True\n",
      "encoder.layer.4.attention.attn.q.weight: True\n",
      "encoder.layer.4.attention.attn.q.bias: True\n",
      "encoder.layer.4.attention.attn.k.weight: True\n",
      "encoder.layer.4.attention.attn.k.bias: True\n",
      "encoder.layer.4.attention.attn.v.weight: True\n",
      "encoder.layer.4.attention.attn.v.bias: True\n",
      "encoder.layer.4.attention.attn.o.weight: True\n",
      "encoder.layer.4.attention.attn.o.bias: True\n",
      "encoder.layer.4.attention.LayerNorm.weight: True\n",
      "encoder.layer.4.attention.LayerNorm.bias: True\n",
      "encoder.layer.4.intermediate.dense.weight: True\n",
      "encoder.layer.4.intermediate.dense.bias: True\n",
      "encoder.layer.4.output.dense.weight: True\n",
      "encoder.layer.4.output.dense.bias: True\n",
      "encoder.layer.4.output.LayerNorm.weight: True\n",
      "encoder.layer.4.output.LayerNorm.bias: True\n",
      "encoder.layer.5.attention.attn.q.weight: True\n",
      "encoder.layer.5.attention.attn.q.bias: True\n",
      "encoder.layer.5.attention.attn.k.weight: True\n",
      "encoder.layer.5.attention.attn.k.bias: True\n",
      "encoder.layer.5.attention.attn.v.weight: True\n",
      "encoder.layer.5.attention.attn.v.bias: True\n",
      "encoder.layer.5.attention.attn.o.weight: True\n",
      "encoder.layer.5.attention.attn.o.bias: True\n",
      "encoder.layer.5.attention.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.attn.q.weight: True\n",
      "encoder.layer.6.attention.attn.q.bias: True\n",
      "encoder.layer.6.attention.attn.k.weight: True\n",
      "encoder.layer.6.attention.attn.k.bias: True\n",
      "encoder.layer.6.attention.attn.v.weight: True\n",
      "encoder.layer.6.attention.attn.v.bias: True\n",
      "encoder.layer.6.attention.attn.o.weight: True\n",
      "encoder.layer.6.attention.attn.o.bias: True\n",
      "encoder.layer.6.attention.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.attn.q.weight: True\n",
      "encoder.layer.7.attention.attn.q.bias: True\n",
      "encoder.layer.7.attention.attn.k.weight: True\n",
      "encoder.layer.7.attention.attn.k.bias: True\n",
      "encoder.layer.7.attention.attn.v.weight: True\n",
      "encoder.layer.7.attention.attn.v.bias: True\n",
      "encoder.layer.7.attention.attn.o.weight: True\n",
      "encoder.layer.7.attention.attn.o.bias: True\n",
      "encoder.layer.7.attention.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.attn.q.weight: True\n",
      "encoder.layer.8.attention.attn.q.bias: True\n",
      "encoder.layer.8.attention.attn.k.weight: True\n",
      "encoder.layer.8.attention.attn.k.bias: True\n",
      "encoder.layer.8.attention.attn.v.weight: True\n",
      "encoder.layer.8.attention.attn.v.bias: True\n",
      "encoder.layer.8.attention.attn.o.weight: True\n",
      "encoder.layer.8.attention.attn.o.bias: True\n",
      "encoder.layer.8.attention.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.attn.q.weight: True\n",
      "encoder.layer.9.attention.attn.q.bias: True\n",
      "encoder.layer.9.attention.attn.k.weight: True\n",
      "encoder.layer.9.attention.attn.k.bias: True\n",
      "encoder.layer.9.attention.attn.v.weight: True\n",
      "encoder.layer.9.attention.attn.v.bias: True\n",
      "encoder.layer.9.attention.attn.o.weight: True\n",
      "encoder.layer.9.attention.attn.o.bias: True\n",
      "encoder.layer.9.attention.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.attn.q.weight: True\n",
      "encoder.layer.10.attention.attn.q.bias: True\n",
      "encoder.layer.10.attention.attn.k.weight: True\n",
      "encoder.layer.10.attention.attn.k.bias: True\n",
      "encoder.layer.10.attention.attn.v.weight: True\n",
      "encoder.layer.10.attention.attn.v.bias: True\n",
      "encoder.layer.10.attention.attn.o.weight: True\n",
      "encoder.layer.10.attention.attn.o.bias: True\n",
      "encoder.layer.10.attention.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.attn.q.weight: True\n",
      "encoder.layer.11.attention.attn.q.bias: True\n",
      "encoder.layer.11.attention.attn.k.weight: True\n",
      "encoder.layer.11.attention.attn.k.bias: True\n",
      "encoder.layer.11.attention.attn.v.weight: True\n",
      "encoder.layer.11.attention.attn.v.bias: True\n",
      "encoder.layer.11.attention.attn.o.weight: True\n",
      "encoder.layer.11.attention.attn.o.bias: True\n",
      "encoder.layer.11.attention.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "encoder.relative_attention_bias.weight: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7024da55a390480e815bc271c2067db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b608dc8cf84ec6bc69a7df72a07f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First unfrozen layer: 3\n",
      "Model :  MPNet\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/mpnet-base\n",
      "Before training -------------------\n",
      "embeddings.word_embeddings.weight: False\n",
      "embeddings.position_embeddings.weight: False\n",
      "embeddings.LayerNorm.weight: False\n",
      "embeddings.LayerNorm.bias: False\n",
      "encoder.layer.0.attention.attn.q.weight: False\n",
      "encoder.layer.0.attention.attn.q.bias: False\n",
      "encoder.layer.0.attention.attn.k.weight: False\n",
      "encoder.layer.0.attention.attn.k.bias: False\n",
      "encoder.layer.0.attention.attn.v.weight: False\n",
      "encoder.layer.0.attention.attn.v.bias: False\n",
      "encoder.layer.0.attention.attn.o.weight: False\n",
      "encoder.layer.0.attention.attn.o.bias: False\n",
      "encoder.layer.0.attention.LayerNorm.weight: False\n",
      "encoder.layer.0.attention.LayerNorm.bias: False\n",
      "encoder.layer.0.intermediate.dense.weight: False\n",
      "encoder.layer.0.intermediate.dense.bias: False\n",
      "encoder.layer.0.output.dense.weight: False\n",
      "encoder.layer.0.output.dense.bias: False\n",
      "encoder.layer.0.output.LayerNorm.weight: False\n",
      "encoder.layer.0.output.LayerNorm.bias: False\n",
      "encoder.layer.1.attention.attn.q.weight: False\n",
      "encoder.layer.1.attention.attn.q.bias: False\n",
      "encoder.layer.1.attention.attn.k.weight: False\n",
      "encoder.layer.1.attention.attn.k.bias: False\n",
      "encoder.layer.1.attention.attn.v.weight: False\n",
      "encoder.layer.1.attention.attn.v.bias: False\n",
      "encoder.layer.1.attention.attn.o.weight: False\n",
      "encoder.layer.1.attention.attn.o.bias: False\n",
      "encoder.layer.1.attention.LayerNorm.weight: False\n",
      "encoder.layer.1.attention.LayerNorm.bias: False\n",
      "encoder.layer.1.intermediate.dense.weight: False\n",
      "encoder.layer.1.intermediate.dense.bias: False\n",
      "encoder.layer.1.output.dense.weight: False\n",
      "encoder.layer.1.output.dense.bias: False\n",
      "encoder.layer.1.output.LayerNorm.weight: False\n",
      "encoder.layer.1.output.LayerNorm.bias: False\n",
      "encoder.layer.2.attention.attn.q.weight: False\n",
      "encoder.layer.2.attention.attn.q.bias: False\n",
      "encoder.layer.2.attention.attn.k.weight: False\n",
      "encoder.layer.2.attention.attn.k.bias: False\n",
      "encoder.layer.2.attention.attn.v.weight: False\n",
      "encoder.layer.2.attention.attn.v.bias: False\n",
      "encoder.layer.2.attention.attn.o.weight: False\n",
      "encoder.layer.2.attention.attn.o.bias: False\n",
      "encoder.layer.2.attention.LayerNorm.weight: False\n",
      "encoder.layer.2.attention.LayerNorm.bias: False\n",
      "encoder.layer.2.intermediate.dense.weight: False\n",
      "encoder.layer.2.intermediate.dense.bias: False\n",
      "encoder.layer.2.output.dense.weight: False\n",
      "encoder.layer.2.output.dense.bias: False\n",
      "encoder.layer.2.output.LayerNorm.weight: False\n",
      "encoder.layer.2.output.LayerNorm.bias: False\n",
      "encoder.layer.3.attention.attn.q.weight: True\n",
      "encoder.layer.3.attention.attn.q.bias: True\n",
      "encoder.layer.3.attention.attn.k.weight: True\n",
      "encoder.layer.3.attention.attn.k.bias: True\n",
      "encoder.layer.3.attention.attn.v.weight: True\n",
      "encoder.layer.3.attention.attn.v.bias: True\n",
      "encoder.layer.3.attention.attn.o.weight: True\n",
      "encoder.layer.3.attention.attn.o.bias: True\n",
      "encoder.layer.3.attention.LayerNorm.weight: True\n",
      "encoder.layer.3.attention.LayerNorm.bias: True\n",
      "encoder.layer.3.intermediate.dense.weight: True\n",
      "encoder.layer.3.intermediate.dense.bias: True\n",
      "encoder.layer.3.output.dense.weight: True\n",
      "encoder.layer.3.output.dense.bias: True\n",
      "encoder.layer.3.output.LayerNorm.weight: True\n",
      "encoder.layer.3.output.LayerNorm.bias: True\n",
      "encoder.layer.4.attention.attn.q.weight: True\n",
      "encoder.layer.4.attention.attn.q.bias: True\n",
      "encoder.layer.4.attention.attn.k.weight: True\n",
      "encoder.layer.4.attention.attn.k.bias: True\n",
      "encoder.layer.4.attention.attn.v.weight: True\n",
      "encoder.layer.4.attention.attn.v.bias: True\n",
      "encoder.layer.4.attention.attn.o.weight: True\n",
      "encoder.layer.4.attention.attn.o.bias: True\n",
      "encoder.layer.4.attention.LayerNorm.weight: True\n",
      "encoder.layer.4.attention.LayerNorm.bias: True\n",
      "encoder.layer.4.intermediate.dense.weight: True\n",
      "encoder.layer.4.intermediate.dense.bias: True\n",
      "encoder.layer.4.output.dense.weight: True\n",
      "encoder.layer.4.output.dense.bias: True\n",
      "encoder.layer.4.output.LayerNorm.weight: True\n",
      "encoder.layer.4.output.LayerNorm.bias: True\n",
      "encoder.layer.5.attention.attn.q.weight: True\n",
      "encoder.layer.5.attention.attn.q.bias: True\n",
      "encoder.layer.5.attention.attn.k.weight: True\n",
      "encoder.layer.5.attention.attn.k.bias: True\n",
      "encoder.layer.5.attention.attn.v.weight: True\n",
      "encoder.layer.5.attention.attn.v.bias: True\n",
      "encoder.layer.5.attention.attn.o.weight: True\n",
      "encoder.layer.5.attention.attn.o.bias: True\n",
      "encoder.layer.5.attention.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.attn.q.weight: True\n",
      "encoder.layer.6.attention.attn.q.bias: True\n",
      "encoder.layer.6.attention.attn.k.weight: True\n",
      "encoder.layer.6.attention.attn.k.bias: True\n",
      "encoder.layer.6.attention.attn.v.weight: True\n",
      "encoder.layer.6.attention.attn.v.bias: True\n",
      "encoder.layer.6.attention.attn.o.weight: True\n",
      "encoder.layer.6.attention.attn.o.bias: True\n",
      "encoder.layer.6.attention.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.attn.q.weight: True\n",
      "encoder.layer.7.attention.attn.q.bias: True\n",
      "encoder.layer.7.attention.attn.k.weight: True\n",
      "encoder.layer.7.attention.attn.k.bias: True\n",
      "encoder.layer.7.attention.attn.v.weight: True\n",
      "encoder.layer.7.attention.attn.v.bias: True\n",
      "encoder.layer.7.attention.attn.o.weight: True\n",
      "encoder.layer.7.attention.attn.o.bias: True\n",
      "encoder.layer.7.attention.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.attn.q.weight: True\n",
      "encoder.layer.8.attention.attn.q.bias: True\n",
      "encoder.layer.8.attention.attn.k.weight: True\n",
      "encoder.layer.8.attention.attn.k.bias: True\n",
      "encoder.layer.8.attention.attn.v.weight: True\n",
      "encoder.layer.8.attention.attn.v.bias: True\n",
      "encoder.layer.8.attention.attn.o.weight: True\n",
      "encoder.layer.8.attention.attn.o.bias: True\n",
      "encoder.layer.8.attention.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.attn.q.weight: True\n",
      "encoder.layer.9.attention.attn.q.bias: True\n",
      "encoder.layer.9.attention.attn.k.weight: True\n",
      "encoder.layer.9.attention.attn.k.bias: True\n",
      "encoder.layer.9.attention.attn.v.weight: True\n",
      "encoder.layer.9.attention.attn.v.bias: True\n",
      "encoder.layer.9.attention.attn.o.weight: True\n",
      "encoder.layer.9.attention.attn.o.bias: True\n",
      "encoder.layer.9.attention.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.attn.q.weight: True\n",
      "encoder.layer.10.attention.attn.q.bias: True\n",
      "encoder.layer.10.attention.attn.k.weight: True\n",
      "encoder.layer.10.attention.attn.k.bias: True\n",
      "encoder.layer.10.attention.attn.v.weight: True\n",
      "encoder.layer.10.attention.attn.v.bias: True\n",
      "encoder.layer.10.attention.attn.o.weight: True\n",
      "encoder.layer.10.attention.attn.o.bias: True\n",
      "encoder.layer.10.attention.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.attn.q.weight: True\n",
      "encoder.layer.11.attention.attn.q.bias: True\n",
      "encoder.layer.11.attention.attn.k.weight: True\n",
      "encoder.layer.11.attention.attn.k.bias: True\n",
      "encoder.layer.11.attention.attn.v.weight: True\n",
      "encoder.layer.11.attention.attn.v.bias: True\n",
      "encoder.layer.11.attention.attn.o.weight: True\n",
      "encoder.layer.11.attention.attn.o.bias: True\n",
      "encoder.layer.11.attention.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "encoder.relative_attention_bias.weight: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273e81cc5fc34cbcade74d022cedf3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64280f54531045df964c917df4a48e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First unfrozen layer: 4\n",
      "Model :  MPNet\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/mpnet-base\n",
      "Before training -------------------\n",
      "embeddings.word_embeddings.weight: False\n",
      "embeddings.position_embeddings.weight: False\n",
      "embeddings.LayerNorm.weight: False\n",
      "embeddings.LayerNorm.bias: False\n",
      "encoder.layer.0.attention.attn.q.weight: False\n",
      "encoder.layer.0.attention.attn.q.bias: False\n",
      "encoder.layer.0.attention.attn.k.weight: False\n",
      "encoder.layer.0.attention.attn.k.bias: False\n",
      "encoder.layer.0.attention.attn.v.weight: False\n",
      "encoder.layer.0.attention.attn.v.bias: False\n",
      "encoder.layer.0.attention.attn.o.weight: False\n",
      "encoder.layer.0.attention.attn.o.bias: False\n",
      "encoder.layer.0.attention.LayerNorm.weight: False\n",
      "encoder.layer.0.attention.LayerNorm.bias: False\n",
      "encoder.layer.0.intermediate.dense.weight: False\n",
      "encoder.layer.0.intermediate.dense.bias: False\n",
      "encoder.layer.0.output.dense.weight: False\n",
      "encoder.layer.0.output.dense.bias: False\n",
      "encoder.layer.0.output.LayerNorm.weight: False\n",
      "encoder.layer.0.output.LayerNorm.bias: False\n",
      "encoder.layer.1.attention.attn.q.weight: False\n",
      "encoder.layer.1.attention.attn.q.bias: False\n",
      "encoder.layer.1.attention.attn.k.weight: False\n",
      "encoder.layer.1.attention.attn.k.bias: False\n",
      "encoder.layer.1.attention.attn.v.weight: False\n",
      "encoder.layer.1.attention.attn.v.bias: False\n",
      "encoder.layer.1.attention.attn.o.weight: False\n",
      "encoder.layer.1.attention.attn.o.bias: False\n",
      "encoder.layer.1.attention.LayerNorm.weight: False\n",
      "encoder.layer.1.attention.LayerNorm.bias: False\n",
      "encoder.layer.1.intermediate.dense.weight: False\n",
      "encoder.layer.1.intermediate.dense.bias: False\n",
      "encoder.layer.1.output.dense.weight: False\n",
      "encoder.layer.1.output.dense.bias: False\n",
      "encoder.layer.1.output.LayerNorm.weight: False\n",
      "encoder.layer.1.output.LayerNorm.bias: False\n",
      "encoder.layer.2.attention.attn.q.weight: False\n",
      "encoder.layer.2.attention.attn.q.bias: False\n",
      "encoder.layer.2.attention.attn.k.weight: False\n",
      "encoder.layer.2.attention.attn.k.bias: False\n",
      "encoder.layer.2.attention.attn.v.weight: False\n",
      "encoder.layer.2.attention.attn.v.bias: False\n",
      "encoder.layer.2.attention.attn.o.weight: False\n",
      "encoder.layer.2.attention.attn.o.bias: False\n",
      "encoder.layer.2.attention.LayerNorm.weight: False\n",
      "encoder.layer.2.attention.LayerNorm.bias: False\n",
      "encoder.layer.2.intermediate.dense.weight: False\n",
      "encoder.layer.2.intermediate.dense.bias: False\n",
      "encoder.layer.2.output.dense.weight: False\n",
      "encoder.layer.2.output.dense.bias: False\n",
      "encoder.layer.2.output.LayerNorm.weight: False\n",
      "encoder.layer.2.output.LayerNorm.bias: False\n",
      "encoder.layer.3.attention.attn.q.weight: False\n",
      "encoder.layer.3.attention.attn.q.bias: False\n",
      "encoder.layer.3.attention.attn.k.weight: False\n",
      "encoder.layer.3.attention.attn.k.bias: False\n",
      "encoder.layer.3.attention.attn.v.weight: False\n",
      "encoder.layer.3.attention.attn.v.bias: False\n",
      "encoder.layer.3.attention.attn.o.weight: False\n",
      "encoder.layer.3.attention.attn.o.bias: False\n",
      "encoder.layer.3.attention.LayerNorm.weight: False\n",
      "encoder.layer.3.attention.LayerNorm.bias: False\n",
      "encoder.layer.3.intermediate.dense.weight: False\n",
      "encoder.layer.3.intermediate.dense.bias: False\n",
      "encoder.layer.3.output.dense.weight: False\n",
      "encoder.layer.3.output.dense.bias: False\n",
      "encoder.layer.3.output.LayerNorm.weight: False\n",
      "encoder.layer.3.output.LayerNorm.bias: False\n",
      "encoder.layer.4.attention.attn.q.weight: True\n",
      "encoder.layer.4.attention.attn.q.bias: True\n",
      "encoder.layer.4.attention.attn.k.weight: True\n",
      "encoder.layer.4.attention.attn.k.bias: True\n",
      "encoder.layer.4.attention.attn.v.weight: True\n",
      "encoder.layer.4.attention.attn.v.bias: True\n",
      "encoder.layer.4.attention.attn.o.weight: True\n",
      "encoder.layer.4.attention.attn.o.bias: True\n",
      "encoder.layer.4.attention.LayerNorm.weight: True\n",
      "encoder.layer.4.attention.LayerNorm.bias: True\n",
      "encoder.layer.4.intermediate.dense.weight: True\n",
      "encoder.layer.4.intermediate.dense.bias: True\n",
      "encoder.layer.4.output.dense.weight: True\n",
      "encoder.layer.4.output.dense.bias: True\n",
      "encoder.layer.4.output.LayerNorm.weight: True\n",
      "encoder.layer.4.output.LayerNorm.bias: True\n",
      "encoder.layer.5.attention.attn.q.weight: True\n",
      "encoder.layer.5.attention.attn.q.bias: True\n",
      "encoder.layer.5.attention.attn.k.weight: True\n",
      "encoder.layer.5.attention.attn.k.bias: True\n",
      "encoder.layer.5.attention.attn.v.weight: True\n",
      "encoder.layer.5.attention.attn.v.bias: True\n",
      "encoder.layer.5.attention.attn.o.weight: True\n",
      "encoder.layer.5.attention.attn.o.bias: True\n",
      "encoder.layer.5.attention.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.attn.q.weight: True\n",
      "encoder.layer.6.attention.attn.q.bias: True\n",
      "encoder.layer.6.attention.attn.k.weight: True\n",
      "encoder.layer.6.attention.attn.k.bias: True\n",
      "encoder.layer.6.attention.attn.v.weight: True\n",
      "encoder.layer.6.attention.attn.v.bias: True\n",
      "encoder.layer.6.attention.attn.o.weight: True\n",
      "encoder.layer.6.attention.attn.o.bias: True\n",
      "encoder.layer.6.attention.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.attn.q.weight: True\n",
      "encoder.layer.7.attention.attn.q.bias: True\n",
      "encoder.layer.7.attention.attn.k.weight: True\n",
      "encoder.layer.7.attention.attn.k.bias: True\n",
      "encoder.layer.7.attention.attn.v.weight: True\n",
      "encoder.layer.7.attention.attn.v.bias: True\n",
      "encoder.layer.7.attention.attn.o.weight: True\n",
      "encoder.layer.7.attention.attn.o.bias: True\n",
      "encoder.layer.7.attention.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.attn.q.weight: True\n",
      "encoder.layer.8.attention.attn.q.bias: True\n",
      "encoder.layer.8.attention.attn.k.weight: True\n",
      "encoder.layer.8.attention.attn.k.bias: True\n",
      "encoder.layer.8.attention.attn.v.weight: True\n",
      "encoder.layer.8.attention.attn.v.bias: True\n",
      "encoder.layer.8.attention.attn.o.weight: True\n",
      "encoder.layer.8.attention.attn.o.bias: True\n",
      "encoder.layer.8.attention.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.attn.q.weight: True\n",
      "encoder.layer.9.attention.attn.q.bias: True\n",
      "encoder.layer.9.attention.attn.k.weight: True\n",
      "encoder.layer.9.attention.attn.k.bias: True\n",
      "encoder.layer.9.attention.attn.v.weight: True\n",
      "encoder.layer.9.attention.attn.v.bias: True\n",
      "encoder.layer.9.attention.attn.o.weight: True\n",
      "encoder.layer.9.attention.attn.o.bias: True\n",
      "encoder.layer.9.attention.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.attn.q.weight: True\n",
      "encoder.layer.10.attention.attn.q.bias: True\n",
      "encoder.layer.10.attention.attn.k.weight: True\n",
      "encoder.layer.10.attention.attn.k.bias: True\n",
      "encoder.layer.10.attention.attn.v.weight: True\n",
      "encoder.layer.10.attention.attn.v.bias: True\n",
      "encoder.layer.10.attention.attn.o.weight: True\n",
      "encoder.layer.10.attention.attn.o.bias: True\n",
      "encoder.layer.10.attention.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.attn.q.weight: True\n",
      "encoder.layer.11.attention.attn.q.bias: True\n",
      "encoder.layer.11.attention.attn.k.weight: True\n",
      "encoder.layer.11.attention.attn.k.bias: True\n",
      "encoder.layer.11.attention.attn.v.weight: True\n",
      "encoder.layer.11.attention.attn.v.bias: True\n",
      "encoder.layer.11.attention.attn.o.weight: True\n",
      "encoder.layer.11.attention.attn.o.bias: True\n",
      "encoder.layer.11.attention.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "encoder.relative_attention_bias.weight: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f60e6fde7994b5398757ade13be1a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fd9cd439024f9aa46ebd4c20237a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First unfrozen layer: 5\n",
      "Model :  MPNet\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/mpnet-base\n",
      "Before training -------------------\n",
      "embeddings.word_embeddings.weight: False\n",
      "embeddings.position_embeddings.weight: False\n",
      "embeddings.LayerNorm.weight: False\n",
      "embeddings.LayerNorm.bias: False\n",
      "encoder.layer.0.attention.attn.q.weight: False\n",
      "encoder.layer.0.attention.attn.q.bias: False\n",
      "encoder.layer.0.attention.attn.k.weight: False\n",
      "encoder.layer.0.attention.attn.k.bias: False\n",
      "encoder.layer.0.attention.attn.v.weight: False\n",
      "encoder.layer.0.attention.attn.v.bias: False\n",
      "encoder.layer.0.attention.attn.o.weight: False\n",
      "encoder.layer.0.attention.attn.o.bias: False\n",
      "encoder.layer.0.attention.LayerNorm.weight: False\n",
      "encoder.layer.0.attention.LayerNorm.bias: False\n",
      "encoder.layer.0.intermediate.dense.weight: False\n",
      "encoder.layer.0.intermediate.dense.bias: False\n",
      "encoder.layer.0.output.dense.weight: False\n",
      "encoder.layer.0.output.dense.bias: False\n",
      "encoder.layer.0.output.LayerNorm.weight: False\n",
      "encoder.layer.0.output.LayerNorm.bias: False\n",
      "encoder.layer.1.attention.attn.q.weight: False\n",
      "encoder.layer.1.attention.attn.q.bias: False\n",
      "encoder.layer.1.attention.attn.k.weight: False\n",
      "encoder.layer.1.attention.attn.k.bias: False\n",
      "encoder.layer.1.attention.attn.v.weight: False\n",
      "encoder.layer.1.attention.attn.v.bias: False\n",
      "encoder.layer.1.attention.attn.o.weight: False\n",
      "encoder.layer.1.attention.attn.o.bias: False\n",
      "encoder.layer.1.attention.LayerNorm.weight: False\n",
      "encoder.layer.1.attention.LayerNorm.bias: False\n",
      "encoder.layer.1.intermediate.dense.weight: False\n",
      "encoder.layer.1.intermediate.dense.bias: False\n",
      "encoder.layer.1.output.dense.weight: False\n",
      "encoder.layer.1.output.dense.bias: False\n",
      "encoder.layer.1.output.LayerNorm.weight: False\n",
      "encoder.layer.1.output.LayerNorm.bias: False\n",
      "encoder.layer.2.attention.attn.q.weight: False\n",
      "encoder.layer.2.attention.attn.q.bias: False\n",
      "encoder.layer.2.attention.attn.k.weight: False\n",
      "encoder.layer.2.attention.attn.k.bias: False\n",
      "encoder.layer.2.attention.attn.v.weight: False\n",
      "encoder.layer.2.attention.attn.v.bias: False\n",
      "encoder.layer.2.attention.attn.o.weight: False\n",
      "encoder.layer.2.attention.attn.o.bias: False\n",
      "encoder.layer.2.attention.LayerNorm.weight: False\n",
      "encoder.layer.2.attention.LayerNorm.bias: False\n",
      "encoder.layer.2.intermediate.dense.weight: False\n",
      "encoder.layer.2.intermediate.dense.bias: False\n",
      "encoder.layer.2.output.dense.weight: False\n",
      "encoder.layer.2.output.dense.bias: False\n",
      "encoder.layer.2.output.LayerNorm.weight: False\n",
      "encoder.layer.2.output.LayerNorm.bias: False\n",
      "encoder.layer.3.attention.attn.q.weight: False\n",
      "encoder.layer.3.attention.attn.q.bias: False\n",
      "encoder.layer.3.attention.attn.k.weight: False\n",
      "encoder.layer.3.attention.attn.k.bias: False\n",
      "encoder.layer.3.attention.attn.v.weight: False\n",
      "encoder.layer.3.attention.attn.v.bias: False\n",
      "encoder.layer.3.attention.attn.o.weight: False\n",
      "encoder.layer.3.attention.attn.o.bias: False\n",
      "encoder.layer.3.attention.LayerNorm.weight: False\n",
      "encoder.layer.3.attention.LayerNorm.bias: False\n",
      "encoder.layer.3.intermediate.dense.weight: False\n",
      "encoder.layer.3.intermediate.dense.bias: False\n",
      "encoder.layer.3.output.dense.weight: False\n",
      "encoder.layer.3.output.dense.bias: False\n",
      "encoder.layer.3.output.LayerNorm.weight: False\n",
      "encoder.layer.3.output.LayerNorm.bias: False\n",
      "encoder.layer.4.attention.attn.q.weight: False\n",
      "encoder.layer.4.attention.attn.q.bias: False\n",
      "encoder.layer.4.attention.attn.k.weight: False\n",
      "encoder.layer.4.attention.attn.k.bias: False\n",
      "encoder.layer.4.attention.attn.v.weight: False\n",
      "encoder.layer.4.attention.attn.v.bias: False\n",
      "encoder.layer.4.attention.attn.o.weight: False\n",
      "encoder.layer.4.attention.attn.o.bias: False\n",
      "encoder.layer.4.attention.LayerNorm.weight: False\n",
      "encoder.layer.4.attention.LayerNorm.bias: False\n",
      "encoder.layer.4.intermediate.dense.weight: False\n",
      "encoder.layer.4.intermediate.dense.bias: False\n",
      "encoder.layer.4.output.dense.weight: False\n",
      "encoder.layer.4.output.dense.bias: False\n",
      "encoder.layer.4.output.LayerNorm.weight: False\n",
      "encoder.layer.4.output.LayerNorm.bias: False\n",
      "encoder.layer.5.attention.attn.q.weight: True\n",
      "encoder.layer.5.attention.attn.q.bias: True\n",
      "encoder.layer.5.attention.attn.k.weight: True\n",
      "encoder.layer.5.attention.attn.k.bias: True\n",
      "encoder.layer.5.attention.attn.v.weight: True\n",
      "encoder.layer.5.attention.attn.v.bias: True\n",
      "encoder.layer.5.attention.attn.o.weight: True\n",
      "encoder.layer.5.attention.attn.o.bias: True\n",
      "encoder.layer.5.attention.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.attn.q.weight: True\n",
      "encoder.layer.6.attention.attn.q.bias: True\n",
      "encoder.layer.6.attention.attn.k.weight: True\n",
      "encoder.layer.6.attention.attn.k.bias: True\n",
      "encoder.layer.6.attention.attn.v.weight: True\n",
      "encoder.layer.6.attention.attn.v.bias: True\n",
      "encoder.layer.6.attention.attn.o.weight: True\n",
      "encoder.layer.6.attention.attn.o.bias: True\n",
      "encoder.layer.6.attention.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.attn.q.weight: True\n",
      "encoder.layer.7.attention.attn.q.bias: True\n",
      "encoder.layer.7.attention.attn.k.weight: True\n",
      "encoder.layer.7.attention.attn.k.bias: True\n",
      "encoder.layer.7.attention.attn.v.weight: True\n",
      "encoder.layer.7.attention.attn.v.bias: True\n",
      "encoder.layer.7.attention.attn.o.weight: True\n",
      "encoder.layer.7.attention.attn.o.bias: True\n",
      "encoder.layer.7.attention.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.attn.q.weight: True\n",
      "encoder.layer.8.attention.attn.q.bias: True\n",
      "encoder.layer.8.attention.attn.k.weight: True\n",
      "encoder.layer.8.attention.attn.k.bias: True\n",
      "encoder.layer.8.attention.attn.v.weight: True\n",
      "encoder.layer.8.attention.attn.v.bias: True\n",
      "encoder.layer.8.attention.attn.o.weight: True\n",
      "encoder.layer.8.attention.attn.o.bias: True\n",
      "encoder.layer.8.attention.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.attn.q.weight: True\n",
      "encoder.layer.9.attention.attn.q.bias: True\n",
      "encoder.layer.9.attention.attn.k.weight: True\n",
      "encoder.layer.9.attention.attn.k.bias: True\n",
      "encoder.layer.9.attention.attn.v.weight: True\n",
      "encoder.layer.9.attention.attn.v.bias: True\n",
      "encoder.layer.9.attention.attn.o.weight: True\n",
      "encoder.layer.9.attention.attn.o.bias: True\n",
      "encoder.layer.9.attention.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.attn.q.weight: True\n",
      "encoder.layer.10.attention.attn.q.bias: True\n",
      "encoder.layer.10.attention.attn.k.weight: True\n",
      "encoder.layer.10.attention.attn.k.bias: True\n",
      "encoder.layer.10.attention.attn.v.weight: True\n",
      "encoder.layer.10.attention.attn.v.bias: True\n",
      "encoder.layer.10.attention.attn.o.weight: True\n",
      "encoder.layer.10.attention.attn.o.bias: True\n",
      "encoder.layer.10.attention.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.attn.q.weight: True\n",
      "encoder.layer.11.attention.attn.q.bias: True\n",
      "encoder.layer.11.attention.attn.k.weight: True\n",
      "encoder.layer.11.attention.attn.k.bias: True\n",
      "encoder.layer.11.attention.attn.v.weight: True\n",
      "encoder.layer.11.attention.attn.v.bias: True\n",
      "encoder.layer.11.attention.attn.o.weight: True\n",
      "encoder.layer.11.attention.attn.o.bias: True\n",
      "encoder.layer.11.attention.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "encoder.relative_attention_bias.weight: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18514b4869b94c20b344e5ea08b65338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb16b501f4f14d30b50f55a848b14911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First unfrozen layer: 6\n",
      "Model :  MPNet\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/mpnet-base\n",
      "Before training -------------------\n",
      "embeddings.word_embeddings.weight: False\n",
      "embeddings.position_embeddings.weight: False\n",
      "embeddings.LayerNorm.weight: False\n",
      "embeddings.LayerNorm.bias: False\n",
      "encoder.layer.0.attention.attn.q.weight: False\n",
      "encoder.layer.0.attention.attn.q.bias: False\n",
      "encoder.layer.0.attention.attn.k.weight: False\n",
      "encoder.layer.0.attention.attn.k.bias: False\n",
      "encoder.layer.0.attention.attn.v.weight: False\n",
      "encoder.layer.0.attention.attn.v.bias: False\n",
      "encoder.layer.0.attention.attn.o.weight: False\n",
      "encoder.layer.0.attention.attn.o.bias: False\n",
      "encoder.layer.0.attention.LayerNorm.weight: False\n",
      "encoder.layer.0.attention.LayerNorm.bias: False\n",
      "encoder.layer.0.intermediate.dense.weight: False\n",
      "encoder.layer.0.intermediate.dense.bias: False\n",
      "encoder.layer.0.output.dense.weight: False\n",
      "encoder.layer.0.output.dense.bias: False\n",
      "encoder.layer.0.output.LayerNorm.weight: False\n",
      "encoder.layer.0.output.LayerNorm.bias: False\n",
      "encoder.layer.1.attention.attn.q.weight: False\n",
      "encoder.layer.1.attention.attn.q.bias: False\n",
      "encoder.layer.1.attention.attn.k.weight: False\n",
      "encoder.layer.1.attention.attn.k.bias: False\n",
      "encoder.layer.1.attention.attn.v.weight: False\n",
      "encoder.layer.1.attention.attn.v.bias: False\n",
      "encoder.layer.1.attention.attn.o.weight: False\n",
      "encoder.layer.1.attention.attn.o.bias: False\n",
      "encoder.layer.1.attention.LayerNorm.weight: False\n",
      "encoder.layer.1.attention.LayerNorm.bias: False\n",
      "encoder.layer.1.intermediate.dense.weight: False\n",
      "encoder.layer.1.intermediate.dense.bias: False\n",
      "encoder.layer.1.output.dense.weight: False\n",
      "encoder.layer.1.output.dense.bias: False\n",
      "encoder.layer.1.output.LayerNorm.weight: False\n",
      "encoder.layer.1.output.LayerNorm.bias: False\n",
      "encoder.layer.2.attention.attn.q.weight: False\n",
      "encoder.layer.2.attention.attn.q.bias: False\n",
      "encoder.layer.2.attention.attn.k.weight: False\n",
      "encoder.layer.2.attention.attn.k.bias: False\n",
      "encoder.layer.2.attention.attn.v.weight: False\n",
      "encoder.layer.2.attention.attn.v.bias: False\n",
      "encoder.layer.2.attention.attn.o.weight: False\n",
      "encoder.layer.2.attention.attn.o.bias: False\n",
      "encoder.layer.2.attention.LayerNorm.weight: False\n",
      "encoder.layer.2.attention.LayerNorm.bias: False\n",
      "encoder.layer.2.intermediate.dense.weight: False\n",
      "encoder.layer.2.intermediate.dense.bias: False\n",
      "encoder.layer.2.output.dense.weight: False\n",
      "encoder.layer.2.output.dense.bias: False\n",
      "encoder.layer.2.output.LayerNorm.weight: False\n",
      "encoder.layer.2.output.LayerNorm.bias: False\n",
      "encoder.layer.3.attention.attn.q.weight: False\n",
      "encoder.layer.3.attention.attn.q.bias: False\n",
      "encoder.layer.3.attention.attn.k.weight: False\n",
      "encoder.layer.3.attention.attn.k.bias: False\n",
      "encoder.layer.3.attention.attn.v.weight: False\n",
      "encoder.layer.3.attention.attn.v.bias: False\n",
      "encoder.layer.3.attention.attn.o.weight: False\n",
      "encoder.layer.3.attention.attn.o.bias: False\n",
      "encoder.layer.3.attention.LayerNorm.weight: False\n",
      "encoder.layer.3.attention.LayerNorm.bias: False\n",
      "encoder.layer.3.intermediate.dense.weight: False\n",
      "encoder.layer.3.intermediate.dense.bias: False\n",
      "encoder.layer.3.output.dense.weight: False\n",
      "encoder.layer.3.output.dense.bias: False\n",
      "encoder.layer.3.output.LayerNorm.weight: False\n",
      "encoder.layer.3.output.LayerNorm.bias: False\n",
      "encoder.layer.4.attention.attn.q.weight: False\n",
      "encoder.layer.4.attention.attn.q.bias: False\n",
      "encoder.layer.4.attention.attn.k.weight: False\n",
      "encoder.layer.4.attention.attn.k.bias: False\n",
      "encoder.layer.4.attention.attn.v.weight: False\n",
      "encoder.layer.4.attention.attn.v.bias: False\n",
      "encoder.layer.4.attention.attn.o.weight: False\n",
      "encoder.layer.4.attention.attn.o.bias: False\n",
      "encoder.layer.4.attention.LayerNorm.weight: False\n",
      "encoder.layer.4.attention.LayerNorm.bias: False\n",
      "encoder.layer.4.intermediate.dense.weight: False\n",
      "encoder.layer.4.intermediate.dense.bias: False\n",
      "encoder.layer.4.output.dense.weight: False\n",
      "encoder.layer.4.output.dense.bias: False\n",
      "encoder.layer.4.output.LayerNorm.weight: False\n",
      "encoder.layer.4.output.LayerNorm.bias: False\n",
      "encoder.layer.5.attention.attn.q.weight: False\n",
      "encoder.layer.5.attention.attn.q.bias: False\n",
      "encoder.layer.5.attention.attn.k.weight: False\n",
      "encoder.layer.5.attention.attn.k.bias: False\n",
      "encoder.layer.5.attention.attn.v.weight: False\n",
      "encoder.layer.5.attention.attn.v.bias: False\n",
      "encoder.layer.5.attention.attn.o.weight: False\n",
      "encoder.layer.5.attention.attn.o.bias: False\n",
      "encoder.layer.5.attention.LayerNorm.weight: False\n",
      "encoder.layer.5.attention.LayerNorm.bias: False\n",
      "encoder.layer.5.intermediate.dense.weight: False\n",
      "encoder.layer.5.intermediate.dense.bias: False\n",
      "encoder.layer.5.output.dense.weight: False\n",
      "encoder.layer.5.output.dense.bias: False\n",
      "encoder.layer.5.output.LayerNorm.weight: False\n",
      "encoder.layer.5.output.LayerNorm.bias: False\n",
      "encoder.layer.6.attention.attn.q.weight: True\n",
      "encoder.layer.6.attention.attn.q.bias: True\n",
      "encoder.layer.6.attention.attn.k.weight: True\n",
      "encoder.layer.6.attention.attn.k.bias: True\n",
      "encoder.layer.6.attention.attn.v.weight: True\n",
      "encoder.layer.6.attention.attn.v.bias: True\n",
      "encoder.layer.6.attention.attn.o.weight: True\n",
      "encoder.layer.6.attention.attn.o.bias: True\n",
      "encoder.layer.6.attention.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.attn.q.weight: True\n",
      "encoder.layer.7.attention.attn.q.bias: True\n",
      "encoder.layer.7.attention.attn.k.weight: True\n",
      "encoder.layer.7.attention.attn.k.bias: True\n",
      "encoder.layer.7.attention.attn.v.weight: True\n",
      "encoder.layer.7.attention.attn.v.bias: True\n",
      "encoder.layer.7.attention.attn.o.weight: True\n",
      "encoder.layer.7.attention.attn.o.bias: True\n",
      "encoder.layer.7.attention.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.attn.q.weight: True\n",
      "encoder.layer.8.attention.attn.q.bias: True\n",
      "encoder.layer.8.attention.attn.k.weight: True\n",
      "encoder.layer.8.attention.attn.k.bias: True\n",
      "encoder.layer.8.attention.attn.v.weight: True\n",
      "encoder.layer.8.attention.attn.v.bias: True\n",
      "encoder.layer.8.attention.attn.o.weight: True\n",
      "encoder.layer.8.attention.attn.o.bias: True\n",
      "encoder.layer.8.attention.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.attn.q.weight: True\n",
      "encoder.layer.9.attention.attn.q.bias: True\n",
      "encoder.layer.9.attention.attn.k.weight: True\n",
      "encoder.layer.9.attention.attn.k.bias: True\n",
      "encoder.layer.9.attention.attn.v.weight: True\n",
      "encoder.layer.9.attention.attn.v.bias: True\n",
      "encoder.layer.9.attention.attn.o.weight: True\n",
      "encoder.layer.9.attention.attn.o.bias: True\n",
      "encoder.layer.9.attention.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.attn.q.weight: True\n",
      "encoder.layer.10.attention.attn.q.bias: True\n",
      "encoder.layer.10.attention.attn.k.weight: True\n",
      "encoder.layer.10.attention.attn.k.bias: True\n",
      "encoder.layer.10.attention.attn.v.weight: True\n",
      "encoder.layer.10.attention.attn.v.bias: True\n",
      "encoder.layer.10.attention.attn.o.weight: True\n",
      "encoder.layer.10.attention.attn.o.bias: True\n",
      "encoder.layer.10.attention.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.attn.q.weight: True\n",
      "encoder.layer.11.attention.attn.q.bias: True\n",
      "encoder.layer.11.attention.attn.k.weight: True\n",
      "encoder.layer.11.attention.attn.k.bias: True\n",
      "encoder.layer.11.attention.attn.v.weight: True\n",
      "encoder.layer.11.attention.attn.v.bias: True\n",
      "encoder.layer.11.attention.attn.o.weight: True\n",
      "encoder.layer.11.attention.attn.o.bias: True\n",
      "encoder.layer.11.attention.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "encoder.relative_attention_bias.weight: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c3bc21dafa46f9a15fe38041aee941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc29c3d476784b50ba89d2629acca795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First unfrozen layer: 7\n",
      "Model :  MPNet\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/mpnet-base\n",
      "Before training -------------------\n",
      "embeddings.word_embeddings.weight: False\n",
      "embeddings.position_embeddings.weight: False\n",
      "embeddings.LayerNorm.weight: False\n",
      "embeddings.LayerNorm.bias: False\n",
      "encoder.layer.0.attention.attn.q.weight: False\n",
      "encoder.layer.0.attention.attn.q.bias: False\n",
      "encoder.layer.0.attention.attn.k.weight: False\n",
      "encoder.layer.0.attention.attn.k.bias: False\n",
      "encoder.layer.0.attention.attn.v.weight: False\n",
      "encoder.layer.0.attention.attn.v.bias: False\n",
      "encoder.layer.0.attention.attn.o.weight: False\n",
      "encoder.layer.0.attention.attn.o.bias: False\n",
      "encoder.layer.0.attention.LayerNorm.weight: False\n",
      "encoder.layer.0.attention.LayerNorm.bias: False\n",
      "encoder.layer.0.intermediate.dense.weight: False\n",
      "encoder.layer.0.intermediate.dense.bias: False\n",
      "encoder.layer.0.output.dense.weight: False\n",
      "encoder.layer.0.output.dense.bias: False\n",
      "encoder.layer.0.output.LayerNorm.weight: False\n",
      "encoder.layer.0.output.LayerNorm.bias: False\n",
      "encoder.layer.1.attention.attn.q.weight: False\n",
      "encoder.layer.1.attention.attn.q.bias: False\n",
      "encoder.layer.1.attention.attn.k.weight: False\n",
      "encoder.layer.1.attention.attn.k.bias: False\n",
      "encoder.layer.1.attention.attn.v.weight: False\n",
      "encoder.layer.1.attention.attn.v.bias: False\n",
      "encoder.layer.1.attention.attn.o.weight: False\n",
      "encoder.layer.1.attention.attn.o.bias: False\n",
      "encoder.layer.1.attention.LayerNorm.weight: False\n",
      "encoder.layer.1.attention.LayerNorm.bias: False\n",
      "encoder.layer.1.intermediate.dense.weight: False\n",
      "encoder.layer.1.intermediate.dense.bias: False\n",
      "encoder.layer.1.output.dense.weight: False\n",
      "encoder.layer.1.output.dense.bias: False\n",
      "encoder.layer.1.output.LayerNorm.weight: False\n",
      "encoder.layer.1.output.LayerNorm.bias: False\n",
      "encoder.layer.2.attention.attn.q.weight: False\n",
      "encoder.layer.2.attention.attn.q.bias: False\n",
      "encoder.layer.2.attention.attn.k.weight: False\n",
      "encoder.layer.2.attention.attn.k.bias: False\n",
      "encoder.layer.2.attention.attn.v.weight: False\n",
      "encoder.layer.2.attention.attn.v.bias: False\n",
      "encoder.layer.2.attention.attn.o.weight: False\n",
      "encoder.layer.2.attention.attn.o.bias: False\n",
      "encoder.layer.2.attention.LayerNorm.weight: False\n",
      "encoder.layer.2.attention.LayerNorm.bias: False\n",
      "encoder.layer.2.intermediate.dense.weight: False\n",
      "encoder.layer.2.intermediate.dense.bias: False\n",
      "encoder.layer.2.output.dense.weight: False\n",
      "encoder.layer.2.output.dense.bias: False\n",
      "encoder.layer.2.output.LayerNorm.weight: False\n",
      "encoder.layer.2.output.LayerNorm.bias: False\n",
      "encoder.layer.3.attention.attn.q.weight: False\n",
      "encoder.layer.3.attention.attn.q.bias: False\n",
      "encoder.layer.3.attention.attn.k.weight: False\n",
      "encoder.layer.3.attention.attn.k.bias: False\n",
      "encoder.layer.3.attention.attn.v.weight: False\n",
      "encoder.layer.3.attention.attn.v.bias: False\n",
      "encoder.layer.3.attention.attn.o.weight: False\n",
      "encoder.layer.3.attention.attn.o.bias: False\n",
      "encoder.layer.3.attention.LayerNorm.weight: False\n",
      "encoder.layer.3.attention.LayerNorm.bias: False\n",
      "encoder.layer.3.intermediate.dense.weight: False\n",
      "encoder.layer.3.intermediate.dense.bias: False\n",
      "encoder.layer.3.output.dense.weight: False\n",
      "encoder.layer.3.output.dense.bias: False\n",
      "encoder.layer.3.output.LayerNorm.weight: False\n",
      "encoder.layer.3.output.LayerNorm.bias: False\n",
      "encoder.layer.4.attention.attn.q.weight: False\n",
      "encoder.layer.4.attention.attn.q.bias: False\n",
      "encoder.layer.4.attention.attn.k.weight: False\n",
      "encoder.layer.4.attention.attn.k.bias: False\n",
      "encoder.layer.4.attention.attn.v.weight: False\n",
      "encoder.layer.4.attention.attn.v.bias: False\n",
      "encoder.layer.4.attention.attn.o.weight: False\n",
      "encoder.layer.4.attention.attn.o.bias: False\n",
      "encoder.layer.4.attention.LayerNorm.weight: False\n",
      "encoder.layer.4.attention.LayerNorm.bias: False\n",
      "encoder.layer.4.intermediate.dense.weight: False\n",
      "encoder.layer.4.intermediate.dense.bias: False\n",
      "encoder.layer.4.output.dense.weight: False\n",
      "encoder.layer.4.output.dense.bias: False\n",
      "encoder.layer.4.output.LayerNorm.weight: False\n",
      "encoder.layer.4.output.LayerNorm.bias: False\n",
      "encoder.layer.5.attention.attn.q.weight: False\n",
      "encoder.layer.5.attention.attn.q.bias: False\n",
      "encoder.layer.5.attention.attn.k.weight: False\n",
      "encoder.layer.5.attention.attn.k.bias: False\n",
      "encoder.layer.5.attention.attn.v.weight: False\n",
      "encoder.layer.5.attention.attn.v.bias: False\n",
      "encoder.layer.5.attention.attn.o.weight: False\n",
      "encoder.layer.5.attention.attn.o.bias: False\n",
      "encoder.layer.5.attention.LayerNorm.weight: False\n",
      "encoder.layer.5.attention.LayerNorm.bias: False\n",
      "encoder.layer.5.intermediate.dense.weight: False\n",
      "encoder.layer.5.intermediate.dense.bias: False\n",
      "encoder.layer.5.output.dense.weight: False\n",
      "encoder.layer.5.output.dense.bias: False\n",
      "encoder.layer.5.output.LayerNorm.weight: False\n",
      "encoder.layer.5.output.LayerNorm.bias: False\n",
      "encoder.layer.6.attention.attn.q.weight: False\n",
      "encoder.layer.6.attention.attn.q.bias: False\n",
      "encoder.layer.6.attention.attn.k.weight: False\n",
      "encoder.layer.6.attention.attn.k.bias: False\n",
      "encoder.layer.6.attention.attn.v.weight: False\n",
      "encoder.layer.6.attention.attn.v.bias: False\n",
      "encoder.layer.6.attention.attn.o.weight: False\n",
      "encoder.layer.6.attention.attn.o.bias: False\n",
      "encoder.layer.6.attention.LayerNorm.weight: False\n",
      "encoder.layer.6.attention.LayerNorm.bias: False\n",
      "encoder.layer.6.intermediate.dense.weight: False\n",
      "encoder.layer.6.intermediate.dense.bias: False\n",
      "encoder.layer.6.output.dense.weight: False\n",
      "encoder.layer.6.output.dense.bias: False\n",
      "encoder.layer.6.output.LayerNorm.weight: False\n",
      "encoder.layer.6.output.LayerNorm.bias: False\n",
      "encoder.layer.7.attention.attn.q.weight: True\n",
      "encoder.layer.7.attention.attn.q.bias: True\n",
      "encoder.layer.7.attention.attn.k.weight: True\n",
      "encoder.layer.7.attention.attn.k.bias: True\n",
      "encoder.layer.7.attention.attn.v.weight: True\n",
      "encoder.layer.7.attention.attn.v.bias: True\n",
      "encoder.layer.7.attention.attn.o.weight: True\n",
      "encoder.layer.7.attention.attn.o.bias: True\n",
      "encoder.layer.7.attention.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.attn.q.weight: True\n",
      "encoder.layer.8.attention.attn.q.bias: True\n",
      "encoder.layer.8.attention.attn.k.weight: True\n",
      "encoder.layer.8.attention.attn.k.bias: True\n",
      "encoder.layer.8.attention.attn.v.weight: True\n",
      "encoder.layer.8.attention.attn.v.bias: True\n",
      "encoder.layer.8.attention.attn.o.weight: True\n",
      "encoder.layer.8.attention.attn.o.bias: True\n",
      "encoder.layer.8.attention.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.attn.q.weight: True\n",
      "encoder.layer.9.attention.attn.q.bias: True\n",
      "encoder.layer.9.attention.attn.k.weight: True\n",
      "encoder.layer.9.attention.attn.k.bias: True\n",
      "encoder.layer.9.attention.attn.v.weight: True\n",
      "encoder.layer.9.attention.attn.v.bias: True\n",
      "encoder.layer.9.attention.attn.o.weight: True\n",
      "encoder.layer.9.attention.attn.o.bias: True\n",
      "encoder.layer.9.attention.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.attn.q.weight: True\n",
      "encoder.layer.10.attention.attn.q.bias: True\n",
      "encoder.layer.10.attention.attn.k.weight: True\n",
      "encoder.layer.10.attention.attn.k.bias: True\n",
      "encoder.layer.10.attention.attn.v.weight: True\n",
      "encoder.layer.10.attention.attn.v.bias: True\n",
      "encoder.layer.10.attention.attn.o.weight: True\n",
      "encoder.layer.10.attention.attn.o.bias: True\n",
      "encoder.layer.10.attention.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.attn.q.weight: True\n",
      "encoder.layer.11.attention.attn.q.bias: True\n",
      "encoder.layer.11.attention.attn.k.weight: True\n",
      "encoder.layer.11.attention.attn.k.bias: True\n",
      "encoder.layer.11.attention.attn.v.weight: True\n",
      "encoder.layer.11.attention.attn.v.bias: True\n",
      "encoder.layer.11.attention.attn.o.weight: True\n",
      "encoder.layer.11.attention.attn.o.bias: True\n",
      "encoder.layer.11.attention.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "encoder.relative_attention_bias.weight: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2686f442671a4f81b99f8907ab15e8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d21ba29c5d4f5d9b251f6bd257d22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First unfrozen layer: 8\n",
      "Model :  MPNet\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/mpnet-base\n",
      "Before training -------------------\n",
      "embeddings.word_embeddings.weight: False\n",
      "embeddings.position_embeddings.weight: False\n",
      "embeddings.LayerNorm.weight: False\n",
      "embeddings.LayerNorm.bias: False\n",
      "encoder.layer.0.attention.attn.q.weight: False\n",
      "encoder.layer.0.attention.attn.q.bias: False\n",
      "encoder.layer.0.attention.attn.k.weight: False\n",
      "encoder.layer.0.attention.attn.k.bias: False\n",
      "encoder.layer.0.attention.attn.v.weight: False\n",
      "encoder.layer.0.attention.attn.v.bias: False\n",
      "encoder.layer.0.attention.attn.o.weight: False\n",
      "encoder.layer.0.attention.attn.o.bias: False\n",
      "encoder.layer.0.attention.LayerNorm.weight: False\n",
      "encoder.layer.0.attention.LayerNorm.bias: False\n",
      "encoder.layer.0.intermediate.dense.weight: False\n",
      "encoder.layer.0.intermediate.dense.bias: False\n",
      "encoder.layer.0.output.dense.weight: False\n",
      "encoder.layer.0.output.dense.bias: False\n",
      "encoder.layer.0.output.LayerNorm.weight: False\n",
      "encoder.layer.0.output.LayerNorm.bias: False\n",
      "encoder.layer.1.attention.attn.q.weight: False\n",
      "encoder.layer.1.attention.attn.q.bias: False\n",
      "encoder.layer.1.attention.attn.k.weight: False\n",
      "encoder.layer.1.attention.attn.k.bias: False\n",
      "encoder.layer.1.attention.attn.v.weight: False\n",
      "encoder.layer.1.attention.attn.v.bias: False\n",
      "encoder.layer.1.attention.attn.o.weight: False\n",
      "encoder.layer.1.attention.attn.o.bias: False\n",
      "encoder.layer.1.attention.LayerNorm.weight: False\n",
      "encoder.layer.1.attention.LayerNorm.bias: False\n",
      "encoder.layer.1.intermediate.dense.weight: False\n",
      "encoder.layer.1.intermediate.dense.bias: False\n",
      "encoder.layer.1.output.dense.weight: False\n",
      "encoder.layer.1.output.dense.bias: False\n",
      "encoder.layer.1.output.LayerNorm.weight: False\n",
      "encoder.layer.1.output.LayerNorm.bias: False\n",
      "encoder.layer.2.attention.attn.q.weight: False\n",
      "encoder.layer.2.attention.attn.q.bias: False\n",
      "encoder.layer.2.attention.attn.k.weight: False\n",
      "encoder.layer.2.attention.attn.k.bias: False\n",
      "encoder.layer.2.attention.attn.v.weight: False\n",
      "encoder.layer.2.attention.attn.v.bias: False\n",
      "encoder.layer.2.attention.attn.o.weight: False\n",
      "encoder.layer.2.attention.attn.o.bias: False\n",
      "encoder.layer.2.attention.LayerNorm.weight: False\n",
      "encoder.layer.2.attention.LayerNorm.bias: False\n",
      "encoder.layer.2.intermediate.dense.weight: False\n",
      "encoder.layer.2.intermediate.dense.bias: False\n",
      "encoder.layer.2.output.dense.weight: False\n",
      "encoder.layer.2.output.dense.bias: False\n",
      "encoder.layer.2.output.LayerNorm.weight: False\n",
      "encoder.layer.2.output.LayerNorm.bias: False\n",
      "encoder.layer.3.attention.attn.q.weight: False\n",
      "encoder.layer.3.attention.attn.q.bias: False\n",
      "encoder.layer.3.attention.attn.k.weight: False\n",
      "encoder.layer.3.attention.attn.k.bias: False\n",
      "encoder.layer.3.attention.attn.v.weight: False\n",
      "encoder.layer.3.attention.attn.v.bias: False\n",
      "encoder.layer.3.attention.attn.o.weight: False\n",
      "encoder.layer.3.attention.attn.o.bias: False\n",
      "encoder.layer.3.attention.LayerNorm.weight: False\n",
      "encoder.layer.3.attention.LayerNorm.bias: False\n",
      "encoder.layer.3.intermediate.dense.weight: False\n",
      "encoder.layer.3.intermediate.dense.bias: False\n",
      "encoder.layer.3.output.dense.weight: False\n",
      "encoder.layer.3.output.dense.bias: False\n",
      "encoder.layer.3.output.LayerNorm.weight: False\n",
      "encoder.layer.3.output.LayerNorm.bias: False\n",
      "encoder.layer.4.attention.attn.q.weight: False\n",
      "encoder.layer.4.attention.attn.q.bias: False\n",
      "encoder.layer.4.attention.attn.k.weight: False\n",
      "encoder.layer.4.attention.attn.k.bias: False\n",
      "encoder.layer.4.attention.attn.v.weight: False\n",
      "encoder.layer.4.attention.attn.v.bias: False\n",
      "encoder.layer.4.attention.attn.o.weight: False\n",
      "encoder.layer.4.attention.attn.o.bias: False\n",
      "encoder.layer.4.attention.LayerNorm.weight: False\n",
      "encoder.layer.4.attention.LayerNorm.bias: False\n",
      "encoder.layer.4.intermediate.dense.weight: False\n",
      "encoder.layer.4.intermediate.dense.bias: False\n",
      "encoder.layer.4.output.dense.weight: False\n",
      "encoder.layer.4.output.dense.bias: False\n",
      "encoder.layer.4.output.LayerNorm.weight: False\n",
      "encoder.layer.4.output.LayerNorm.bias: False\n",
      "encoder.layer.5.attention.attn.q.weight: False\n",
      "encoder.layer.5.attention.attn.q.bias: False\n",
      "encoder.layer.5.attention.attn.k.weight: False\n",
      "encoder.layer.5.attention.attn.k.bias: False\n",
      "encoder.layer.5.attention.attn.v.weight: False\n",
      "encoder.layer.5.attention.attn.v.bias: False\n",
      "encoder.layer.5.attention.attn.o.weight: False\n",
      "encoder.layer.5.attention.attn.o.bias: False\n",
      "encoder.layer.5.attention.LayerNorm.weight: False\n",
      "encoder.layer.5.attention.LayerNorm.bias: False\n",
      "encoder.layer.5.intermediate.dense.weight: False\n",
      "encoder.layer.5.intermediate.dense.bias: False\n",
      "encoder.layer.5.output.dense.weight: False\n",
      "encoder.layer.5.output.dense.bias: False\n",
      "encoder.layer.5.output.LayerNorm.weight: False\n",
      "encoder.layer.5.output.LayerNorm.bias: False\n",
      "encoder.layer.6.attention.attn.q.weight: False\n",
      "encoder.layer.6.attention.attn.q.bias: False\n",
      "encoder.layer.6.attention.attn.k.weight: False\n",
      "encoder.layer.6.attention.attn.k.bias: False\n",
      "encoder.layer.6.attention.attn.v.weight: False\n",
      "encoder.layer.6.attention.attn.v.bias: False\n",
      "encoder.layer.6.attention.attn.o.weight: False\n",
      "encoder.layer.6.attention.attn.o.bias: False\n",
      "encoder.layer.6.attention.LayerNorm.weight: False\n",
      "encoder.layer.6.attention.LayerNorm.bias: False\n",
      "encoder.layer.6.intermediate.dense.weight: False\n",
      "encoder.layer.6.intermediate.dense.bias: False\n",
      "encoder.layer.6.output.dense.weight: False\n",
      "encoder.layer.6.output.dense.bias: False\n",
      "encoder.layer.6.output.LayerNorm.weight: False\n",
      "encoder.layer.6.output.LayerNorm.bias: False\n",
      "encoder.layer.7.attention.attn.q.weight: False\n",
      "encoder.layer.7.attention.attn.q.bias: False\n",
      "encoder.layer.7.attention.attn.k.weight: False\n",
      "encoder.layer.7.attention.attn.k.bias: False\n",
      "encoder.layer.7.attention.attn.v.weight: False\n",
      "encoder.layer.7.attention.attn.v.bias: False\n",
      "encoder.layer.7.attention.attn.o.weight: False\n",
      "encoder.layer.7.attention.attn.o.bias: False\n",
      "encoder.layer.7.attention.LayerNorm.weight: False\n",
      "encoder.layer.7.attention.LayerNorm.bias: False\n",
      "encoder.layer.7.intermediate.dense.weight: False\n",
      "encoder.layer.7.intermediate.dense.bias: False\n",
      "encoder.layer.7.output.dense.weight: False\n",
      "encoder.layer.7.output.dense.bias: False\n",
      "encoder.layer.7.output.LayerNorm.weight: False\n",
      "encoder.layer.7.output.LayerNorm.bias: False\n",
      "encoder.layer.8.attention.attn.q.weight: True\n",
      "encoder.layer.8.attention.attn.q.bias: True\n",
      "encoder.layer.8.attention.attn.k.weight: True\n",
      "encoder.layer.8.attention.attn.k.bias: True\n",
      "encoder.layer.8.attention.attn.v.weight: True\n",
      "encoder.layer.8.attention.attn.v.bias: True\n",
      "encoder.layer.8.attention.attn.o.weight: True\n",
      "encoder.layer.8.attention.attn.o.bias: True\n",
      "encoder.layer.8.attention.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.attn.q.weight: True\n",
      "encoder.layer.9.attention.attn.q.bias: True\n",
      "encoder.layer.9.attention.attn.k.weight: True\n",
      "encoder.layer.9.attention.attn.k.bias: True\n",
      "encoder.layer.9.attention.attn.v.weight: True\n",
      "encoder.layer.9.attention.attn.v.bias: True\n",
      "encoder.layer.9.attention.attn.o.weight: True\n",
      "encoder.layer.9.attention.attn.o.bias: True\n",
      "encoder.layer.9.attention.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.attn.q.weight: True\n",
      "encoder.layer.10.attention.attn.q.bias: True\n",
      "encoder.layer.10.attention.attn.k.weight: True\n",
      "encoder.layer.10.attention.attn.k.bias: True\n",
      "encoder.layer.10.attention.attn.v.weight: True\n",
      "encoder.layer.10.attention.attn.v.bias: True\n",
      "encoder.layer.10.attention.attn.o.weight: True\n",
      "encoder.layer.10.attention.attn.o.bias: True\n",
      "encoder.layer.10.attention.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.attn.q.weight: True\n",
      "encoder.layer.11.attention.attn.q.bias: True\n",
      "encoder.layer.11.attention.attn.k.weight: True\n",
      "encoder.layer.11.attention.attn.k.bias: True\n",
      "encoder.layer.11.attention.attn.v.weight: True\n",
      "encoder.layer.11.attention.attn.v.bias: True\n",
      "encoder.layer.11.attention.attn.o.weight: True\n",
      "encoder.layer.11.attention.attn.o.bias: True\n",
      "encoder.layer.11.attention.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "encoder.relative_attention_bias.weight: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac6e9889cdb434aadadfc1154f7b28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89e5640de784b769a37de7fc50c8504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First unfrozen layer: 9\n",
      "Model :  MPNet\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/mpnet-base\n",
      "Before training -------------------\n",
      "embeddings.word_embeddings.weight: False\n",
      "embeddings.position_embeddings.weight: False\n",
      "embeddings.LayerNorm.weight: False\n",
      "embeddings.LayerNorm.bias: False\n",
      "encoder.layer.0.attention.attn.q.weight: False\n",
      "encoder.layer.0.attention.attn.q.bias: False\n",
      "encoder.layer.0.attention.attn.k.weight: False\n",
      "encoder.layer.0.attention.attn.k.bias: False\n",
      "encoder.layer.0.attention.attn.v.weight: False\n",
      "encoder.layer.0.attention.attn.v.bias: False\n",
      "encoder.layer.0.attention.attn.o.weight: False\n",
      "encoder.layer.0.attention.attn.o.bias: False\n",
      "encoder.layer.0.attention.LayerNorm.weight: False\n",
      "encoder.layer.0.attention.LayerNorm.bias: False\n",
      "encoder.layer.0.intermediate.dense.weight: False\n",
      "encoder.layer.0.intermediate.dense.bias: False\n",
      "encoder.layer.0.output.dense.weight: False\n",
      "encoder.layer.0.output.dense.bias: False\n",
      "encoder.layer.0.output.LayerNorm.weight: False\n",
      "encoder.layer.0.output.LayerNorm.bias: False\n",
      "encoder.layer.1.attention.attn.q.weight: False\n",
      "encoder.layer.1.attention.attn.q.bias: False\n",
      "encoder.layer.1.attention.attn.k.weight: False\n",
      "encoder.layer.1.attention.attn.k.bias: False\n",
      "encoder.layer.1.attention.attn.v.weight: False\n",
      "encoder.layer.1.attention.attn.v.bias: False\n",
      "encoder.layer.1.attention.attn.o.weight: False\n",
      "encoder.layer.1.attention.attn.o.bias: False\n",
      "encoder.layer.1.attention.LayerNorm.weight: False\n",
      "encoder.layer.1.attention.LayerNorm.bias: False\n",
      "encoder.layer.1.intermediate.dense.weight: False\n",
      "encoder.layer.1.intermediate.dense.bias: False\n",
      "encoder.layer.1.output.dense.weight: False\n",
      "encoder.layer.1.output.dense.bias: False\n",
      "encoder.layer.1.output.LayerNorm.weight: False\n",
      "encoder.layer.1.output.LayerNorm.bias: False\n",
      "encoder.layer.2.attention.attn.q.weight: False\n",
      "encoder.layer.2.attention.attn.q.bias: False\n",
      "encoder.layer.2.attention.attn.k.weight: False\n",
      "encoder.layer.2.attention.attn.k.bias: False\n",
      "encoder.layer.2.attention.attn.v.weight: False\n",
      "encoder.layer.2.attention.attn.v.bias: False\n",
      "encoder.layer.2.attention.attn.o.weight: False\n",
      "encoder.layer.2.attention.attn.o.bias: False\n",
      "encoder.layer.2.attention.LayerNorm.weight: False\n",
      "encoder.layer.2.attention.LayerNorm.bias: False\n",
      "encoder.layer.2.intermediate.dense.weight: False\n",
      "encoder.layer.2.intermediate.dense.bias: False\n",
      "encoder.layer.2.output.dense.weight: False\n",
      "encoder.layer.2.output.dense.bias: False\n",
      "encoder.layer.2.output.LayerNorm.weight: False\n",
      "encoder.layer.2.output.LayerNorm.bias: False\n",
      "encoder.layer.3.attention.attn.q.weight: False\n",
      "encoder.layer.3.attention.attn.q.bias: False\n",
      "encoder.layer.3.attention.attn.k.weight: False\n",
      "encoder.layer.3.attention.attn.k.bias: False\n",
      "encoder.layer.3.attention.attn.v.weight: False\n",
      "encoder.layer.3.attention.attn.v.bias: False\n",
      "encoder.layer.3.attention.attn.o.weight: False\n",
      "encoder.layer.3.attention.attn.o.bias: False\n",
      "encoder.layer.3.attention.LayerNorm.weight: False\n",
      "encoder.layer.3.attention.LayerNorm.bias: False\n",
      "encoder.layer.3.intermediate.dense.weight: False\n",
      "encoder.layer.3.intermediate.dense.bias: False\n",
      "encoder.layer.3.output.dense.weight: False\n",
      "encoder.layer.3.output.dense.bias: False\n",
      "encoder.layer.3.output.LayerNorm.weight: False\n",
      "encoder.layer.3.output.LayerNorm.bias: False\n",
      "encoder.layer.4.attention.attn.q.weight: False\n",
      "encoder.layer.4.attention.attn.q.bias: False\n",
      "encoder.layer.4.attention.attn.k.weight: False\n",
      "encoder.layer.4.attention.attn.k.bias: False\n",
      "encoder.layer.4.attention.attn.v.weight: False\n",
      "encoder.layer.4.attention.attn.v.bias: False\n",
      "encoder.layer.4.attention.attn.o.weight: False\n",
      "encoder.layer.4.attention.attn.o.bias: False\n",
      "encoder.layer.4.attention.LayerNorm.weight: False\n",
      "encoder.layer.4.attention.LayerNorm.bias: False\n",
      "encoder.layer.4.intermediate.dense.weight: False\n",
      "encoder.layer.4.intermediate.dense.bias: False\n",
      "encoder.layer.4.output.dense.weight: False\n",
      "encoder.layer.4.output.dense.bias: False\n",
      "encoder.layer.4.output.LayerNorm.weight: False\n",
      "encoder.layer.4.output.LayerNorm.bias: False\n",
      "encoder.layer.5.attention.attn.q.weight: False\n",
      "encoder.layer.5.attention.attn.q.bias: False\n",
      "encoder.layer.5.attention.attn.k.weight: False\n",
      "encoder.layer.5.attention.attn.k.bias: False\n",
      "encoder.layer.5.attention.attn.v.weight: False\n",
      "encoder.layer.5.attention.attn.v.bias: False\n",
      "encoder.layer.5.attention.attn.o.weight: False\n",
      "encoder.layer.5.attention.attn.o.bias: False\n",
      "encoder.layer.5.attention.LayerNorm.weight: False\n",
      "encoder.layer.5.attention.LayerNorm.bias: False\n",
      "encoder.layer.5.intermediate.dense.weight: False\n",
      "encoder.layer.5.intermediate.dense.bias: False\n",
      "encoder.layer.5.output.dense.weight: False\n",
      "encoder.layer.5.output.dense.bias: False\n",
      "encoder.layer.5.output.LayerNorm.weight: False\n",
      "encoder.layer.5.output.LayerNorm.bias: False\n",
      "encoder.layer.6.attention.attn.q.weight: False\n",
      "encoder.layer.6.attention.attn.q.bias: False\n",
      "encoder.layer.6.attention.attn.k.weight: False\n",
      "encoder.layer.6.attention.attn.k.bias: False\n",
      "encoder.layer.6.attention.attn.v.weight: False\n",
      "encoder.layer.6.attention.attn.v.bias: False\n",
      "encoder.layer.6.attention.attn.o.weight: False\n",
      "encoder.layer.6.attention.attn.o.bias: False\n",
      "encoder.layer.6.attention.LayerNorm.weight: False\n",
      "encoder.layer.6.attention.LayerNorm.bias: False\n",
      "encoder.layer.6.intermediate.dense.weight: False\n",
      "encoder.layer.6.intermediate.dense.bias: False\n",
      "encoder.layer.6.output.dense.weight: False\n",
      "encoder.layer.6.output.dense.bias: False\n",
      "encoder.layer.6.output.LayerNorm.weight: False\n",
      "encoder.layer.6.output.LayerNorm.bias: False\n",
      "encoder.layer.7.attention.attn.q.weight: False\n",
      "encoder.layer.7.attention.attn.q.bias: False\n",
      "encoder.layer.7.attention.attn.k.weight: False\n",
      "encoder.layer.7.attention.attn.k.bias: False\n",
      "encoder.layer.7.attention.attn.v.weight: False\n",
      "encoder.layer.7.attention.attn.v.bias: False\n",
      "encoder.layer.7.attention.attn.o.weight: False\n",
      "encoder.layer.7.attention.attn.o.bias: False\n",
      "encoder.layer.7.attention.LayerNorm.weight: False\n",
      "encoder.layer.7.attention.LayerNorm.bias: False\n",
      "encoder.layer.7.intermediate.dense.weight: False\n",
      "encoder.layer.7.intermediate.dense.bias: False\n",
      "encoder.layer.7.output.dense.weight: False\n",
      "encoder.layer.7.output.dense.bias: False\n",
      "encoder.layer.7.output.LayerNorm.weight: False\n",
      "encoder.layer.7.output.LayerNorm.bias: False\n",
      "encoder.layer.8.attention.attn.q.weight: False\n",
      "encoder.layer.8.attention.attn.q.bias: False\n",
      "encoder.layer.8.attention.attn.k.weight: False\n",
      "encoder.layer.8.attention.attn.k.bias: False\n",
      "encoder.layer.8.attention.attn.v.weight: False\n",
      "encoder.layer.8.attention.attn.v.bias: False\n",
      "encoder.layer.8.attention.attn.o.weight: False\n",
      "encoder.layer.8.attention.attn.o.bias: False\n",
      "encoder.layer.8.attention.LayerNorm.weight: False\n",
      "encoder.layer.8.attention.LayerNorm.bias: False\n",
      "encoder.layer.8.intermediate.dense.weight: False\n",
      "encoder.layer.8.intermediate.dense.bias: False\n",
      "encoder.layer.8.output.dense.weight: False\n",
      "encoder.layer.8.output.dense.bias: False\n",
      "encoder.layer.8.output.LayerNorm.weight: False\n",
      "encoder.layer.8.output.LayerNorm.bias: False\n",
      "encoder.layer.9.attention.attn.q.weight: True\n",
      "encoder.layer.9.attention.attn.q.bias: True\n",
      "encoder.layer.9.attention.attn.k.weight: True\n",
      "encoder.layer.9.attention.attn.k.bias: True\n",
      "encoder.layer.9.attention.attn.v.weight: True\n",
      "encoder.layer.9.attention.attn.v.bias: True\n",
      "encoder.layer.9.attention.attn.o.weight: True\n",
      "encoder.layer.9.attention.attn.o.bias: True\n",
      "encoder.layer.9.attention.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.attn.q.weight: True\n",
      "encoder.layer.10.attention.attn.q.bias: True\n",
      "encoder.layer.10.attention.attn.k.weight: True\n",
      "encoder.layer.10.attention.attn.k.bias: True\n",
      "encoder.layer.10.attention.attn.v.weight: True\n",
      "encoder.layer.10.attention.attn.v.bias: True\n",
      "encoder.layer.10.attention.attn.o.weight: True\n",
      "encoder.layer.10.attention.attn.o.bias: True\n",
      "encoder.layer.10.attention.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.attn.q.weight: True\n",
      "encoder.layer.11.attention.attn.q.bias: True\n",
      "encoder.layer.11.attention.attn.k.weight: True\n",
      "encoder.layer.11.attention.attn.k.bias: True\n",
      "encoder.layer.11.attention.attn.v.weight: True\n",
      "encoder.layer.11.attention.attn.v.bias: True\n",
      "encoder.layer.11.attention.attn.o.weight: True\n",
      "encoder.layer.11.attention.attn.o.bias: True\n",
      "encoder.layer.11.attention.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "encoder.relative_attention_bias.weight: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61dd059384524f81bd63812e418f5423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2e1c7d93fa4e1381745516f66019d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First unfrozen layer: 10\n",
      "Model :  MPNet\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/mpnet-base\n",
      "Before training -------------------\n",
      "embeddings.word_embeddings.weight: False\n",
      "embeddings.position_embeddings.weight: False\n",
      "embeddings.LayerNorm.weight: False\n",
      "embeddings.LayerNorm.bias: False\n",
      "encoder.layer.0.attention.attn.q.weight: False\n",
      "encoder.layer.0.attention.attn.q.bias: False\n",
      "encoder.layer.0.attention.attn.k.weight: False\n",
      "encoder.layer.0.attention.attn.k.bias: False\n",
      "encoder.layer.0.attention.attn.v.weight: False\n",
      "encoder.layer.0.attention.attn.v.bias: False\n",
      "encoder.layer.0.attention.attn.o.weight: False\n",
      "encoder.layer.0.attention.attn.o.bias: False\n",
      "encoder.layer.0.attention.LayerNorm.weight: False\n",
      "encoder.layer.0.attention.LayerNorm.bias: False\n",
      "encoder.layer.0.intermediate.dense.weight: False\n",
      "encoder.layer.0.intermediate.dense.bias: False\n",
      "encoder.layer.0.output.dense.weight: False\n",
      "encoder.layer.0.output.dense.bias: False\n",
      "encoder.layer.0.output.LayerNorm.weight: False\n",
      "encoder.layer.0.output.LayerNorm.bias: False\n",
      "encoder.layer.1.attention.attn.q.weight: False\n",
      "encoder.layer.1.attention.attn.q.bias: False\n",
      "encoder.layer.1.attention.attn.k.weight: False\n",
      "encoder.layer.1.attention.attn.k.bias: False\n",
      "encoder.layer.1.attention.attn.v.weight: False\n",
      "encoder.layer.1.attention.attn.v.bias: False\n",
      "encoder.layer.1.attention.attn.o.weight: False\n",
      "encoder.layer.1.attention.attn.o.bias: False\n",
      "encoder.layer.1.attention.LayerNorm.weight: False\n",
      "encoder.layer.1.attention.LayerNorm.bias: False\n",
      "encoder.layer.1.intermediate.dense.weight: False\n",
      "encoder.layer.1.intermediate.dense.bias: False\n",
      "encoder.layer.1.output.dense.weight: False\n",
      "encoder.layer.1.output.dense.bias: False\n",
      "encoder.layer.1.output.LayerNorm.weight: False\n",
      "encoder.layer.1.output.LayerNorm.bias: False\n",
      "encoder.layer.2.attention.attn.q.weight: False\n",
      "encoder.layer.2.attention.attn.q.bias: False\n",
      "encoder.layer.2.attention.attn.k.weight: False\n",
      "encoder.layer.2.attention.attn.k.bias: False\n",
      "encoder.layer.2.attention.attn.v.weight: False\n",
      "encoder.layer.2.attention.attn.v.bias: False\n",
      "encoder.layer.2.attention.attn.o.weight: False\n",
      "encoder.layer.2.attention.attn.o.bias: False\n",
      "encoder.layer.2.attention.LayerNorm.weight: False\n",
      "encoder.layer.2.attention.LayerNorm.bias: False\n",
      "encoder.layer.2.intermediate.dense.weight: False\n",
      "encoder.layer.2.intermediate.dense.bias: False\n",
      "encoder.layer.2.output.dense.weight: False\n",
      "encoder.layer.2.output.dense.bias: False\n",
      "encoder.layer.2.output.LayerNorm.weight: False\n",
      "encoder.layer.2.output.LayerNorm.bias: False\n",
      "encoder.layer.3.attention.attn.q.weight: False\n",
      "encoder.layer.3.attention.attn.q.bias: False\n",
      "encoder.layer.3.attention.attn.k.weight: False\n",
      "encoder.layer.3.attention.attn.k.bias: False\n",
      "encoder.layer.3.attention.attn.v.weight: False\n",
      "encoder.layer.3.attention.attn.v.bias: False\n",
      "encoder.layer.3.attention.attn.o.weight: False\n",
      "encoder.layer.3.attention.attn.o.bias: False\n",
      "encoder.layer.3.attention.LayerNorm.weight: False\n",
      "encoder.layer.3.attention.LayerNorm.bias: False\n",
      "encoder.layer.3.intermediate.dense.weight: False\n",
      "encoder.layer.3.intermediate.dense.bias: False\n",
      "encoder.layer.3.output.dense.weight: False\n",
      "encoder.layer.3.output.dense.bias: False\n",
      "encoder.layer.3.output.LayerNorm.weight: False\n",
      "encoder.layer.3.output.LayerNorm.bias: False\n",
      "encoder.layer.4.attention.attn.q.weight: False\n",
      "encoder.layer.4.attention.attn.q.bias: False\n",
      "encoder.layer.4.attention.attn.k.weight: False\n",
      "encoder.layer.4.attention.attn.k.bias: False\n",
      "encoder.layer.4.attention.attn.v.weight: False\n",
      "encoder.layer.4.attention.attn.v.bias: False\n",
      "encoder.layer.4.attention.attn.o.weight: False\n",
      "encoder.layer.4.attention.attn.o.bias: False\n",
      "encoder.layer.4.attention.LayerNorm.weight: False\n",
      "encoder.layer.4.attention.LayerNorm.bias: False\n",
      "encoder.layer.4.intermediate.dense.weight: False\n",
      "encoder.layer.4.intermediate.dense.bias: False\n",
      "encoder.layer.4.output.dense.weight: False\n",
      "encoder.layer.4.output.dense.bias: False\n",
      "encoder.layer.4.output.LayerNorm.weight: False\n",
      "encoder.layer.4.output.LayerNorm.bias: False\n",
      "encoder.layer.5.attention.attn.q.weight: False\n",
      "encoder.layer.5.attention.attn.q.bias: False\n",
      "encoder.layer.5.attention.attn.k.weight: False\n",
      "encoder.layer.5.attention.attn.k.bias: False\n",
      "encoder.layer.5.attention.attn.v.weight: False\n",
      "encoder.layer.5.attention.attn.v.bias: False\n",
      "encoder.layer.5.attention.attn.o.weight: False\n",
      "encoder.layer.5.attention.attn.o.bias: False\n",
      "encoder.layer.5.attention.LayerNorm.weight: False\n",
      "encoder.layer.5.attention.LayerNorm.bias: False\n",
      "encoder.layer.5.intermediate.dense.weight: False\n",
      "encoder.layer.5.intermediate.dense.bias: False\n",
      "encoder.layer.5.output.dense.weight: False\n",
      "encoder.layer.5.output.dense.bias: False\n",
      "encoder.layer.5.output.LayerNorm.weight: False\n",
      "encoder.layer.5.output.LayerNorm.bias: False\n",
      "encoder.layer.6.attention.attn.q.weight: False\n",
      "encoder.layer.6.attention.attn.q.bias: False\n",
      "encoder.layer.6.attention.attn.k.weight: False\n",
      "encoder.layer.6.attention.attn.k.bias: False\n",
      "encoder.layer.6.attention.attn.v.weight: False\n",
      "encoder.layer.6.attention.attn.v.bias: False\n",
      "encoder.layer.6.attention.attn.o.weight: False\n",
      "encoder.layer.6.attention.attn.o.bias: False\n",
      "encoder.layer.6.attention.LayerNorm.weight: False\n",
      "encoder.layer.6.attention.LayerNorm.bias: False\n",
      "encoder.layer.6.intermediate.dense.weight: False\n",
      "encoder.layer.6.intermediate.dense.bias: False\n",
      "encoder.layer.6.output.dense.weight: False\n",
      "encoder.layer.6.output.dense.bias: False\n",
      "encoder.layer.6.output.LayerNorm.weight: False\n",
      "encoder.layer.6.output.LayerNorm.bias: False\n",
      "encoder.layer.7.attention.attn.q.weight: False\n",
      "encoder.layer.7.attention.attn.q.bias: False\n",
      "encoder.layer.7.attention.attn.k.weight: False\n",
      "encoder.layer.7.attention.attn.k.bias: False\n",
      "encoder.layer.7.attention.attn.v.weight: False\n",
      "encoder.layer.7.attention.attn.v.bias: False\n",
      "encoder.layer.7.attention.attn.o.weight: False\n",
      "encoder.layer.7.attention.attn.o.bias: False\n",
      "encoder.layer.7.attention.LayerNorm.weight: False\n",
      "encoder.layer.7.attention.LayerNorm.bias: False\n",
      "encoder.layer.7.intermediate.dense.weight: False\n",
      "encoder.layer.7.intermediate.dense.bias: False\n",
      "encoder.layer.7.output.dense.weight: False\n",
      "encoder.layer.7.output.dense.bias: False\n",
      "encoder.layer.7.output.LayerNorm.weight: False\n",
      "encoder.layer.7.output.LayerNorm.bias: False\n",
      "encoder.layer.8.attention.attn.q.weight: False\n",
      "encoder.layer.8.attention.attn.q.bias: False\n",
      "encoder.layer.8.attention.attn.k.weight: False\n",
      "encoder.layer.8.attention.attn.k.bias: False\n",
      "encoder.layer.8.attention.attn.v.weight: False\n",
      "encoder.layer.8.attention.attn.v.bias: False\n",
      "encoder.layer.8.attention.attn.o.weight: False\n",
      "encoder.layer.8.attention.attn.o.bias: False\n",
      "encoder.layer.8.attention.LayerNorm.weight: False\n",
      "encoder.layer.8.attention.LayerNorm.bias: False\n",
      "encoder.layer.8.intermediate.dense.weight: False\n",
      "encoder.layer.8.intermediate.dense.bias: False\n",
      "encoder.layer.8.output.dense.weight: False\n",
      "encoder.layer.8.output.dense.bias: False\n",
      "encoder.layer.8.output.LayerNorm.weight: False\n",
      "encoder.layer.8.output.LayerNorm.bias: False\n",
      "encoder.layer.9.attention.attn.q.weight: False\n",
      "encoder.layer.9.attention.attn.q.bias: False\n",
      "encoder.layer.9.attention.attn.k.weight: False\n",
      "encoder.layer.9.attention.attn.k.bias: False\n",
      "encoder.layer.9.attention.attn.v.weight: False\n",
      "encoder.layer.9.attention.attn.v.bias: False\n",
      "encoder.layer.9.attention.attn.o.weight: False\n",
      "encoder.layer.9.attention.attn.o.bias: False\n",
      "encoder.layer.9.attention.LayerNorm.weight: False\n",
      "encoder.layer.9.attention.LayerNorm.bias: False\n",
      "encoder.layer.9.intermediate.dense.weight: False\n",
      "encoder.layer.9.intermediate.dense.bias: False\n",
      "encoder.layer.9.output.dense.weight: False\n",
      "encoder.layer.9.output.dense.bias: False\n",
      "encoder.layer.9.output.LayerNorm.weight: False\n",
      "encoder.layer.9.output.LayerNorm.bias: False\n",
      "encoder.layer.10.attention.attn.q.weight: True\n",
      "encoder.layer.10.attention.attn.q.bias: True\n",
      "encoder.layer.10.attention.attn.k.weight: True\n",
      "encoder.layer.10.attention.attn.k.bias: True\n",
      "encoder.layer.10.attention.attn.v.weight: True\n",
      "encoder.layer.10.attention.attn.v.bias: True\n",
      "encoder.layer.10.attention.attn.o.weight: True\n",
      "encoder.layer.10.attention.attn.o.bias: True\n",
      "encoder.layer.10.attention.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.attn.q.weight: True\n",
      "encoder.layer.11.attention.attn.q.bias: True\n",
      "encoder.layer.11.attention.attn.k.weight: True\n",
      "encoder.layer.11.attention.attn.k.bias: True\n",
      "encoder.layer.11.attention.attn.v.weight: True\n",
      "encoder.layer.11.attention.attn.v.bias: True\n",
      "encoder.layer.11.attention.attn.o.weight: True\n",
      "encoder.layer.11.attention.attn.o.bias: True\n",
      "encoder.layer.11.attention.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "encoder.relative_attention_bias.weight: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937295a4a6c3453ba4b2fcb5e3e00eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b2d224f4d54d858129bad06943ef04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First unfrozen layer: 11\n",
      "Model :  MPNet\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/mpnet-base\n",
      "Before training -------------------\n",
      "embeddings.word_embeddings.weight: False\n",
      "embeddings.position_embeddings.weight: False\n",
      "embeddings.LayerNorm.weight: False\n",
      "embeddings.LayerNorm.bias: False\n",
      "encoder.layer.0.attention.attn.q.weight: False\n",
      "encoder.layer.0.attention.attn.q.bias: False\n",
      "encoder.layer.0.attention.attn.k.weight: False\n",
      "encoder.layer.0.attention.attn.k.bias: False\n",
      "encoder.layer.0.attention.attn.v.weight: False\n",
      "encoder.layer.0.attention.attn.v.bias: False\n",
      "encoder.layer.0.attention.attn.o.weight: False\n",
      "encoder.layer.0.attention.attn.o.bias: False\n",
      "encoder.layer.0.attention.LayerNorm.weight: False\n",
      "encoder.layer.0.attention.LayerNorm.bias: False\n",
      "encoder.layer.0.intermediate.dense.weight: False\n",
      "encoder.layer.0.intermediate.dense.bias: False\n",
      "encoder.layer.0.output.dense.weight: False\n",
      "encoder.layer.0.output.dense.bias: False\n",
      "encoder.layer.0.output.LayerNorm.weight: False\n",
      "encoder.layer.0.output.LayerNorm.bias: False\n",
      "encoder.layer.1.attention.attn.q.weight: False\n",
      "encoder.layer.1.attention.attn.q.bias: False\n",
      "encoder.layer.1.attention.attn.k.weight: False\n",
      "encoder.layer.1.attention.attn.k.bias: False\n",
      "encoder.layer.1.attention.attn.v.weight: False\n",
      "encoder.layer.1.attention.attn.v.bias: False\n",
      "encoder.layer.1.attention.attn.o.weight: False\n",
      "encoder.layer.1.attention.attn.o.bias: False\n",
      "encoder.layer.1.attention.LayerNorm.weight: False\n",
      "encoder.layer.1.attention.LayerNorm.bias: False\n",
      "encoder.layer.1.intermediate.dense.weight: False\n",
      "encoder.layer.1.intermediate.dense.bias: False\n",
      "encoder.layer.1.output.dense.weight: False\n",
      "encoder.layer.1.output.dense.bias: False\n",
      "encoder.layer.1.output.LayerNorm.weight: False\n",
      "encoder.layer.1.output.LayerNorm.bias: False\n",
      "encoder.layer.2.attention.attn.q.weight: False\n",
      "encoder.layer.2.attention.attn.q.bias: False\n",
      "encoder.layer.2.attention.attn.k.weight: False\n",
      "encoder.layer.2.attention.attn.k.bias: False\n",
      "encoder.layer.2.attention.attn.v.weight: False\n",
      "encoder.layer.2.attention.attn.v.bias: False\n",
      "encoder.layer.2.attention.attn.o.weight: False\n",
      "encoder.layer.2.attention.attn.o.bias: False\n",
      "encoder.layer.2.attention.LayerNorm.weight: False\n",
      "encoder.layer.2.attention.LayerNorm.bias: False\n",
      "encoder.layer.2.intermediate.dense.weight: False\n",
      "encoder.layer.2.intermediate.dense.bias: False\n",
      "encoder.layer.2.output.dense.weight: False\n",
      "encoder.layer.2.output.dense.bias: False\n",
      "encoder.layer.2.output.LayerNorm.weight: False\n",
      "encoder.layer.2.output.LayerNorm.bias: False\n",
      "encoder.layer.3.attention.attn.q.weight: False\n",
      "encoder.layer.3.attention.attn.q.bias: False\n",
      "encoder.layer.3.attention.attn.k.weight: False\n",
      "encoder.layer.3.attention.attn.k.bias: False\n",
      "encoder.layer.3.attention.attn.v.weight: False\n",
      "encoder.layer.3.attention.attn.v.bias: False\n",
      "encoder.layer.3.attention.attn.o.weight: False\n",
      "encoder.layer.3.attention.attn.o.bias: False\n",
      "encoder.layer.3.attention.LayerNorm.weight: False\n",
      "encoder.layer.3.attention.LayerNorm.bias: False\n",
      "encoder.layer.3.intermediate.dense.weight: False\n",
      "encoder.layer.3.intermediate.dense.bias: False\n",
      "encoder.layer.3.output.dense.weight: False\n",
      "encoder.layer.3.output.dense.bias: False\n",
      "encoder.layer.3.output.LayerNorm.weight: False\n",
      "encoder.layer.3.output.LayerNorm.bias: False\n",
      "encoder.layer.4.attention.attn.q.weight: False\n",
      "encoder.layer.4.attention.attn.q.bias: False\n",
      "encoder.layer.4.attention.attn.k.weight: False\n",
      "encoder.layer.4.attention.attn.k.bias: False\n",
      "encoder.layer.4.attention.attn.v.weight: False\n",
      "encoder.layer.4.attention.attn.v.bias: False\n",
      "encoder.layer.4.attention.attn.o.weight: False\n",
      "encoder.layer.4.attention.attn.o.bias: False\n",
      "encoder.layer.4.attention.LayerNorm.weight: False\n",
      "encoder.layer.4.attention.LayerNorm.bias: False\n",
      "encoder.layer.4.intermediate.dense.weight: False\n",
      "encoder.layer.4.intermediate.dense.bias: False\n",
      "encoder.layer.4.output.dense.weight: False\n",
      "encoder.layer.4.output.dense.bias: False\n",
      "encoder.layer.4.output.LayerNorm.weight: False\n",
      "encoder.layer.4.output.LayerNorm.bias: False\n",
      "encoder.layer.5.attention.attn.q.weight: False\n",
      "encoder.layer.5.attention.attn.q.bias: False\n",
      "encoder.layer.5.attention.attn.k.weight: False\n",
      "encoder.layer.5.attention.attn.k.bias: False\n",
      "encoder.layer.5.attention.attn.v.weight: False\n",
      "encoder.layer.5.attention.attn.v.bias: False\n",
      "encoder.layer.5.attention.attn.o.weight: False\n",
      "encoder.layer.5.attention.attn.o.bias: False\n",
      "encoder.layer.5.attention.LayerNorm.weight: False\n",
      "encoder.layer.5.attention.LayerNorm.bias: False\n",
      "encoder.layer.5.intermediate.dense.weight: False\n",
      "encoder.layer.5.intermediate.dense.bias: False\n",
      "encoder.layer.5.output.dense.weight: False\n",
      "encoder.layer.5.output.dense.bias: False\n",
      "encoder.layer.5.output.LayerNorm.weight: False\n",
      "encoder.layer.5.output.LayerNorm.bias: False\n",
      "encoder.layer.6.attention.attn.q.weight: False\n",
      "encoder.layer.6.attention.attn.q.bias: False\n",
      "encoder.layer.6.attention.attn.k.weight: False\n",
      "encoder.layer.6.attention.attn.k.bias: False\n",
      "encoder.layer.6.attention.attn.v.weight: False\n",
      "encoder.layer.6.attention.attn.v.bias: False\n",
      "encoder.layer.6.attention.attn.o.weight: False\n",
      "encoder.layer.6.attention.attn.o.bias: False\n",
      "encoder.layer.6.attention.LayerNorm.weight: False\n",
      "encoder.layer.6.attention.LayerNorm.bias: False\n",
      "encoder.layer.6.intermediate.dense.weight: False\n",
      "encoder.layer.6.intermediate.dense.bias: False\n",
      "encoder.layer.6.output.dense.weight: False\n",
      "encoder.layer.6.output.dense.bias: False\n",
      "encoder.layer.6.output.LayerNorm.weight: False\n",
      "encoder.layer.6.output.LayerNorm.bias: False\n",
      "encoder.layer.7.attention.attn.q.weight: False\n",
      "encoder.layer.7.attention.attn.q.bias: False\n",
      "encoder.layer.7.attention.attn.k.weight: False\n",
      "encoder.layer.7.attention.attn.k.bias: False\n",
      "encoder.layer.7.attention.attn.v.weight: False\n",
      "encoder.layer.7.attention.attn.v.bias: False\n",
      "encoder.layer.7.attention.attn.o.weight: False\n",
      "encoder.layer.7.attention.attn.o.bias: False\n",
      "encoder.layer.7.attention.LayerNorm.weight: False\n",
      "encoder.layer.7.attention.LayerNorm.bias: False\n",
      "encoder.layer.7.intermediate.dense.weight: False\n",
      "encoder.layer.7.intermediate.dense.bias: False\n",
      "encoder.layer.7.output.dense.weight: False\n",
      "encoder.layer.7.output.dense.bias: False\n",
      "encoder.layer.7.output.LayerNorm.weight: False\n",
      "encoder.layer.7.output.LayerNorm.bias: False\n",
      "encoder.layer.8.attention.attn.q.weight: False\n",
      "encoder.layer.8.attention.attn.q.bias: False\n",
      "encoder.layer.8.attention.attn.k.weight: False\n",
      "encoder.layer.8.attention.attn.k.bias: False\n",
      "encoder.layer.8.attention.attn.v.weight: False\n",
      "encoder.layer.8.attention.attn.v.bias: False\n",
      "encoder.layer.8.attention.attn.o.weight: False\n",
      "encoder.layer.8.attention.attn.o.bias: False\n",
      "encoder.layer.8.attention.LayerNorm.weight: False\n",
      "encoder.layer.8.attention.LayerNorm.bias: False\n",
      "encoder.layer.8.intermediate.dense.weight: False\n",
      "encoder.layer.8.intermediate.dense.bias: False\n",
      "encoder.layer.8.output.dense.weight: False\n",
      "encoder.layer.8.output.dense.bias: False\n",
      "encoder.layer.8.output.LayerNorm.weight: False\n",
      "encoder.layer.8.output.LayerNorm.bias: False\n",
      "encoder.layer.9.attention.attn.q.weight: False\n",
      "encoder.layer.9.attention.attn.q.bias: False\n",
      "encoder.layer.9.attention.attn.k.weight: False\n",
      "encoder.layer.9.attention.attn.k.bias: False\n",
      "encoder.layer.9.attention.attn.v.weight: False\n",
      "encoder.layer.9.attention.attn.v.bias: False\n",
      "encoder.layer.9.attention.attn.o.weight: False\n",
      "encoder.layer.9.attention.attn.o.bias: False\n",
      "encoder.layer.9.attention.LayerNorm.weight: False\n",
      "encoder.layer.9.attention.LayerNorm.bias: False\n",
      "encoder.layer.9.intermediate.dense.weight: False\n",
      "encoder.layer.9.intermediate.dense.bias: False\n",
      "encoder.layer.9.output.dense.weight: False\n",
      "encoder.layer.9.output.dense.bias: False\n",
      "encoder.layer.9.output.LayerNorm.weight: False\n",
      "encoder.layer.9.output.LayerNorm.bias: False\n",
      "encoder.layer.10.attention.attn.q.weight: False\n",
      "encoder.layer.10.attention.attn.q.bias: False\n",
      "encoder.layer.10.attention.attn.k.weight: False\n",
      "encoder.layer.10.attention.attn.k.bias: False\n",
      "encoder.layer.10.attention.attn.v.weight: False\n",
      "encoder.layer.10.attention.attn.v.bias: False\n",
      "encoder.layer.10.attention.attn.o.weight: False\n",
      "encoder.layer.10.attention.attn.o.bias: False\n",
      "encoder.layer.10.attention.LayerNorm.weight: False\n",
      "encoder.layer.10.attention.LayerNorm.bias: False\n",
      "encoder.layer.10.intermediate.dense.weight: False\n",
      "encoder.layer.10.intermediate.dense.bias: False\n",
      "encoder.layer.10.output.dense.weight: False\n",
      "encoder.layer.10.output.dense.bias: False\n",
      "encoder.layer.10.output.LayerNorm.weight: False\n",
      "encoder.layer.10.output.LayerNorm.bias: False\n",
      "encoder.layer.11.attention.attn.q.weight: True\n",
      "encoder.layer.11.attention.attn.q.bias: True\n",
      "encoder.layer.11.attention.attn.k.weight: True\n",
      "encoder.layer.11.attention.attn.k.bias: True\n",
      "encoder.layer.11.attention.attn.v.weight: True\n",
      "encoder.layer.11.attention.attn.v.bias: True\n",
      "encoder.layer.11.attention.attn.o.weight: True\n",
      "encoder.layer.11.attention.attn.o.bias: True\n",
      "encoder.layer.11.attention.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "encoder.relative_attention_bias.weight: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484d06807a23454d985c1df57fe2b238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7eb817906f49ad93ea8381fe537b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable \t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First unfrozen layer: 12\n",
      "Model :  MPNet\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/mpnet-base\n",
      "Before training -------------------\n",
      "embeddings.word_embeddings.weight: False\n",
      "embeddings.position_embeddings.weight: False\n",
      "embeddings.LayerNorm.weight: False\n",
      "embeddings.LayerNorm.bias: False\n",
      "encoder.layer.0.attention.attn.q.weight: False\n",
      "encoder.layer.0.attention.attn.q.bias: False\n",
      "encoder.layer.0.attention.attn.k.weight: False\n",
      "encoder.layer.0.attention.attn.k.bias: False\n",
      "encoder.layer.0.attention.attn.v.weight: False\n",
      "encoder.layer.0.attention.attn.v.bias: False\n",
      "encoder.layer.0.attention.attn.o.weight: False\n",
      "encoder.layer.0.attention.attn.o.bias: False\n",
      "encoder.layer.0.attention.LayerNorm.weight: False\n",
      "encoder.layer.0.attention.LayerNorm.bias: False\n",
      "encoder.layer.0.intermediate.dense.weight: False\n",
      "encoder.layer.0.intermediate.dense.bias: False\n",
      "encoder.layer.0.output.dense.weight: False\n",
      "encoder.layer.0.output.dense.bias: False\n",
      "encoder.layer.0.output.LayerNorm.weight: False\n",
      "encoder.layer.0.output.LayerNorm.bias: False\n",
      "encoder.layer.1.attention.attn.q.weight: False\n",
      "encoder.layer.1.attention.attn.q.bias: False\n",
      "encoder.layer.1.attention.attn.k.weight: False\n",
      "encoder.layer.1.attention.attn.k.bias: False\n",
      "encoder.layer.1.attention.attn.v.weight: False\n",
      "encoder.layer.1.attention.attn.v.bias: False\n",
      "encoder.layer.1.attention.attn.o.weight: False\n",
      "encoder.layer.1.attention.attn.o.bias: False\n",
      "encoder.layer.1.attention.LayerNorm.weight: False\n",
      "encoder.layer.1.attention.LayerNorm.bias: False\n",
      "encoder.layer.1.intermediate.dense.weight: False\n",
      "encoder.layer.1.intermediate.dense.bias: False\n",
      "encoder.layer.1.output.dense.weight: False\n",
      "encoder.layer.1.output.dense.bias: False\n",
      "encoder.layer.1.output.LayerNorm.weight: False\n",
      "encoder.layer.1.output.LayerNorm.bias: False\n",
      "encoder.layer.2.attention.attn.q.weight: False\n",
      "encoder.layer.2.attention.attn.q.bias: False\n",
      "encoder.layer.2.attention.attn.k.weight: False\n",
      "encoder.layer.2.attention.attn.k.bias: False\n",
      "encoder.layer.2.attention.attn.v.weight: False\n",
      "encoder.layer.2.attention.attn.v.bias: False\n",
      "encoder.layer.2.attention.attn.o.weight: False\n",
      "encoder.layer.2.attention.attn.o.bias: False\n",
      "encoder.layer.2.attention.LayerNorm.weight: False\n",
      "encoder.layer.2.attention.LayerNorm.bias: False\n",
      "encoder.layer.2.intermediate.dense.weight: False\n",
      "encoder.layer.2.intermediate.dense.bias: False\n",
      "encoder.layer.2.output.dense.weight: False\n",
      "encoder.layer.2.output.dense.bias: False\n",
      "encoder.layer.2.output.LayerNorm.weight: False\n",
      "encoder.layer.2.output.LayerNorm.bias: False\n",
      "encoder.layer.3.attention.attn.q.weight: False\n",
      "encoder.layer.3.attention.attn.q.bias: False\n",
      "encoder.layer.3.attention.attn.k.weight: False\n",
      "encoder.layer.3.attention.attn.k.bias: False\n",
      "encoder.layer.3.attention.attn.v.weight: False\n",
      "encoder.layer.3.attention.attn.v.bias: False\n",
      "encoder.layer.3.attention.attn.o.weight: False\n",
      "encoder.layer.3.attention.attn.o.bias: False\n",
      "encoder.layer.3.attention.LayerNorm.weight: False\n",
      "encoder.layer.3.attention.LayerNorm.bias: False\n",
      "encoder.layer.3.intermediate.dense.weight: False\n",
      "encoder.layer.3.intermediate.dense.bias: False\n",
      "encoder.layer.3.output.dense.weight: False\n",
      "encoder.layer.3.output.dense.bias: False\n",
      "encoder.layer.3.output.LayerNorm.weight: False\n",
      "encoder.layer.3.output.LayerNorm.bias: False\n",
      "encoder.layer.4.attention.attn.q.weight: False\n",
      "encoder.layer.4.attention.attn.q.bias: False\n",
      "encoder.layer.4.attention.attn.k.weight: False\n",
      "encoder.layer.4.attention.attn.k.bias: False\n",
      "encoder.layer.4.attention.attn.v.weight: False\n",
      "encoder.layer.4.attention.attn.v.bias: False\n",
      "encoder.layer.4.attention.attn.o.weight: False\n",
      "encoder.layer.4.attention.attn.o.bias: False\n",
      "encoder.layer.4.attention.LayerNorm.weight: False\n",
      "encoder.layer.4.attention.LayerNorm.bias: False\n",
      "encoder.layer.4.intermediate.dense.weight: False\n",
      "encoder.layer.4.intermediate.dense.bias: False\n",
      "encoder.layer.4.output.dense.weight: False\n",
      "encoder.layer.4.output.dense.bias: False\n",
      "encoder.layer.4.output.LayerNorm.weight: False\n",
      "encoder.layer.4.output.LayerNorm.bias: False\n",
      "encoder.layer.5.attention.attn.q.weight: False\n",
      "encoder.layer.5.attention.attn.q.bias: False\n",
      "encoder.layer.5.attention.attn.k.weight: False\n",
      "encoder.layer.5.attention.attn.k.bias: False\n",
      "encoder.layer.5.attention.attn.v.weight: False\n",
      "encoder.layer.5.attention.attn.v.bias: False\n",
      "encoder.layer.5.attention.attn.o.weight: False\n",
      "encoder.layer.5.attention.attn.o.bias: False\n",
      "encoder.layer.5.attention.LayerNorm.weight: False\n",
      "encoder.layer.5.attention.LayerNorm.bias: False\n",
      "encoder.layer.5.intermediate.dense.weight: False\n",
      "encoder.layer.5.intermediate.dense.bias: False\n",
      "encoder.layer.5.output.dense.weight: False\n",
      "encoder.layer.5.output.dense.bias: False\n",
      "encoder.layer.5.output.LayerNorm.weight: False\n",
      "encoder.layer.5.output.LayerNorm.bias: False\n",
      "encoder.layer.6.attention.attn.q.weight: False\n",
      "encoder.layer.6.attention.attn.q.bias: False\n",
      "encoder.layer.6.attention.attn.k.weight: False\n",
      "encoder.layer.6.attention.attn.k.bias: False\n",
      "encoder.layer.6.attention.attn.v.weight: False\n",
      "encoder.layer.6.attention.attn.v.bias: False\n",
      "encoder.layer.6.attention.attn.o.weight: False\n",
      "encoder.layer.6.attention.attn.o.bias: False\n",
      "encoder.layer.6.attention.LayerNorm.weight: False\n",
      "encoder.layer.6.attention.LayerNorm.bias: False\n",
      "encoder.layer.6.intermediate.dense.weight: False\n",
      "encoder.layer.6.intermediate.dense.bias: False\n",
      "encoder.layer.6.output.dense.weight: False\n",
      "encoder.layer.6.output.dense.bias: False\n",
      "encoder.layer.6.output.LayerNorm.weight: False\n",
      "encoder.layer.6.output.LayerNorm.bias: False\n",
      "encoder.layer.7.attention.attn.q.weight: False\n",
      "encoder.layer.7.attention.attn.q.bias: False\n",
      "encoder.layer.7.attention.attn.k.weight: False\n",
      "encoder.layer.7.attention.attn.k.bias: False\n",
      "encoder.layer.7.attention.attn.v.weight: False\n",
      "encoder.layer.7.attention.attn.v.bias: False\n",
      "encoder.layer.7.attention.attn.o.weight: False\n",
      "encoder.layer.7.attention.attn.o.bias: False\n",
      "encoder.layer.7.attention.LayerNorm.weight: False\n",
      "encoder.layer.7.attention.LayerNorm.bias: False\n",
      "encoder.layer.7.intermediate.dense.weight: False\n",
      "encoder.layer.7.intermediate.dense.bias: False\n",
      "encoder.layer.7.output.dense.weight: False\n",
      "encoder.layer.7.output.dense.bias: False\n",
      "encoder.layer.7.output.LayerNorm.weight: False\n",
      "encoder.layer.7.output.LayerNorm.bias: False\n",
      "encoder.layer.8.attention.attn.q.weight: False\n",
      "encoder.layer.8.attention.attn.q.bias: False\n",
      "encoder.layer.8.attention.attn.k.weight: False\n",
      "encoder.layer.8.attention.attn.k.bias: False\n",
      "encoder.layer.8.attention.attn.v.weight: False\n",
      "encoder.layer.8.attention.attn.v.bias: False\n",
      "encoder.layer.8.attention.attn.o.weight: False\n",
      "encoder.layer.8.attention.attn.o.bias: False\n",
      "encoder.layer.8.attention.LayerNorm.weight: False\n",
      "encoder.layer.8.attention.LayerNorm.bias: False\n",
      "encoder.layer.8.intermediate.dense.weight: False\n",
      "encoder.layer.8.intermediate.dense.bias: False\n",
      "encoder.layer.8.output.dense.weight: False\n",
      "encoder.layer.8.output.dense.bias: False\n",
      "encoder.layer.8.output.LayerNorm.weight: False\n",
      "encoder.layer.8.output.LayerNorm.bias: False\n",
      "encoder.layer.9.attention.attn.q.weight: False\n",
      "encoder.layer.9.attention.attn.q.bias: False\n",
      "encoder.layer.9.attention.attn.k.weight: False\n",
      "encoder.layer.9.attention.attn.k.bias: False\n",
      "encoder.layer.9.attention.attn.v.weight: False\n",
      "encoder.layer.9.attention.attn.v.bias: False\n",
      "encoder.layer.9.attention.attn.o.weight: False\n",
      "encoder.layer.9.attention.attn.o.bias: False\n",
      "encoder.layer.9.attention.LayerNorm.weight: False\n",
      "encoder.layer.9.attention.LayerNorm.bias: False\n",
      "encoder.layer.9.intermediate.dense.weight: False\n",
      "encoder.layer.9.intermediate.dense.bias: False\n",
      "encoder.layer.9.output.dense.weight: False\n",
      "encoder.layer.9.output.dense.bias: False\n",
      "encoder.layer.9.output.LayerNorm.weight: False\n",
      "encoder.layer.9.output.LayerNorm.bias: False\n",
      "encoder.layer.10.attention.attn.q.weight: False\n",
      "encoder.layer.10.attention.attn.q.bias: False\n",
      "encoder.layer.10.attention.attn.k.weight: False\n",
      "encoder.layer.10.attention.attn.k.bias: False\n",
      "encoder.layer.10.attention.attn.v.weight: False\n",
      "encoder.layer.10.attention.attn.v.bias: False\n",
      "encoder.layer.10.attention.attn.o.weight: False\n",
      "encoder.layer.10.attention.attn.o.bias: False\n",
      "encoder.layer.10.attention.LayerNorm.weight: False\n",
      "encoder.layer.10.attention.LayerNorm.bias: False\n",
      "encoder.layer.10.intermediate.dense.weight: False\n",
      "encoder.layer.10.intermediate.dense.bias: False\n",
      "encoder.layer.10.output.dense.weight: False\n",
      "encoder.layer.10.output.dense.bias: False\n",
      "encoder.layer.10.output.LayerNorm.weight: False\n",
      "encoder.layer.10.output.LayerNorm.bias: False\n",
      "encoder.layer.11.attention.attn.q.weight: False\n",
      "encoder.layer.11.attention.attn.q.bias: False\n",
      "encoder.layer.11.attention.attn.k.weight: False\n",
      "encoder.layer.11.attention.attn.k.bias: False\n",
      "encoder.layer.11.attention.attn.v.weight: False\n",
      "encoder.layer.11.attention.attn.v.bias: False\n",
      "encoder.layer.11.attention.attn.o.weight: False\n",
      "encoder.layer.11.attention.attn.o.bias: False\n",
      "encoder.layer.11.attention.LayerNorm.weight: False\n",
      "encoder.layer.11.attention.LayerNorm.bias: False\n",
      "encoder.layer.11.intermediate.dense.weight: False\n",
      "encoder.layer.11.intermediate.dense.bias: False\n",
      "encoder.layer.11.output.dense.weight: False\n",
      "encoder.layer.11.output.dense.bias: False\n",
      "encoder.layer.11.output.LayerNorm.weight: False\n",
      "encoder.layer.11.output.LayerNorm.bias: False\n",
      "encoder.relative_attention_bias.weight: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9324169377411ebf54c27e26f4fe86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35dd7c798c2343fa95f24a0b984311b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 42min 8s, sys: 1h 32min 19s, total: 3h 14min 28s\n",
      "Wall time: 1h 46min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for first_unfrozen_layer in np.arange(13):\n",
    "    print(\"First unfrozen layer:\", first_unfrozen_layer)\n",
    "    ## set random seeds\n",
    "    seed = 42\n",
    "    # Set the random seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # Set the random seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "    # Set the random seed\n",
    "    random.seed(seed)\n",
    "\n",
    "    ## initialize model and tokenizer\n",
    "    i = 1  # MPNet\n",
    "    model_name = model_names[i]\n",
    "    print(\"Model : \", model_names[i])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Running on device: {}\".format(device))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_paths[i])\n",
    "    model = AutoModel.from_pretrained(model_paths[i])\n",
    "    print(model_paths[i])\n",
    "\n",
    "    ## freeze layers\n",
    "    # last_frozen_layer = 6  # layers start at 0, so layer 11 in the model output corresponds to 12, and layer 11 of the model won't be frozen in this case\n",
    "    modules = [\n",
    "        model.embeddings,\n",
    "        *model.encoder.layer[:first_unfrozen_layer],\n",
    "    ]\n",
    "    for module in modules:\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # check that you actually froze the layer\n",
    "    print(\"Before training -------------------\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.requires_grad}\")\n",
    "\n",
    "    # data\n",
    "    training_dataset = MultOverlappingSentencesPairDataset(\n",
    "        iclr2024.abstract, tokenizer, device, n_cons_sntcs=2, seed=42\n",
    "    )\n",
    "\n",
    "    gen = torch.Generator()\n",
    "    gen.manual_seed(seed)\n",
    "    training_loader = torch.utils.data.DataLoader(\n",
    "        training_dataset, batch_size=64, shuffle=True, generator=gen\n",
    "    )\n",
    "\n",
    "    # training\n",
    "    losses, knn_accuracies = train_loop(\n",
    "        model,\n",
    "        training_loader,\n",
    "        device,\n",
    "        iclr2024.abstract.to_list(),\n",
    "        tokenizer,\n",
    "        (labels_iclr != \"unlabeled\"),\n",
    "        labels_acc=labels_iclr[labels_iclr != \"unlabeled\"],\n",
    "        optimized_rep=\"av\",\n",
    "        n_epochs=1,\n",
    "        lr=2e-5,\n",
    "    )\n",
    "\n",
    "    ## save losses and accuracies\n",
    "    saving_path = (\n",
    "        Path(\"embeddings_\" + model_name.lower())\n",
    "        / Path(\"updated_dataset\")\n",
    "        / Path(\"freezing_experiment\")\n",
    "    )\n",
    "    (variables_path / saving_path).mkdir(exist_ok=True)\n",
    "\n",
    "    saving_name_losses = Path(\n",
    "        \"losses_first_unfrozen_layer_\" + str(first_unfrozen_layer) + \"_v1\"\n",
    "    )\n",
    "    saving_name_accuracies = Path(\n",
    "        \"knn_accuracies_first_unfrozen_layer_\"\n",
    "        + str(first_unfrozen_layer)\n",
    "        + \"_v1\"\n",
    "    )\n",
    "\n",
    "    np.save(variables_path / saving_path / saving_name_losses, losses)\n",
    "    np.save(\n",
    "        variables_path / saving_path / saving_name_accuracies,\n",
    "        knn_accuracies,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/EAAALUCAYAAABD17KkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAC4jAAAuIwF4pT92AAB6aUlEQVR4nOzdd3hUVf7H8c+kJ7QAAelJgIQuEEGlN1FBKYLoqkBoIkVFXRULmoCgi+JiBQQRECyLKEUUBVdQDFWC9BZIApEegkAqydzfH/xmljEdQmZu8n49zzy73nPuvd+ZJMN85px7rsUwDEMAAAAAAMDluTm7AAAAAAAAUDCEeAAAAAAATIIQDwAAAACASRDiAQAAAAAwCUI8AAAAAAAmQYgHAAAAAMAkCPEAAAAAAJgEIR4AAAAAAJMgxAMAAAAAYBKEeAAAAAAATIIQDwAAAACASRDiAQAAAAAwCUI8AAAAAAAmQYgHAAAAAMAkCPEAAAAAAJgEIR4AAAAAAJMgxAMAAAAAYBKEeAAAAAAATIIQDwAAAACASRDiAQAAAAAwCUI8AAAAAAAmQYgHAAAAAMAkCPEAAAAAAJgEIR4AAAAAAJMgxAMAAAAAYBKEeAAAAAAATIIQDwAAAACASRDi4SAzM1NxcXHKzMx0dikAAAAAgL8hxMNBQkKCgoODlZCQ4OxSAAAAAAB/Q4gHAAAAAMAkCPEAAAAAAJgEIR4AAAAAAJMgxAMAAAAAYBKEeAAAAAAATIIQD9N77bXXZLFY5OPjo8TExFz7ff/997r55pvl4+OjypUr68cff5QkrVu3TmvXri2ucvOsJygoSBaLxWVu8ffNN99o586dxXY+V3v+AAAAgKvxcHYBwPUwDEPz589XmTJllJycrHnz5unZZ5/N1u/cuXMaMGCAMjIyNGTIEFWsWFFNmzbVrFmzNHr0aM2ZM0ddunQptrpzq+epp57S+fPn5ebm/O/XXnjhBU2dOlVr1qwptnO60vMHAAAAXBEhHqa2bt06HTlyRC+99JLeeecdzZ49W//85z9lsVgc+u3Zs0cpKSm65557NGfOHPv2kydPFnfJedbz1FNPOaWenDjjtXGl5w8AAAC4Ioa7YGqffPKJJOm+++7TPffco0OHDum///1vtn7p6emSpICAgGKtLzeuVg8AAAAAcyDEw7QuXLigr7/+WjfddJNuueUWPfTQQ5KkmTNnOvQLCgpS9+7dJUkLFiyQxWLRkCFDFBQUpIkTJ0qSHn30UVksFsXFxdn327p1q+677z5VqVJF3t7eatCggV555RVdunTJ4fiRkZGyWCxauXKlunfvLm9vb9WsWVMbNmzIse7c6rG1XX1N+Lp162SxWDRt2jStWLFCbdu2VZkyZeTv76/evXtrx44d2Y5vtVo1Y8YMtWrVSmXKlFH58uXVqVMnLVu2rMCvrcVi0YIFCyRJ3bt3t89ssNUzcODAbPt8/PHHslgsioyMtG8bMmSILBaLjh07pgkTJqhu3bry9vZWUFCQxo8fr+Tk5GyvzfU+/3PnzumZZ55RUFCQfHx8dPPNN+vTTz/V5MmTZbFYtG7dugK/DgAAAICrIcTDtL744gulpqbqoYceksVi0T333KNKlSppxYoVOnHihL3fU089pUGDBkmSmjdvroiICPXt21dPPfWUOnXqJEnq1auXIiIi5O/vL0n6z3/+o7Zt22rNmjW6++679fTTT6tq1aqaPHmy2rdvrwsXLmSrZ8SIETp79qzGjRun5s2bKywsLMe6c6snL4sXL1bfvn1VsWJFPf7442rVqpW+/fZbdejQQceOHbP3s1qt6t+/v8aOHavk5GQNHz5cgwcPVkxMjO677z5NmjSpQK9tRESEmjdvLkkaNGiQIiIiCrRfbvr3768ZM2aoW7duGjt2rKxWq958880cvwzISUGf//nz59WxY0dNnz5dNWrU0BNPPKFatWopPDxc8+fPv67nAAAAALgEA7hKbGysIcmIjY11din5uvXWWw1JRnR0tH3bmDFjDEnGpEmTHPquWbPGkGSEh4c7bI+IiDAkGXPmzLFvO3nypOHn52fcdNNNxpEjRxz6R0ZGGpKMMWPGZDtGcHCwkZKSUqDac6snMDDQkGRcvnzZMAzDWLt2rSHJkGQsWbLEoe/gwYMNScbkyZPt29577z1DkvHII48YGRkZ9u3nz583mjdvblgsFmPz5s0FqjE8PNyQZKxZs8a+zVbPI488kq3/nDlzDElGREREtmPUrVvXOHXqlH37mTNnjEqVKhmSjISEhCJ7/k8//bQhyXjqqacc+k6bNs1+nLVr1xbo+QMAAACuiJF4mNLevXu1ZcsWNWnSRC1btrRvDw8PlyTNmTNHWVlZ13TsTz/9VCkpKXrppZcUHBzs0Pbyyy+rSpUqWrBggS5fvuzQ1rt3b/n6+l7TOfPTpEkT9e/f32GbbfQ+NjbWvm3WrFlyc3PTBx98IE9PT/v2ChUqaNKkSTIMQ3Pnzr0hNeZl9OjRqlq1qv2/AwIC1L59e0mO9eemIM8/KytLn376qSpWrKjXXnvNoe9TTz2lkJCQ63kKAAAAgEtgdXqYkm1Bu8GDBztsv/XWW9WwYUPt379f3333nXr37l3oY2/dutX+v1df323j5+enM2fO6ODBg2rSpIl9e926dQt9roJq2LBhtm22qf+2RfKSk5O1d+9elS1bVu+88062/ufPn5ckRUdH36gyc1WQ+q93/8OHDysxMVEdO3ZU2bJlHfq6u7urTZs2OnToUCErBwAAAFwLIR6mk5mZqUWLFkmSxo8fr/Hjx+fYb9asWdcU4pOSkiTJfo7cnDt3zuG//fz8Cn2ugvLx8cm2zbbYnGEYkv4X0i9dumRfsC8ntrqXLVumP/74I1t7Tl9cXK+C1H+9+589e1aSVL169RyPUbNmzYIVCwAAALgwQjxMZ+XKlTp16pRCQkLUtWvXHPvMnTtXP/74o+Li4hQUFFSo45crV06StGnTJt12223XW26xsdXdsGFD7du3L9/+y5Yts69Af7W8QrwtOFut1mxtf19pvrjZnv9ff/2VY/vFixeLsxwAAADghiDEw3RsU+lfeukl+63Z/u7kyZNavny5Zs+erddffz3XY9lC6dVatmyppUuXavPmzTmG+JdeeknlypXTk08+qTJlylzbk7gBypcvr7p16+rw4cM6e/ZstnvQ79mzR/PmzdPtt9+u+++/X/Pnz89zxfacXhsvLy9JOQdiZ09Vb9iwocqWLavff/9dmZmZ8vBwfHvbtGmTkyoDAAAAig4L28FUTp48qVWrVqlMmTK6//77c+336KOPSroyIv/3BeiuZlv8LSMjw75t0KBB8vT01KRJk3TgwAGH/h988IHeeOMNff311y4V4G2GDx+uy5cva/To0Q7XmqempmrUqFF6++23dfLkyQIdK6fXpn79+vLw8FBUVJTD5QQxMTH67LPPiuhZXBtPT0+Fh4fr7Nmz2S4nmDdvnn7//XcnVQYAAAAUHUbiYSqffvqpMjMz9cgjj2RbvOxqPXr0UO3atXXs2DF98803qly5co796tSpI0maPn26jh49qieeeEJBQUGaMWOGHnvsMbVo0UJ9+/ZVnTp1tH37dq1Zs0YVK1Z0ygrvBfHcc8/p559/1pIlS7R9+3bdeeed8vT01IoVKxQXF6cePXpo5MiRBTqW7bV55ZVXtH79er366quqUqWKHnjgAX3++edq1aqV+vXrp6SkJH311Ve65ZZbtG7duhv47PI3adIk/fjjj5o8ebLWrl2r2267TXv37tUPP/yggIAAnT17Vu7u7k6tEQAAALgejMTDVObNmydJuU6jt3Fzc9Pw4cMlXVngLjcDBgzQI488opMnT+rDDz/U7t27JUkjRozQL7/8ojvvvFNr1qzRu+++qyNHjujRRx/Vtm3b1Lx586J5QkXM09NTq1at0vTp01WhQgX7lPlKlSrpvffe09KlS+1T4vMzZswY9ejRQ/v27dPMmTPtt3KbM2eOnnvuOWVmZur999/Xhg0bNHnyZL399ts38qkVSKVKlRQVFaVHH31Uhw8f1gcffKDjx4/rq6++UseOHSXJJWdQAAAAAAVlMQqyNDRKjbi4OAUHBys2NrbQC8IBzhYTE6OaNWvK19c3W1u7du20YcMGnT59WlWqVHFCdQAAAMD1YyQeQIkxYMAAVaxYUcePH3fYvn79em3cuFE333wzAR4AAACmxjXxAEqMJ554QsOHD7evZVC5cmXFxMRoxYoV8vX11YwZM5xdIgAAAHBdCPEASoxhw4apRo0aevfdd7Vy5UolJiaqatWqevDBB/XCCy+ocePGzi4RAAAAuC5cEw8HrnxNfHJycq5t7u7u8vHxKVBfNzc3h2umC9M3JSVFuf3JWCwW+fn5XVPf1NRUWa3WXOu4ejG2wvRNS0tTVlZWkfT18/Oz3zs+PT1dmZmZRdLX19dXbm5XruzJyMjI85aAhenr4+NjX4m+MH0vX77scFu9v/P29rbfg74wfTMzMx1u+/d3Xl5e9tv6FaZvVlaW0tLScu3r6elpX8ywMH2tVqtSU1OLpK+Hh4e8vb0lSYZhKCUlpUj6FubvnveInPvyHsF7BO8Rhe/Le8S19eU94greIwrf9+q/e/w/A7hKbGysIcmIjY11dil2WVlZxq+//mpIyvXRs2dPh338/Pxy7dupUyeHvgEBAbn2bdWqlUPfwMDAXPs2btzYoW/jxo1z7RsYGOjQt1WrVrn2DQgIcOjbqVOnXPv6+fk59O3Zs2eer9vV7r///jz7Xrp0yd43PDw8z76nT5+29x0zZkyefa/+XXv22Wfz7Lt7925734iIiDz7btmyxd73zTffzLPv2rVr7X0/+OCDPPuuXLnS3nfevHl59l28eLG97+LFi/PsO2/ePHvflStX5tn3gw8+sPddu3Ztnn3ffPNNe98tW7bk2TciIsLed/fu3Xn2ffbZZ+19be8buT3GjBlj73v69Ok8+4aHh9v7Xrp0Kc++999/v8PvcF59eY+48uA94n8P3iOuPHiPuPLgPeLKg/eI/z14j7jycLX3CFzBwnZwaceOHVPr1q3ttwcDAAAAgNKM6fRw4ErT6a1Wq1q3bq3o6Ohsbc2bN9f69evtU6KYBpdzX6bBMQ3O1abBGUyVvaa+vEfwHsF7ROH78h7xP7xHFL4v7xFXuNp7BK4gxMOBK4X49evX5zkCv379erVv374YKwIAAAAA52I6PVxWXFxcnu2xsbHFUwgAAAAAuAhCPFxWfjMBgoODi6cQAAAAAHARhHi4rHbt2iksLCzHtrCwMLVt27aYKwIAAAAA5yLEw2W5ublp2bJl2YJ8WFiYli1bZl+YBAAAAABKCw9nFwDkpXbt2tq6das2bNig2NhYBQcHq23btgR4AAAAAKUSIR4uz83NTe3bt2clegAAAAClHiEecDFWq1VRUVH22/0x8wAAAACADSEecCHHjh1T3759FR0dbd9mWwOgdu3aTqwMAAAAgCtgeA9wEVarNVuAl6To6Gj17dtXVqvVSZUBAAAAcBWEeMBFREVFZQvwNtHR0dqwYUMxV3TjWa1WrV+/XgsXLtRvv/3GFxUAAABAPphOD7iIuLi4PNtjY2NL1OJ+XDoAAAAAFB4hHnARQUFBebYHBwcXTyHFIL9LB7Zu3VriFvNjwULAXPibBQC4KkI84CLatWunsLCwHKfUh4WFqW3btk6o6sYoyKUDzDoA4Cz8zQIAXBlfKQMuws3NTcuWLVNYWJjDdtsHx5I0AlSQSwdKChYsBMyFv1kAgKsrOakAKAFq166trVu3av369fr000+1fv16bd26tcSN/JSmSwdK44KFgJnxNwsAcHWEeMDFuLm5qX379ho0aJDat29fokbgbWyXDuSkpF06UJpmHQAlQWn8m+VOIQBgLiUvHQBweaXp0oHSNOsAKAlK29/ssWPH1Lp1a3Xs2FGDBw9Whw4d1Lp1ax07dszZpQEAcmExDMNwdhFwHbZVeGNjY/P9IFPskpOdXQGKmNVq1aZNmxQfH6/AwEDdfvvtJSrAS1eeY4cOHfTHjh3Z2lo0b67169eXuOcMmFlp+pstTc8VgMmVKePsClwKIR4OXDrEWyzOrgAAAABAcSOyOuDrVQAAAAAATIL7xMM8Ll1ydgUA4CAhIUEPPvigw3TkFs2b6z//+Y9q1arlxMqA/EVFRenOu+7KtX3N6tUlaqFRACgpmE4PBy49nR4AXIjValXr1q1zvB1ZWFiYtm7dyvXEcGn8DgOAOfHODADANeB+4jC70nSnEAAoSZhODwAoUlarVVFRUfaZPW3bti2RYaAg9xNv37598RQDXKPatWtr69at2rBhg2JjY0v03ywAlBSEeABAkTl27Jj69u3rMEJtG9WrXbu2EysreqXtfuIoudzc3NS+fXu+dAIAk+BrVgBAkbBardkCvHRlannfvn1ltVqdVNmN0a5du2zTkG3CwsJYEAwAANwQhHgAQJEobdeIcz0xAABwBqbTAwCKRGm8RpzriQEAQHEjxAMAikRpvUac64kBAEBxYqgAAFAkuEYcAADgxiPEAwCKBNeIl3xWq1Xr16/XwoUL9dtvv5W4xQoBADADptMDAIoM14iXXKXp9oEAALgyi2EYhrOLgOuIi4tTcHCwYmNj872+FQBQOlitVrVu3TrHuw+EhYVp69atfFEDAEAx4V9cAACQp9J2+0AAAFwZIR4AAOSpILcPBAAAxYMQDwAA8lRabx8IAIArIsQDAIA8cftAAABcByEeAADkidsHAgDgOrjFHAAAyBe3DwQAwDUQ4gEAQIG4ubmpffv2at++vbNLAQCg1OLrcwAAAAAATIIQDwAAAACASRDiAQAAAAAwCUI8AAAAAAAmwcJ2AAAAKDWsVquioqIUFxfHXRYAmBIhHgAAAKXCsWPH1LdvX0VHR9u3hYWFadmyZapdu7YTKwOAguNrRwAAAJR4Vqs1W4CXpOjoaPXt21dWq9VJld04VqtV69ev18KFC/Xbb7+VyOcIlEaMxAMAAKDEi4qKyhbgbaKjo7Vhwwa1b9++mKu6cZh1AJRcjMQDAACgxIuLi8uzPTY2tngKKQalcdYBUJoQ4gEAAFDiBQUF5dkeHBxcPIUUg4LMOgBgXoR4AAAAlHjt2rVTWFhYjm1hYWFq27ZtMVd045SmWQdAaUSIBwAAQInn5uamZcuWZQvytuvES9Jt5krTrAOgNLIYhmE4uwi4Dts9U2NjY/P9BwAAAMBsrFarNmzYoNjY2BJ7n3ir1arWrVvnOKU+LCxMW7duLXHPGShNCPFwQIgHAAAwP1anB0oubjEHAAAAlDC1a9fW1q1bS/ysA6A0IsQDAAAAJZCbm5vat2+v9u3bO7sUAEWIr+IAAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoR4AAAAAABMghAPAAAAAIBJEOIBAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoR4AAAAAABMghAPAAAAAIBJEOIBAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoR4AAAAAABMghAPAAAAAIBJEOIBAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoT4AkhJSVFkZKQaNmwob29vBQQE6K677tKqVauu6XhxcXGyWCx5Plq0aJFtv3Xr1uW7X9++fa/vyQIAAAAAXJaHswtwdcnJyerWrZs2b94sT09PNW3aVImJiVq9erVWr16tyMhIRUREFOqYO3bskCRVqlRJjRo1yrFPSEhIrvtVr15ddevWzXG/xo0bF6oWAAAAAIB5EOLzMXbsWG3evFktWrTQihUrVLt2bUnSwoULNWzYMEVGRqpdu3a64447CnxMWxh/4IEHNHPmzELvN27cOI0fP74QzwIAAAAAUBIwnT4Phw8f1qJFi+Tm5qbPPvvMHuAladCgQXrhhRckSZGRkYU6ri2MN2vWrFj2AwAAAACUDIT4PCxcuFBZWVlq06ZNjtPUR40aJUmKiorS0aNHC3xcWxhv2rRpgffJzMzUnj17Cr0fAAAAAKDkIMTnYePGjZKk9u3b59hes2ZNBQYGSpJ++eWXAh3z0qVLOnLkiKTCjagfOHBA6enpqlChgurUqVPg/QAAAAAAJQfXxOchJiZGklSvXr1c+wQFBSk+Pl4HDx4s0DF37twpwzBUo0YNnTlzRtOmTdP27duVmZmp0NBQPfTQQ2rXrl22/Wyj902aNFF0dLQWLVqk3bt3y93dXU2aNFF4eDjT7AEAAACghCPE5+H06dOSpCpVquTap3LlypKks2fPFuiYtjCelJSkxo0bKysry962Zs0affjhhxo2bJhmzZolT0/PbPvt3LlTt9xyi8Mxf/jhB02fPl0vvviiJk+enG8NTZo0ybUtIyOjQM8DAAAAAFD8mE6fh5SUFEmSj49Prn18fX0d+ubHFsbT0tL06KOPas+ePUpPT1d8fLwmT54sT09PffLJJxo3blyu+7388ss6cuSI0tPTdejQIT311FMyDENTpkzRm2++WejnCQAAAAAwB0bi8+Du7i6r1VqgvhaLpUD9OnToIKvVqpYtW2r06NH27XXq1NHLL7+soKAgDRw4ULNmzdLYsWPto+b33nuvatSooXvuuUf9+/e371e/fn1Nnz5dAQEBmjBhgiZOnKhhw4YpICAg1xpsC+TlJC4uTsHBwQV6LgAAAACA4sVIfB7Kli0r6crod25SU1MlSX5+fgU65iOPPKLZs2c7BPi/t4eEhMgwDC1fvty+/fHHH9cnn3ziEOCv9uyzz6ps2bJKSUnR6tWrC1QLAAAAAMBcCPF5sI1mJyYm5trHdi181apVi+y8LVu2lCTFxsYWeB9vb2/7bfAKsx8AAAAAwDwI8Xlo1KiRpCtTzHNjawsNDS3wcS9fvuywoN3f2abwX72wnZT3jIC89gMAAAAAlAyE+Dzcdtttkv53v/i/S0hI0NGjRyVJbdu2zfd4SUlJqlSpkry8vBymyv/d9u3bJck+sr5z505VqFBBvr6+io6OznGftLQ07d2712E/AAAAAEDJQojPw4ABAyRJ69at04EDB7K1z5o1S5LUqVMnBQUF5Xu8ihUrqlq1apKk+fPn59hnyZIlOnz4sLy8vNSvXz9JUoMGDeTm5pbnfh988IFSUlIUEBCgbt265VsLAAAAAMB8CPF5CAkJ0cMPP6ysrCz169dPMTEx9rZFixZp6tSpkqQJEyZk2/fw4cPav3+/Tpw44bD9xRdflCR9++23evHFF5Wenm5vW7JkiYYOHSpJev7551WjRg1JV653f+aZZyRdCevvvfeefeq81WrVjBkz7Mf917/+Zb/tHQAAAACgZLEYhmE4uwhXlpiYqC5dumjXrl1yd3dXs2bNlJSUpPj4eEnSlClT9NJLL2XbLygoSPHx8QoPD882ev7cc89p2rRpkqRy5copJCREp06d0p9//ilJGjFihD766CP76LskZWVlaeDAgfryyy8lSZUqVVLdunUVFxens2fPymKxKCIiQhEREdf1fG23mIuNjS3Q7AIAAAAAQPFhJD4flStX1saNGxUREaHQ0FDt27dPiYmJ6tSpk77++uscA3x+3nrrLa1Zs0Z9+vSRr6+vdu7cqcuXL+vee+/VypUrNWfOHIcAL125Z/0XX3yhxYsX684775Qk7dixQ56ennrggQf022+/XXeABwAAAAC4Nkbi4YCReAAAAABwXYzEAwAAAABgEoR4AAAAAABMghAPAAAAAIBJEOIBAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoR4AAAAAABMghAPAAAAAIBJEOIBAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoR4AAAAAABMghAPAAAAAIBJEOIBAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoR4AAAAAABMghAPAAAAAIBJEOIBAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoR4AAAAAABMghAPAAAAAIBJEOIBAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoR4AAAAAABMghAPAAAAAIBJEOIBAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoR4AAAAAABMghAPAAAAAIBJEOIBAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoR4AAAAAABMghAPAAAAAIBJEOIBAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoR4AAAAAABMghAPAAAAAIBJEOIBAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoR4AAAAAABMghAPAAAAAIBJEOIBAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoR4AAAAAABMghAPAAAAAIBJuEyIP3nypLNLAAAAAADApblMiK9Tp4569eqlb775RpcvX3Z2OQAAAAAAuByXCfFZWVn67rvvNGDAANWoUUNPPfWU/vjjD2eXBQAAAACAy3CZEB8fH6/JkycrJCREiYmJeu+993TLLbcoLCxM77//vhITE51dIgAAAAAATmUxDMNwdhF/t3nzZs2fP1+LFy9WUlKSLBaLPD091atXLw0ZMkQ9evSQm5vLfP9QosTFxSk4OFixsbEKCgpydjkAAAAAgKu4ZIi3ycjI0IoVK7Rw4UKtWbNGaWlpslgsuummmzR48GANGTJEDRs2dHaZJQohHgAAAABcl0uH+KulpKRo9uzZevXVV5WcnGzf3q5dOz3zzDPq27ev84orQQjxAAAAAOC6PJxdQH7i4uL0+eef65tvvtH27dtl+86hefPmOnnypH777TdFRUXprrvu0ldffaUyZco4uWIAAAAAAG4Ml7yw/MKFC5ozZ446duyoevXq6ZVXXlF0dLQqVqyoJ554Qtu3b9f27duVkJCgJUuWqGrVqvrxxx81btw4Z5cOAAAAAMAN4zLT6bOysvT9999r4cKFWrlypdLT02UYhtzc3HTHHXdo2LBh6tu3r7y8vLLtu3r1at19992qWLEiq9hfJ6bTAwAAAIDrcpnp9NWrV1diYqJ9unzdunU1ZMgQDRkyRLVq1cpzX9vidpmZmTe8TgAAAAAAnMVlQvzZs2fl6+ur/v37a9iwYercuXOB901LS9PIkSN1yy233LgCAQAAAABwMpeZTj979mw99NBDKleunLNLKdWYTg8AAAAArstlFrYbOXKkypUrp7/++ktz5szJ1v7vf/9bkyZN0qlTp5xQHQAAAAAAzucyIV6SfvzxR9WpU0ejRo3Sn3/+6dC2atUqTZw4UY0bN9bq1audVCEAAAAAAM7jMiE+OjpavXr10sWLFxUaGqqMjAyH9vvvv19hYWFKSkpSv379FBMT46RKAQAAAABwDpcJ8W+++aYyMzM1ZswY7d27V8HBwQ7tjz32mLZs2aKxY8cqJSVF//rXv5xUKQAAAAAAzuEyC9vVrFlTaWlpOnHiRI73grdJS0vTTTfdpAoVKujo0aPFWGHpwMJ2AAAAAOC6XGYk/uzZs6pXr16eAV6SfHx8FBISwgJ3AAAAAIBSx2VCfNWqVbMtZpebM2fOqHz58je4IgAAAAAAXIvLhPiwsDCdPHlSX331VZ79Vq5cqWPHjumWW24ppsoAAAAAAHANLhPiR4wYIcMwNGzYMM2ePVvp6ekO7RkZGZo/f74GDRoki8WiESNGOKlSAAAAAACcw2VCfK9evTRw4EAlJydr9OjRqlixopo3b6527drp5ptvlr+/v4YPH66//vpLDzzwgO6///5iqy0lJUWRkZFq2LChvL29FRAQoLvuukurVq26puPFxcXJYrHk+WjRokWO++7evVv/+Mc/dNNNN8nb21tBQUEaM2ZMgS9FAAAAAACYl8usTi9JVqtVb775pqZOnaq//vorW3vZsmX19NNP69VXX5W7u3ux1JScnKxu3bpp8+bN8vT0VNOmTZWYmGhfGT8yMlIRERGFOuby5cvVt29fVapUSY0aNcqxT0hIiObNm+ewbf369brzzjuVlpamgIAABQYG6sCBA7p06ZIqVqyon3/+OdfwX1CsTg8AAAAArsulQrzN5cuX9euvvyomJkaJiYkqU6aMQkND1aFDB5UtW7ZYaxkyZIgWLFigFi1aaMWKFapdu7YkaeHChRo2bJgyMzO1Zs0a3XHHHQU+5qRJkxQREaFRo0Zp5syZBdrn3Llzql+/vpKSkjR+/HhNnjxZHh4eunjxooYOHaqvv/5adevW1b59+/Jd4T8vhHgAAAAAcF0ezi4gJ56enurWrZu6devm1DoOHz6sRYsWyc3NTZ999pk9wEvSoEGDdPDgQU2ePFmRkZGFCvE7duyQJDVr1qzA+7z33ntKSkrS7bffrn/961/27eXKldPnn3+uRo0a6ciRI/r0009ZLwAAAAAASiiXuSa+sDZu3HjDz7Fw4UJlZWWpTZs2aty4cbb2UaNGSZKioqLs0+sLwhbimzZtWuB95s+fL0kaPnx4tjYvLy8NGzZMkvTFF18U+JgAAAAAAHNxqZH448eP691339WuXbuUkpIiq9Xq0J6ZmamUlBQdP35c586dU2Zm5g2tx/ZFQfv27XNsr1mzpgIDAxUfH69ffvlFgwYNyveYly5d0pEjRyQVfCT+xIkTio+Pz7OWdu3aSbryhcLly5fl6elZoGMDAAAAAMzDZUL8iRMndMstt+j06dOyXaZvsVh09SX7FotFkmQYhnx8fG54TTExMZKkevXq5donKChI8fHxOnjwYIGOuXPnThmGoRo1aujMmTOaNm2atm/frszMTIWGhuqhhx6yB/K/12GxWBQcHJxrHZKUnp6uo0eP5lkzAAAAAMCcXCbEv/322zp16pT8/Pz0j3/8Q2XKlNH777+vDh06qEOHDkpISNDKlSuVlJSk7t27a9myZTe8ptOnT0uSqlSpkmufypUrS5LOnj1boGPaptInJSWpcePGysrKsretWbNGH374oYYNG6ZZs2bZR9NtdZQvX17e3t551mGrJa8Q36RJk1zbMjIyCvQ8AAAAAADFz2Wuif/xxx9lsVi0dOlSffzxx3r33XdVsWJFeXh4aPLkyZo/f7727Nmjxo0b67///a89DN9IKSkpkpTnqL+vr69D3/zY6k5LS9Ojjz6qPXv2KD09XfHx8Zo8ebI8PT31ySefaNy4cddUR2FqAQAAAACYi8uMxB89elTVqlVT9+7d7dtatmypzZs3yzAMWSwW3XTTTZozZ47atm2r999/X7fffvsNrcnd3T3bdfm5sU31z0+HDh1ktVrVsmVLjR492r69Tp06evnllxUUFKSBAwdq1qxZGjt2rJo0aSJ3d/dC1Z1fLXv27Mm1zXaLOQAAAACA63GZkfi0tDTVqlXLYVvDhg2VkpKiw4cP27fdfvvtqlmzZrGsTm+7J31aWlqufVJTUyVJfn5+BTrmI488otmzZzsE+L+3h4SEyDAMLV++vNB1FKYWAAAAAIC5uEyIr1ixos6fP++wzTYivG/fPoft1atX18mTJ294TQEBAZKkxMTEXPvYroWvWrVqkZ23ZcuWkqTY2FiHOi5evKjLly/nWUdR1wIAAAAAcB0uE+JvvvlmHT582H77NUkKDQ2VYRjatm2bQ98///wz1wXeilKjRo0kXZlinhtbW2hoaIGPe/nyZYcF7f7ONoXftrCdrQ6r1Zrr/ehtdfj4+Kh27doFrgUAAAAAYB4uE+L79Okjq9Wqnj17atWqVZKk2267TW5ubvrwww/t4f69997TiRMnVLdu3Rte02233SZJuU7dT0hIsIfqtm3b5nu8pKQkVapUSV5eXvap8jnZvn27JKlx48aSrsxSCAkJybOWDRs22Gsu7DX0AAAAAABzcJkQP3z4cDVt2lQHDx5U7969lZGRoapVq6pXr15KTExUo0aNFBAQoKeffloWi0UPPfTQDa9pwIABkqR169bpwIED2dpnzZolSerUqZP9Pu15qVixoqpVqyZJmj9/fo59lixZosOHD8vLy0v9+vWzb3/ggQckSbNnz862T0ZGhj755BNJ0pAhQ/KtAwAAAABgTi4T4n18fPTzzz9r8ODBqlOnjry8vCRdCcoNGzbU5cuXde7cORmGobZt2+rJJ5+84TWFhITo4YcfVlZWlvr166eYmBh726JFizR16lRJ0oQJE7Lte/jwYe3fv18nTpxw2P7iiy9Kkr799lu9+OKLSk9Pt7ctWbJEQ4cOlSQ9//zzqlGjhr3tySeflL+/v9avX68nn3zSfj/3ixcv6pFHHtGRI0dUt25dPfLII0X07AEAAAAArsZiGIbh7CL+LjMzUx4e/7v7XXp6upYtW6bY2Fg1bNhQvXv3lptb8Xz/kJiYqC5dumjXrl1yd3dXs2bNlJSUpPj4eEnSlClT9NJLL2XbLygoSPHx8QoPD8826v7cc89p2rRpkqRy5copJCREp06d0p9//ilJGjFihD766KNsz3HlypXq37+/MjIyVKlSJdWtW1cHDhzQxYsX5e/vr99++01NmjS5rudru8VcbGxsgWYXAAAAAACKj8uE+IEDByo4OFjjx4+331LNVSQnJ+utt97S4sWLdeTIEXl6euqWW27Rk08+6TDl/Wp5hXhJ+umnn/TBBx9o48aNOnfunCpVqqRbb71Vo0aN0j333JNrLbt27dKUKVO0bt06JSYmqkqVKrrzzjv16quvFsk6AYR4AAAAAHBdLhPiK1euLHd3dx0/ftxhFB7FixAPAAAAAK7LZa6JT01NVZ06dQjwAAAAAADkwmVCfJcuXbR79+4cV4EHAAAAAACSywx7f/zxx7rzzjvVoUMHjR07Vu3atVP16tXl6+ub6z7Fca94AAAAAABchctcE1+jRg2lp6crKSlJFosl3/4Wi0WZmZnFUFnpwjXxAAAAAOC6XGYk/uTJk/b/X5DvFVzkuwcAAAAAAIqNy4T42NhYZ5cAAAAAAIBLc5kQHxgY6OwSAAAAAABwaS6zOj0AAAAAAMiby4zET5o0qdD7vPrqqzegEgAAAAAAXJPLrE7v5uZWoFXppSuL2lksFmVlZd3gqkofVqcHAAAAANflMiPxHTt2zDXEJycn6/jx4zp+/LgsFouGDx+uatWqFXOFAAAAAAA4l8uE+HXr1uXbZ8uWLXr44Ye1bt06RUdH3/iiAAAAAABwIaZa2O7WW2/Vl19+qcOHD2vy5MnOLgcAAAAAgGJlqhAvSa1atVJISIi++eYbZ5cCAAAAAECxMl2IlyRfX18lJCQ4uwwAAAAAAIqV6UL8gQMHtGfPHlWqVMnZpQAAAAAAUKxcZmG7n3/+Odc2wzCUnp6u/fv369///resVqu6detWjNUBAAAAAOB8LhPi77jjjgLdJ94wDJUrV04TJkwohqoAAAAAAHAdLjWd3jCMXB9ubm6qWrWq+vfvr19//VWhoaHOLhcAAAAAgGLlMiPxVqvV2SUAAAAAAODSXGokPi8XL150dgkAAAAAADiVy4X4jz/+WO3atdPly5cdto8cOVJ169bVRx995KTKAAAAAABwLpcJ8YZhaPDgwXrssce0adMmxcTEOLTHxsYqLi5OY8aM0YgRI5xUJQAAAAAAzuMyIX7u3LlatGiRfHx8NHnyZNWuXduhfdmyZXrnnXdUpkwZzZs3T998842TKgUAAAAAwDlcJsTPmzdPFotFS5cu1YsvvqiyZcs6tFerVk1PPvmkFi9eLMMwNHPmTCdVCgAAAACAc1gMwzCcXYQkVahQQVWrVtWhQ4fy7RsYGKgLFy4oKSmpGCorXeLi4hQcHKzY2FgFBQU5uxwAAAAAwFVcZiQ+MzNT/v7+Bep70003KS0t7cYWBAAAAACAi3GZ+8TXqVNHe/fu1cWLF1WuXLlc+6Wmpmr//v2qXr16MVYHAAAAwFVZrVZFRUXZZ5W2bdtWbm4uM14JFCmX+c3u0aOHUlNT9dRTT+XZ7/nnn1dycrK6d+9ePIUBAAAAcFnHjh1T69at1bFjRw0ePFgdOnRQ69atdezYMWeXBtwQLnNN/JEjR9SiRQslJyerTZs2GjFihJo3b66yZcvq4sWL2r17tz755BOtX79e3t7e2rFjh0JCQpxddonDNfEAAAAwC6vVqtatWys6OjpbW1hYmLZu3cqIPEoclwnxkrR06VINGjRIKSkpslgs2doNw5C3t7cWLlyo+++/3wkVlnyEeAAAAJjF+vXr1bFjxzzb27dvX4wVATeeS30tdd9992nnzp167LHHVKNGDRmGYX9UqVJF4eHhio6OJsADAAAAUFxcXJ7tsbGxxVMIUIxcZmE7m7p162rmzJmaOXOm0tPTlZiYqDJlyqhChQrOLg0AAACAC8lv5mhwcHDxFAIUI5caiZeuTJlfs2aNJMnb21s1atRQhQoVtHDhQn366adKT093coUAAAAAXEG7du0UFhaWY1tYWJjatm1bzBUBN55Lhfg//vhDwcHB6tGjh06fPu3Q9vnnn2vo0KFq3Lix/vjjD+cUCAAAAMBluLm5admyZdmCfFhYmJYtW8aidiiRXGY6fUxMjDp27KhLly7J399f58+fV9WqVe3tt956q3bv3q3Y2Fjdeeed2rlzp6pVq+bEigEAAAA4W+3atbV161Zt2LBBsbGx3CceJZ7L/Ga/8cYbunTpku677z4lJCQoNDTUoX3ixIk6dOiQ+vfvr7Nnz2rq1KlOqhQAAACAK3Fzc1P79u01aNAgtW/fngCPEs1lbjEXFBSkpKQkJSQkqFy5crn2O3/+vGrWrKnq1asrJiamGCssHbjFHAAAAAC4Lpf5iurEiRMKDQ3NM8BLkr+/vxo0aKCEhIRiqgwAAAAAANfgMiG+UqVKOnv2bIH6Xrp0SX5+fje4IgAAAAAAXIvLhPimTZvq6NGjWrt2bZ79Nm3apJiYGDVr1qyYKgMAAAAAwDW4TIgPDw+XYRj6xz/+odWrV+fYZ926dbr//vtlsVg0ePDgYq4QAAAAAADncpmF7STp7rvv1urVq2WxWFSnTh3dfPPNKlu2rC5evKjdu3crPj5ehmGoc+fO+umnn1h18gZgYTsAAAAAcF0uc594Sfr666/1z3/+U3PmzFF8fLzi4+Md2t3c3DRw4EB9+OGHBHgAAAAAQKnjUiPxNidPntR3332nmJgYJSYmqkyZMgoNDdXdd9+t4OBgZ5dXojESDwAAAACuy6VG4m2qVaum4cOHO7sMAAAAAABcimnnpHOfeAAAAABAaeNSI/EpKSlasGCBdu3apZSUFFmtVof2zMxMpaSkKCEhQTt37lRGRoaTKgUAAAAAoPi5TIg/f/682rZtqwMHDmRrMwxDFovF4b8BAAAAAChtXGY6/bvvvqv9+/fLYrGoS5cu6tOnjwzDUPPmzfXwww+rQ4cO8vC48p1Dly5ddOTIESdXDAAAAABA8XKZkfhvv/1WFotFCxYs0COPPKKsrCxVrFhRNWrU0KJFiyRJe/fu1d13362oqCilpKQ4uWIAAAAAAIqXy4zEHz58WJUrV9YjjzwiSXJ3d1eLFi0UFRVl79O4cWPNnj1bGRkZeuedd5xUKQAAAAAAzuEyIT45OVmBgYEO2xo1aqQLFy4oPj7evu3uu+9W1apV9csvvxR3iQAAAAAAOJXLhPgKFSpkmyJft25dSdL+/fsdttepU4dbzAEAAAAASh2XCfGNGzdWTEyMTp8+bd9Wv359GYah7du3O/Q9c+aM3NxcpnQAAAAAAIqFyyThHj166PLly+rXr5/27dsnSWrdurUk6aOPPlJSUpIk6ZtvvlF8fLyCg4OdVisAAAAAAM7gMiF+9OjRqlWrljZs2KBmzZopPT1dderUUadOnXT06FGFhoaqVatWevDBB2WxWNSrVy9nlwwAAAAAQLFymRBfoUIF/fzzz+rYsaMqVaokb29vSVdG4QMCApSYmKjo6GhlZWWpfv36Gj9+vJMrBgAAAACgeFkMwzCcXcTfnT59WlWrVrX/95kzZ/TJJ58oNjZWDRs21PDhw1WuXDknVlhyxcXFKTg4WLGxsQoKCnJ2OQAAAACAq7hkiIfzEOIBAAAAwHW5zHR6AAAAAACQN0I8AAAAAAAmQYgHAAAAAMAkCPEAAAAAAJgEIR4AAAAAAJMgxAMAAAAAYBKEeAAAAAAATIIQDwAAAACASXg446STJk0qkuO8+uqrRXIcAAAAAADMwGIYhlHcJ3Vzc5PFYrnu42RlZRVBNbhaXFycgoODFRsbq6CgIGeXAwAAAAC4ilNG4jt27FjoEH/58mVt3LhRkmQYhtzcuBIAAAAAAFC6OCXEr1u3rlD9d+3apfDwcElXAnxQUJDmzp17AyoDAAAAAMB1ufRwttVq1ZQpU9S6dWvt2LFDkjRq1Cjt2rVLXbp0cXJ1AAAAAAAUL6eMxBfEnj17NGTIEEVHR8swDAUGBmru3Lnq2rWrs0sDAAAAAMApXG4k3mq16o033lCrVq3sAf6xxx7Trl27CPAAAAAAgFLNpUbi9+3bp/DwcG3bto3RdwAAAAAA/sYlRuINw9DUqVMVFhZmD/CMvgMAAAAA4MjpI/H79+/XkCFDtHXrVhmGoTp16mju3Lnq1q2bs0sDAAAAAMClOG0k3jAMvfXWWwoLC9OWLVtkGIZGjhyp3bt3E+ABAAAAAMiBU0biDxw4oKFDh2rz5s32a98//vhjwjsAAAAAAHmwGIZhFPdJfX19lZGRIcMw5Obmpl69eqlcuXKFOobFYtGCBQtuUIWlV1xcnIKDgxUbG6ugoCBnlwMAAAAAuIpTQrybm+MsfovFooKWYetrsViUlZV1I8or1QjxAAAAAOC6nDKdPjw83BmnBQAAAADA1JwS4ufNm+eM0wIAAAAAYGoucZ94AAAAAACQP0I8AAAAAAAm4ZTp9JMmTSqS47z66qtFchwAAAAAAMzAaavTWyyW6z4Oq9MXPVanBwAAAADX5ZSR+I4dOxY6xF++fFkbN26UJPv95QEAAAAAKE2cEuLXrVtXqP67du2y35bOMAwFBQVp7ty5N6AyAAAAAABcl0sPZ1utVk2ZMkWtW7fWjh07JEmjRo3Srl271KVLFydXBwAAAABA8XLKSHxB7NmzR0OGDFF0dLQMw1BgYKDmzp2rrl27Ors0AAAAAACcwuVG4q1Wq9544w21atXKHuAfe+wx7dq1iwAPAAAAACjVXGokft++fQoPD9e2bdsYfQcAAAAA4G9cYiTeMAxNnTpVYWFh9gDP6DsAAAAAAI6cPhK/f/9+DRkyRFu3bpVhGKpTp47mzp2rbt26Obs0AAAAAABcitNG4g3D0FtvvaWwsDBt2bJFhmFo5MiR2r17NwEeAAAAAIAcOGUk/sCBAxo6dKg2b95sv/b9448/JrwDAAAAAJAHi2EYRnGf1NfXVxkZGTIMQ25uburVq5fKlStXqGNYLBYtWLDgBlVYesXFxSk4OFixsbEKCgpydjkAAAAAgKs4JcS7uTnO4rdYLCpoGba+FotFWVlZN6K8Uo0QDwAAAACuyynT6cPDw51xWgAAAAAATM0pIX7evHnOOC0AAAAAAKbmEveJvxbx8fHOLgEAAAAAgGLlMiF+1qxZBe47Y8YMNWvW7AZWAwAAAACA63HKdPqcPP744/Lx8dGQIUNy7RMfH6/hw4dr7dq1xVcYAAAAAAAuwmVG4q1Wqx599FF99tlnObbPmjVLzZo109q1a2UYhkaMGFFstaWkpCgyMlINGzaUt7e3AgICdNddd2nVqlVFdo5jx46pQoUKslgsiouLy7FPXFycLBZLno8WLVoUWU0AAAAAANfiMiPxH3zwgZ544gkNHTpUnp6eeuCBByQ5jr4bhqF69eppzpw56ty5c7HUlZycrG7dumnz5s3y9PRU06ZNlZiYqNWrV2v16tWKjIxURETEdZ3DMAwNGzZMFy5cyLPfjh07JEmVKlVSo0aNcuwTEhJyXbUAAAAAAFyXy4T4MWPGyMPDQ6NHj9agQYPk6empU6dO6fnnn1dycrLc3Nz0zDPPaOLEifLx8Sm2usaOHavNmzerRYsWWrFihWrXri1JWrhwoYYNG6bIyEi1a9dOd9xxxzWfY8aMGfrpp5/y7WcL8Q888IBmzpx5zecDAAAAAJiTy4R4SRo5cqQ8PDw0cuRI3X///ZKujFI3b95cc+fOVVhYWLHWc/jwYS1atEhubm767LPP7AFekgYNGqSDBw9q8uTJioyMvOYQf/jwYY0fP15+fn5KSUnJs68txLOoHwAAAACUTi5zTbzNsGHD9Mknn8hisUiS+vXrp99//73YA7x0ZbQ9KytLbdq0UePGjbO1jxo1SpIUFRWlo0ePFvr4VqtV4eHhSk5O1uuvv55vf1uIb9q0aaHPBQAAAAAwP5cL8ZI0ePBgLVy4UG5ubvruu+/03//+1yl1bNy4UZLUvn37HNtr1qypwMBASdIvv/xS6OO//fbbioqK0sCBA9WnT588+166dElHjhyRxEg8AAAAAJRWTplOX6dOnQL18/DwUEZGhnr37q2qVas6tFksFsXHx9+I8uxiYmIkSfXq1cu1T1BQkOLj43Xw4MFCHXvv3r165ZVXVL16db333nv666+/8uy/c+dOGYahGjVq6MyZM5o2bZq2b9+uzMxMhYaG6qGHHlK7du0KVQMAAAAAwFycEuITEhIK1T8jIyPbPrbp9jfS6dOnJUlVqlTJtU/lypUlSWfPni3wcTMzMzV48GClp6dr9uzZqlixYr4h3jaVPikpSY0bN1ZWVpa9bc2aNfrwww81bNgwzZo1S56ennkeq0mTJrm2ZWRkFPh5AAAAAACKl1NC/Lx585xx2kKzLTSX12r4vr6+Dn0LYsqUKdq2bZvCw8N17733FmgfW4hPS0vTY489pieeeEL169fXyZMntXDhQk2cOFGffPKJvL29NWPGjALXAgAAAAAwD6eE+PDwcGecttDc3d1ltVoL1LegMwOio6M1ZcoU1axZU++8806Ba+nQoYOsVqtatmyp0aNH27fXqVNHL7/8soKCgjRw4EDNmjVLY8eOzXO0fc+ePbm2xcXFKTg4uMB1AQAAAACKj0subOcqypYtK+nK6HduUlNTJUl+fn75Hi89PV3h4eG6fPmy5syZI39//wLX8sgjj2j27NkOAf7v7SEhITIMQ8uXLy/wcQEAAAAA5kGIz0NAQIAkKTExMdc+tmvh/77wXk5effVV7d69W8OGDVOPHj2KpsirtGzZUpIUGxtb5McGAAAAADgfIT4PjRo1knRlinlubG2hoaH5Hu8///mPJOmTTz6RxWJxeFw9hT04OFgWi0WRkZEO+1++fNlhQbu/s039z29hOwAAAACAOTnlmnizuO2227RixQr7/eL/LiEhQUePHpUktW3bNt/jtW7dWrVq1cqxLT09Xb///rskqVWrVvL29rbfii8pKUn16tVTUlKSvv76a/Xr1y/HY2zfvl2S1Lhx43xrAQAAAACYDyE+DwMGDNDLL7+sdevW6cCBA2rQoIFD+6xZsyRJnTp1UlBQUL7H++qrr3Jtu3pBua+++srheBUrVlS1atWUlJSk+fPn5xjilyxZosOHD8vLyyvXkA8AAAAAMDem0+chJCREDz/8sLKystSvXz/FxMTY2xYtWqSpU6dKkiZMmJBt38OHD2v//v06ceJEkdTy4osvSpK+/fZbvfjii0pPT7e3LVmyREOHDpUkPf/886pRo0aRnBMAAAAA4FoshmEYzi7ClSUmJqpLly7atWuX3N3d1axZMyUlJSk+Pl7SlXu+v/TSS9n2CwoKUnx8vMLDwzV//vx8z3P1SHxsbGyOI/vPPfecpk2bJkkqV66cQkJCdOrUKf3555+SpBEjRuijjz6Sm9u1fzdjqyO3GgAAAAAAzsNIfD4qV66sjRs3KiIiQqGhodq3b58SExPVqVMnff311zkG+Bvlrbfe0po1a9SnTx/5+vpq586dunz5su69916tXLlSc+bMua4ADwAAAABwbYzEwwEj8QAAAADguhi2BQAAAADAJAjxAAAAAACYBCEeAAAAAACTIMQDAAAAAGAShHgAAAAAAEyCEA8AAAAAgEkQ4gEAAAAAMAlCPAAAAAAAJkGIBwAAAADAJAjxAAAAAACYBCEeAAAAAACTIMQDAAAAAGAShHgAAAAAAEyCEA8AAAAAgEkQ4gEAAAAAMAlCPAAAAAAAJkGIBwAAAADAJAjxAAAAAACYBCEeAAAAAACTIMQDAAAAAGAShHgAAAAAAEyCEA8AAAAAgEkQ4gEAAAAAMAlCPAAAAAAAJkGIBwAAAADAJAjxAAAAAACYBCEeAAAAAACTIMQDAAAAAGAShHgAAAAAAEyCEA8AAAAAgEkQ4gEAAAAAMAlCPAAAAAAAJkGIBwAAAADAJAjxAAAAAACYBCEeAAAAAACTIMQDAAAAAGAShHgAAAAAAEyCEA8AAAAAgEkQ4gEAAAAAMAlCPAAAAAAAJkGIBwAAAADAJAjxAAAAAACYBCEeAAAAAACTIMQDAAAAAGAShHgAAAAAAEyCEA8AAAAAgEkQ4gEAAAAAMAlCPAAAAAAAJkGIBwAAAADAJAjxAAAAAACYBCEeAAAAAACTIMQDAAAAAGAShHgAAAAAAEyCEA8AAAAAgEkQ4gEAAAAAMAlCPAAAAAAAJkGIBwAAAADAJAjxAAAAAACYBCEeAAAAAACTIMQDAAAAAGAShHgAAAAAAEyCEA8AAAAAgEkQ4gEAAAAAMAlCPAAAAAAAJkGIBwAAAADAJAjxAAAAAACYBCEeAAAAAACTIMQDAAAAAGAShHgAAAAAAEyCEA8AAAAAgEkQ4gEAAAAAMAlCPAAAAAAAJkGIBwAAAADAJAjxAAAAAACYBCEeAAAAAACTIMQDAAAAAGAShHgAAAAAAEyCEA8AAAAAgEkQ4gEAAAAAMAlCPAAAAAAAJkGIBwAAAADAJAjxAAAAAACYBCEeAAAAAACTIMQDAAAAAGAShHgAAAAAAEyCEA8AAAAAgEkQ4gEAAAAAMAlCPAAAAAAAJkGIBwAAAADAJAjxAAAAAACYBCEeAAAAAACTIMQDAAAAAGAShHgAAAAAAEyCEA8AAAAAgEkQ4gEAAAAAMAlCPAAAAAAAJkGIBwAAAADAJAjxAAAAAACYBCEeAAAAAACTIMQDAAAAAGAShHgAAAAAAEyCEA8AAAAAgEkQ4gEAAAAAMAlCPAAAAAAAJkGIBwAAAADAJAjxBZCSkqLIyEg1bNhQ3t7eCggI0F133aVVq1YV2TmOHTumChUqyGKxKC4uLtd+R48e1YgRI1SrVi15eXmpZs2aGjx4sPbt21dktQAAAAAAXBMhPh/Jycnq2rWrJk6cqCNHjqhJkyYqU6aMVq9erZ49e2rixInXfQ7DMDRs2DBduHAhz34HDhxQWFiY5s6dq0uXLql58+ZKS0vTwoULFRYWph9//PG6awEAAAAAuC5CfD7Gjh2rzZs3q0WLFjp8+LCio6MVHx+vTz/9VB4eHoqMjNRPP/10XeeYMWNGvsfIzMzUvffeq8TERA0aNEgnTpzQ1q1bdeLECT3++ONKS0vTP/7xDyUmJl5XLQAAAAAA10WIz8Phw4e1aNEiubm56bPPPlPt2rXtbYMGDdILL7wgSYqMjLyuc4wfP15+fn559lu0aJFiYmJUp04dffzxx/L19ZUkeXl56b333lOHDh10/vx5TZ8+/ZprAQAAAAC4NkJ8HhYuXKisrCy1adNGjRs3ztY+atQoSVJUVJSOHj1a6ONbrVaFh4crOTlZr7/+ep5958+fL+nKlwdeXl4ObRaLRY899pgk6Ysvvih0HQAAAAAAcyDE52Hjxo2SpPbt2+fYXrNmTQUGBkqSfvnll0If/+2331ZUVJQGDhyoPn365NrParVqy5YtedbSrl07SdKRI0d07NixQtcCAAAAAHB9hPg8xMTESJLq1auXa5+goCBJ0sGDBwt17L179+qVV15R9erV9d577+XZ988//1RqamqetdSuXVvu7u7XVAsAAAAAwBw8nF2AKzt9+rQkqUqVKrn2qVy5siTp7NmzBT5uZmamBg8erPT0dM2ePVsVK1bUX3/9lW8dedXi7u6uChUq6Ny5c/nW0qRJk1zbMjIy8qkeAAAAAOAsjMTnISUlRZLk4+OTax/bAnO2vgUxZcoUbdu2TeHh4br33nsLXMeNqAUAAAAAYB6MxOfB3d1dVqu1QH0tFkuB+kVHR2vKlCmqWbOm3nnnnQLXURj51bJnz55c2+Li4hQcHFyo8wEAAAAAigcj8XkoW7asJCktLS3XPrZr1fO7RZwkpaenKzw8XJcvX9acOXPk7+9fqDqKshYAAAAAgPkQ4vMQEBAgSUpMTMy1j+3686pVq+Z7vFdffVW7d+/WsGHD1KNHj0LXkVctmZmZ9uvqC1ILAAAAAMB8CPF5aNSokaQrU8xzY2sLDQ3N93j/+c9/JEmffPKJLBaLw+PqKezBwcGyWCyKjIyUJNWoUUMVKlTIs5Zjx44pKyurwLUAAAAAAMyHa+LzcNttt2nFihX2+8X/XUJCgo4ePSpJatu2bb7Ha926tWrVqpVjW3p6un7//XdJUqtWreTt7a06derY22+99VatWbNGGzduVLdu3bLtv2HDBklSYGCgatSokW8tAAAAAADzIcTnYcCAAXr55Ze1bt06HThwQA0aNHBonzVrliSpU6dO9vvF5+Wrr77Kte3qBeW++uqrbMd74IEHtGbNGs2bN0/PP/+8vLy8cqxlyJAh+dYBAAAAADAnptPnISQkRA8//LCysrLUr18/xcTE2NsWLVqkqVOnSpImTJiQbd/Dhw9r//79OnHiRJHUMnDgQNWrV09HjhzRww8/rIsXL0q6cl/3J598Ur/99psqVKigJ554okjOBwAAAABwPYzE5+O9997Trl27tGvXLjVs2FDNmjVTUlKS4uPjJV255/sdd9yRbb9u3bopPj5e4eHhmj9//nXX4ePjoy+++ELdu3fX119/rR9//FENGzbUkSNHdO7cOXl5eWnp0qWqXLnydZ8LAAAAAOCaGInPR+XKlbVx40ZFREQoNDRU+/btU2Jiojp16qSvv/5aL730UrHV0rp1a+3YsUPDhw+Xv7+/duzYITc3N/Xv31+bN29Wly5diq0WAAAAAEDxsxiGYTi7CLgO27X5sbGxBbrOHwAAAABQfBiJBwAAAADAJAjxAAAAAACYBCEeAAAAAACTIMQDAAAAAGAShHgAAAAAAEyCEA8AAAAAgEkQ4gEAAAAAMAlCPAAAAAAAJkGIBwAAAADAJAjxAAAAAACYBCEeAAAAAACTIMQDAAAAAGAShHgAAAAAAEyCEA8AAAAAgEkQ4gEAAAAAMAlCPAAAAAAAJkGIBwAAAADAJAjxAAAAAACYBCEeAAAAAACTIMQDAAAAAGAShHgAAAAAAEyCEA8AAAAAgEkQ4gEAAAAAMAlCPAAAAAAAJkGIBwAAAADAJAjxAAAAAACYBCEeAAAAAACTIMQDAAAAAGAShHgAAAAAAEyCEA8AAAAAgEkQ4gEAAAAAMAlCPAAAAAAAJkGIBwAAAADAJAjxAAAAAACYBCEephIZGSmLxZLt4evrq+DgYA0fPlxHjhy57vNkZmbqueeeU7Vq1eTt7a2GDRsWQfVFLzk5WcOGDVPlypXl6+ur7t27a/78+bJYLJowYYKzy5MknT59WjNmzCi287na8wcAAACKkoezCwCuRadOndS5c2dJkmEYSklJ0YEDB7RgwQJ9/fXX2rRp03UF748//ljTpk1TUFCQhgwZooCAgCKqvGi99tprmjdvnm6++Wb17NlT9evXV4sWLRQREaGOHTs6uzydPn1aISEhatasmcaMGVMs53Sl5w8AAAAUNUI8TKlz586KjIzMtv2LL77Qww8/rOeff14rVqy45uNv3bpVkvTWW2/p/vvvv+bj3Gi2OufOnatWrVrZt7do0cJJFTlKSUnRhQsXivWcLVq0cJnnDwAAABQ1ptOjRPnHP/6h8uXL6+eff76u46Snp0uSy47A25ilTgAAAABFgxCPEsViscjd3V0+Pj7Z2s6fP68XXnhBISEh8vb2VpUqVTRgwADt3LnT3mfdunWyWCz67LPPJEldunSRxWLRunXr7H2+//57de/eXf7+/vLx8VHjxo01adIkpaSkOJxvyJAhslgsioqK0q233ipvb28FBQUpNjZWkpSWlqY33nhDTZs2la+vrypWrKgePXro119/zfd52q77joqKkiQFBwfLYrEoLi4ux2vCO3furLJlyyopKUmPP/64atasKW9vbzVo0ECvv/66srKysp3j4MGDGjRokKpXry4vLy8FBQVp3LhxOnPmTL71SVfWLwgODpYkRUVFyWKx2GdPdO7cWRaLRTExMdn2q1WrliwWi/2/4+LiZLFY9PjjjysqKkp33HGHypcvr7Jly6pr164OP5urX5vrff7ff/+9OnbsqAoVKqhy5coaNGiQTpw4IQ8PD/ulHAAAAEBxYzo9SpQlS5YoKSlJY8eOddh+6tQpdejQQYcOHVKnTp3Ut29fnT59Wl999ZVWrlyp5cuX684771RQUJAiIiL0zTffaNeuXQoPD1dQUJCCgoIkSRMnTlRkZKTKly+vXr16qXLlyvrvf/+riIgILV26VL/88ovKly/vcO7+/furQYMGevLJJxUXF6fg4GClpKSoW7du2rRpk2655RaNHj1aly5d0pIlS9SlSxfNnTtXQ4YMyfV52q77/uSTT3Ts2DGNGzdO/v7+8vf3z3WfzMxMde7cWYmJierTp4/c3d31xRdf6OWXX9aZM2c0ffp0e99ff/1V99xzj9LS0tS3b1/Vq1dPO3fu1Hvvvadly5YpKipKtWrVyvNn0blzZ50/f17vvvuuateurWHDhl1X+I2KitLs2bPVpk0bPfbYY4qJidGyZcv022+/KSoqSq1bt85z/8I8/9mzZ2vUqFGqUKGC+vfvr3Llymnx4sVq166dDMO45ucAAAAAXDcDuEpsbKwhyYiNjXV2KTmKiIgwJBmdOnUyIiIi7I/x48cbffr0Mdzd3Y2uXbsaly5dctivX79+hiRjxowZDtt3795tlC1b1qhSpYqRnJxs3/7II48Ykoy1a9fat23atMmQZAQHBxuHDx+2b798+bIRHh5uSDIeffRR+3bbtrZt2xpZWVkO533mmWcMScb48eMNq9Vq356QkGDUqlXL8Pb2No4dO5bv69GuXbtsP6958+YZkoyXX37Zvq1Tp06GJOP22283Ll68aN9+4MABw8PDw/Dz8zMyMjIMwzCMtLQ0o0aNGoavr6/x+++/O5xv/vz5hiSjZ8+e+dZmGP/7fWrXrp3Ddls9hw4dyrZPzZo1javfmmzHkGRMmzbNoe+rr75qSDJGjBhRZM//+PHjRpkyZYzKlSsbMTEx9r5nzpwxgoOD7b9/AAAAgDMwnR6m9Msvv2jixIn2x9SpU7V8+XJlZWUpICBAp0+ftvc9deqUli5dqpYtW2r06NEOx2nSpIkeffRRnTlzRsuXL8/znHPnzpUkTZkyRXXr1rVv9/Dw0DvvvCN/f399+umnSktLc9hvwIABcnP7359aVlaWPv74Y1WpUkVTpkxxmDpes2ZNPffcc0pPT9fChQsL/8Lk45///KfKli1r/+/Q0FA1btxYKSkpOnXqlCRpxYoVOn78uEaNGqVbbrnFYf/w8HC1bNlS33//vY4fP17k9eWlQoUKGjdunMO2vn37SpL9EoX8FOT5/+c//1FycrKeeeYZ1atXz943ICAgx8UUAQAAgOLEdHqYUkREhEOgSktL0/Hjx/Xll1/qlVde0a+//qrt27erWrVq2rZtmwzD0OXLl3MMYbb7ykdHR+uhhx7K9ZzR0dGSrlwn/3f+/v66+eab9euvv2rv3r0KCwuzt10d+CXpwIEDunDhgmrVqqXXXnst27Hi4uIczleUcrrtnm0Kvm2RPNuK94cOHcrx9bJarZKk7du3q0aNGkVeY27q168vDw/Ht6y/156fgjz/LVu2SJLatGmTrW+HDh0KWi4AAABwQxDiUSL4+Piobt26eumll3T27FlNnz5d7733nl5//XUlJSVJknbv3q3du3fneoxz587leY6//vpL0pUR4ZzUrFlTkpScnOyw3c/Pz+G/bfUkJCRo4sSJ11zPtchpwT/bTADj/6/1ttW3cuVKrVy5Ms/6zp8/r3feeSdbW+fOnYt88beC1F4Uxzh79qwkqXr16tn62n7GAAAAgLMQ4lHi3HHHHZo+fbp27NghSSpXrpwkadSoUZo5c+Y1H9e2YN2ff/6p+vXrZ2u3hd/KlSvneRxbPXfffbdWrVp1zfXcKLb6vvzySz344IN59o2Li8v1i4i8QrwtONtG9a/29y9Bipvt+du+tLnaxYsXi7scAAAAwAHXxKPESUxMlPS/adItWrSQ9L9p0n+3dOlSTZgwwT6NPDe2KfK//PJLtra0tDRt2rRJZcqUsd9WLTcNGjSQr6+v/vjjjxyngf/666964YUX9NNPP+V5nBulZcuWkqTNmzfn2P7WW2/ptdde04kTJxQUFCTDMLI9bNPwr77e/2peXl6SsofiM2fO5Biei5NtlfuNGzdma9u0aVNxlwMAAAA4IMSjRElOTrZP777vvvskSXXq1FH37t0VHR2tt99+26H/kSNHNGrUKE2ZMiXHqdZXGzZsmKQr9z+/eiG1zMxMjRs3TufPn9eDDz4oX1/fPI/j7e2tgQMH6uTJkxo/frzDaHRiYqJGjhypqVOnZlsgr7j07dtXlSpV0owZM7RhwwaHtm+//VbPP/+8Zs6cme+MA0ny9PSUJGVkZDhsb9Sokf14V5s0aZLTb+E2cOBA+fj46O2337avTyBduXzglVdecV5hAAAAgJhOD5Nat26dw6JrhmHo1KlT+uabb3TmzBn16dNH/fv3t7fPmTNHHTp00LPPPqtvvvlGt99+u86fP68lS5bowoULmjRpkpo1a5bnOdu0aaNXXnlFr732mlq0aKHevXurcuXK+vnnn7Vr1y41b95c//73vwtU/5tvvqlNmzbp3Xff1dq1a9W5c2ddvnxZX3/9tU6fPq3hw4frnnvuuabX5nqVK1dOCxcuVP/+/dWxY0fde++9atCggQ4ePKhvv/1W3t7eWrBggX00PS9VqlSRj4+PoqOj9cQTT6h79+7q3bu3Ro4cqRkzZui1117T3r17FRwcrF9++UWHDh1Ss2bNtGvXrmJ4pjmrVauWpk6dqnHjxqlly5a677775O3trRUrVujy5cuSJHd3d6fVBwAAgNKNEA9T+uWXXxymtbu7u8vf31/NmjXTQw89pOHDhztM5Q4MDNS2bdv0r3/9S8uXL9cHH3wgf39/tWrVSk899ZR69epVoPNOmjRJrVq10rvvvmsPdfXr19frr7+up59+Ot/RfBt/f39t2LBBb7/9thYvXqyPPvpIZcqUUYMGDfTWW29p4MCBuU5FLw49e/bUli1b9MYbb2jt2rX6/vvvVb16dQ0YMEAvvviibr755gIdx9PTUx999JEmTJig2bNn69KlS+rdu7caN26sNWvWKCIiQt999528vLzUpUsXLVy4UM8995xTQ7wkPfnkk6pevbqmTZumL7/8Uj4+Purbt68GDRqkrl27qkyZMk6tDwAAAKWXxXD23FW4lLi4OAUHBys2NlZBQUHOLgcodomJicrIyMhxdfo1a9bozjvv1JgxY/Thhx86oToAAFDaWa1WRUVF2T+3t23bVm5uXCVdmvDTBoCr/Pbbb6pRo4aeeOIJh+2ZmZn2yyXuuOMOZ5QGAABKuWPHjql169bq2LGjBg8erA4dOqh169Y6duyYs0tDMWIkHg4YiUdpl5qaqhYtWujgwYPq2rWrWrVqpbS0NP344486cOCA+vTpo2XLljm7TAAAUMpYrVa1bt1a0dHR2drCwsK0detWRuRLCa6Jh2nkdf9wd3d3h+vR8+rr5ubmsIJ8YfqmpKTkunq6xWKRn5/fNfVNTU3N8Z7pNldfg12YvmlpacrKyiqSvn5+fvbr9NPT05WZmVkkfX19fe3/4GRkZNgXj7vevj4+PvYF6ArT18PDQ6tXr9aHH36oFStWaPPmzXJ3d1dISIimTZvmMEJ/+fLlbCvvX83b21seHlfeZjMzM3O8paCNl5eXfTX/wvTNysrK804Gnp6e9kUIC9PXarUqNTW1SPp6eHjI29tb0pVFKFNSUoqkb2H+7nmPyLkv7xGFf48ozN897xG8R/Ae8T+8R1xxPe8RP//8c44BXpKio6O1YcMGtW/fvkS/R+D/GcBVYmNjDUlGbGyss0vJRlKuj549ezr09fPzy7Vvp06dHPoGBATk2rdVq1YOfQMDA3Pt27hxY4e+jRs3zrVvYGCgQ99WrVrl2jcgIMChb6dOnXLt6+fn59C3Z8+eeb5uV7v//vvz7Hvp0iV73/Dw8Dz7nj592t53zJgxefa9+nft2WefzbPv7t277X0jIiLy7LtlyxZ73zfffDPPvmvXrrX3/eCDD/Lsu3LlSnvfefPm5dl38eLF9r6LFy/Os++8efPsfVeuXJln3w8++MDed+3atXn2ffPNN+19t2zZkmffiIgIe9/du3fn2ffZZ5+197W9b+T2GDNmjL3v6dOn8+wbHh5u73vp0qU8+95///0Ov8N59eU94sqD94j/PXiPuPLgPeLKg/eIKw/eI/73MNt7hCTj008/NQyjZL9H4ArmWwAAAACAyQUHBzu7BBQTromHA1e+Jp5pcIXvyzQ415oGx1RZMVWW94hC9+U94greIwrfl/eI/+E9ovB9XfE9IjU1VR06dNCOHTuy9W3ZsqV+//13ubm5lej3CFxBiIcDVw7xAAAAQGl27Ngx9e3b1+Ha+LCwMC1btky1a9d2YmUoTixsBwAAAAAmULt2bW3dulUbNmxQbGws94kvpQjxAAAAAGASbm5uat++vdq3b+/sUuAkfGUDAAAAAIBJEOIBAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoR4AAAAAABMghAPAAAAAIBJEOIBAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoR4AAAAAABMghAPAAAAAIBJEOIBAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmISHswuAa8nMzJQkJSQkOLkSAAAAAHB9tWrVkodH8UVrQjwcnDx5UpLUoUMHJ1cCAAAAAK4vNjZWQUFBxXY+i2EYRrGdDS4vLS1Nv//+u6pVq1as3yblp3v37pKkNWvWOLkSFDV+tiUTP9eSi59tycTPteTiZ1sy8XN1LYzEw6l8fHzUvn17Z5eRjZeXlyQV6zdcKB78bEsmfq4lFz/bkomfa8nFz7Zk4udaurGwHQAAAAAAJkGIBwAAAADAJAjxAAAAAACYBCEeAAAAAACTIMQDAAAAAGAS3GIOAAAAAACTYCQeAAAAAACTIMQDAAAAAGAShHgAAAAAAEyCEA8AAAAAgEkQ4gEAAAAAMAlCPAAAAAAAJkGIh8tKSUlRZGSkGjZsKG9vbwUEBOiuu+7SqlWrnF0aikBCQoKefvppNWrUSH5+fvLz81OTJk00fvx4nT592tnloYhkZmbq1ltvlcVi0fz5851dDq7Dzz//rH79+ql69ery8vJSjRo1NHDgQO3bt8/ZpeEaJSYm6vnnn1eDBg3k4+Oj8uXLq23btpo9e7asVquzy8M1+Oijj2SxWPTxxx/n2ofPV+ZTkJ8rn6tKGQNwQZcuXTJuu+02Q5Lh6elptGzZ0qhTp44hyZBkREZGOrtEXIdff/3V8Pf3NyQZ7u7uRoMGDYz69esb7u7uhiSjWrVqxo4dO5xdJorApEmT7H+38+bNc3Y5uEbjx4+3/xyrV69u3HzzzYa3t7chyfDx8TFWr17t7BJRSHFxcfZ/Vz08PIzGjRs7/Dt77733GhkZGc4uE4WwZcsWo1y5coYkY86cOTn24fOV+RTk58rnqtKHEA+XFB4ebkgyWrRoYRw9etS+/dNPPzU8PDwMScaaNWucWCGuVVJSklGlShVDknH33Xcbx48ft7cdPnzYaNeunSHJCA4ONlJTU51YKa7X9u3bDU9PT0K8yc2dO9f+gX/evHmG1Wo1DMMwEhMTjXvuuceQZFStWtW4dOmSkytFYXTt2tWQZDRp0sQ4ePCgffuKFSsMHx8fQ5IxefJkJ1aIwli7dq1RsWJF+/ttbmGPz1fmUpCfK5+rSidCPFxOTEyM4e7ubri5uRl79uzJ1j5hwgRDktGuXTsnVIfrNX36dEOSUaNGDePChQvZ2k+fPm3/B2vhwoVOqBBFIT093WjWrJnh5uZmH7ElxJtPamqqUalSJUOSMWPGjGzt58+ft48Qff75506oENfi6NGj9lAQFRWVrf3VV1+1f+iHa0tNTTUiIiLsI655hT0+X5lHYX6ufK4qnbgmHi5n4cKFysrKUps2bdS4ceNs7aNGjZIkRUVF6ejRo8VdHq7T2rVrJUn33nuvypUrl629SpUqatu2rSRp69atxVobik5ERIR27dqlJ598UtWqVXN2ObhG3377rc6dO6eQkBCNHDkyW3uFChX0/vvv69///rcaNmzohApxLRISEuz/v3nz5tnaW7duLUk6duxYsdWEwouJiVFoaKgmTpwoSZo8ebICAwNz7c/nK3Mo7M+Vz1WlEyEeLmfjxo2SpPbt2+fYXrNmTfub2S+//FJsdaFoTJgwQZ9++qmGDRuWax/DMCRJWVlZxVUWitCmTZv01ltvKTQ0VK+//rqzy8F1WLNmjSSpT58+cnd3z7FPeHi4nn76abVs2bI4S8N1qFOnjv3/b9++PVv7zp07JSnP4ADnS0hI0LFjx3T77bdr8+bNevnll/Psz+crcyjsz5XPVaWTh7MLAP4uJiZGklSvXr1c+wQFBSk+Pl4HDx4srrJQRFq3bm0f5cnJ2bNntW7dOklSkyZNiqkqFJXU1FSFh4fLMAzNmzdPvr6+zi4J18EW5po0aSLDMLR06VKtWLFCCQkJqlSpku666y4NHjxYnp6eTq4UhVGzZk316dNHy5cv1+jRo7Vs2TL7v7k///yz3njjDUnSM88848wykY9atWrpu+++U8+ePQvUn89X5lDYnyufq0onQjxcju02GFWqVMm1T+XKlSVdeWNCyTJu3DilpKTIz89P/fv3d3Y5KKQXXnhBBw8e1D//+U/79D2YV3x8vCTJ09NTnTp10vr16x3av/rqK7377rv67rvvVLt2bWeUiGu0aNEiDR8+XF999ZUaNmyo0NBQpaamKjY2Vv7+/nrnnXc0ZswYZ5eJPNSvX1/169cvcH8+X5lDYX+u+eFzVcnEdHq4nJSUFEmSj49Prn1so3u2vigZJk+erM8//1yS9Oqrr6pq1apOrgiFsW7dOr3//vtq2LChJk+e7OxyUAQuXrwoSXr66ae1detWvfPOOzp9+rSSk5P17bffKigoSLt27dK9996rjIwMJ1eLwrBYLGrevLkqVaqkzMxM7d27V7GxsZIkf39/+fn5OblCFDU+X5U+fK4quQjxcDm5XXeZE4vFcgMrQXGaOHGiXnnlFUlXrr99/vnnnVwRCuPixYsaOnSo3NzcNH/+/Dw/JMI8UlNTJUlnzpzRF198oXHjxqlKlSry8/PTvffeqx9++EGenp7auXOnFixY4ORqUVAXLlxQ165d9fLLLyswMFA///yzUlNTdfbsWc2ZM0fnz5/XyJEjGYkvYfh8VbrwuapkI8TD5ZQtW1aSlJaWlmsf2wdLRgrMLzMzU4899pgiIyMlSXfddZe+/PJLPkCYzDPPPKO4uDj985//1G233ebsclBEbO+xzZs3V9++fbO1N2jQQA899JAkafny5cVZGq7DW2+9pS1btqhGjRr673//qy5dusjHx0eVK1fWiBEjtHr1arm7u2vmzJn2a2lhfny+Kh34XFU6EOLhcgICAiRJiYmJufaxXavFtCBzu3Dhgnr27KnZs2dLkh588EGtWLGCUVyTWbVqlT7++GM1atRIkyZNcnY5KEL+/v6SpBYtWuTap2nTppKkI0eOFENFKApfffWVpCvXytp+xldr3bq17r33XkmyT8WF+fH5quTjc1XpQYiHy2nUqJEkKS4uLtc+trbQ0NBiqAg3QkJCgtq1a2e/hdVzzz2nL774Ql5eXk6uDIX1n//8R5K0b98++fj4yGKxODxsi6MNHTpUFotFnTt3dmK1KAzb+3F6enqufTw8rqyR6+3tXSw14frZ/iYbNmyYax/bfcRt18nD/Ph8VbLxuap0YXV6uJzbbrtNK1assN/P9O8SEhJ09OhRSWL1a5M6fvy4OnfurMOHD8vd3V0ffPCBRo0a5eyycI1CQ0PVrl27XNt///13paenKyQkRFWrVlWzZs2KsTpcj9tvv11r1qzRli1bcu2zf/9+SXnftgqupXz58kpLS9OJEydy7WNbybx8+fLFVRZuMD5flVx8rip9GImHyxkwYICkKytdHzhwIFv7rFmzJEmdOnVSUFBQcZaGIpCRkaHevXvr8OHD8vLy0ldffcU/NCb30ksv6bfffsv1Ua1aNYd+77//vpMrRkE9/PDDkq5MlV+6dGm29tOnT+uLL76QJN1///3FWhuuXdeuXSVJc+fOVVZWVrb2c+fOadmyZZKkbt26FWdpuIH4fFUy8bmqdCLEw+WEhITo4YcfVlZWlvr166eYmBh726JFizR16lRJ0oQJE5xVIq7D1KlTtW3bNknShx9+qPvuu8/JFQHITcOGDTVixAhJVy6H+Pbbb+1tJ0+e1IMPPqiLFy/q5ptvVr9+/ZxVJgrppZdekpeXl7Zu3apBgwY53BM8NjZW99xzjxITExUcHKyhQ4c6sVIUJT5flUx8riqdLIZhGM4uAvi7xMREdenSRbt27ZK7u7uaNWumpKQk+3V8U6ZM0UsvveTkKlFYGRkZqlatmpKSkuTh4ZHvKuY9e/bk51wCBAUFKT4+XvPmzdOQIUOcXQ4KKTU1VQMGDNB3330nSapdu7aqVKmi3bt3KyMjQ4GBgfrhhx/yvL4arufrr7/WoEGDlJqaKi8vLzVq1EhZWVnau3evrFarAgMDtWrVKvt11DAH2/vtnDlz7F/AXY3PV+aU28+Vz1WlF9fEwyVVrlxZGzdu1FtvvaXFixdr37598vT0VKdOnfTkk08y4mNSu3btUlJSkqQrt0CJiorKs3/9+vWLoywAefD19dW3336rzz//XHPnztX27dt19uxZ1a1bV/3799fTTz+typUrO7tMFFL//v3VvHlzvf3221qzZo32798vDw8P3Xzzzbrvvvv05JNP5rhyPcyNz1clC5+rSi9G4gEAAAAAMAmuiQcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoR4AAAAAABMghAPAAAAAIBJEOIBAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoR4AAAAAABMghAPAAAAAIBJEOIBAAAAADAJQjwAAAAAACZBiAcAAAAAwCQI8QAAAAAAmAQhHgAAAAAAkyDEAwAAAABgEoR4AAAAAABMghAPACj1LBZLoR7nz5+XJMXFxdm3xcTEFEutCQkJunDhQrGc61odOHBAffr0UeXKleXj46M6depo165dzi7LJc2fP18Wi0W1atVydikAAJPwcHYBAAC4ipCQEFWtWjXffh4exf/PZ0ZGhiZPnqxp06Zp586dKl++fLHXUBCXLl1S165ddfz4cfn6+qpp06ZKTk5WcHCws0sDAKBEIMQDAPD/XnrpJQ0ZMqTA/WvWrKl9+/ZJkgIDA29QVVccP35cr7322g09R1FYv369jh8/LovFom3btqlRo0bOLgkAgBKFEA8AwDXy9PRUw4YNnV2GSzl79qwk6aabbiLAAwBwA3BNPAAAKDJZWVmSJG9vbydXAgBAyUSIBwDgGuW2sF1kZKQsFoteeOEFLVu2TA0aNJC3t7eCgoL05ZdfSpJSU1M1depUtWrVSmXLlpWPj48CAwP18MMPKyoqyuE8nTt3drimPCQkRBaLRevWrcu3xiFDhshisWjgwIE5ttsWVgsKCsp2TovFoh9++EE7duzQAw88oJtuukne3t6qW7eunn76aZ05c8bef926dbJYLBo6dKgkKT4+3v7azJ8/X5IUFBQki8WinTt3aty4capYsaLKli2rW265RefOnbMf6+DBgxo9erTq168vb29vVahQQbfffrumT5+u1NRUhzpt5y3Iw1aHTXp6ut599121adNGFSpUkK+vrxo0aKBnnnlGJ06cyPZa2c51++236/Lly3r77bfVvHlz+fn5yd/fX127dtXy5cvz/ZkUVGpqqj788EN1795dN910k7y8vFS+fHk1bdpU//znP3X8+HF7X6vVqlq1aslisejf//53rsd89NFHZbFYNHr0aIftp06d0vPPP6/GjRvLz89P5cqVU+vWrfX2228rLS0t23EK8jsOALgxmE4PAMAN8ssvv2jatGmqVKmSGjVqpL1796ply5ZKT09Xt27dtHHjRrm7uyskJERlypTR4cOH9cUXX+jLL7/UnDlzNHz4cElSs2bNlJycrN9//12SdMstt8jHx0cVKlS44c9h1apVmjlzpgzDUGhoqMqVK6fDhw/rnXfe0cqVKxUdHa1y5cqpQoUKateunU6fPq1Dhw7J29tbrVq1knRlav3VxowZo6ioKDVu3FjJycny9vZWpUqVJEmfffaZhg8frvT0dPn6+qpZs2a6ePGiNm/erM2bN2vevHn6/vvv7au5286bmyNHjtgDeZ06dezbT5w4oZ49e+qPP/6QxWJRYGCgKlWqpL1792r69OlasGCBVqxYkeOxMzIy1KNHD/33v/9VQECAGjVqpAMHDmjt2rVau3atZs6cqVGjRl3X637mzBl169ZNu3btksViUb169VSnTh0lJCRoz5492rNnjxYtWqRt27apVq1acnNzU3h4uF5//XUtXLhQzzzzTLZjpqamavHixZJk/7JFkqKiotSnTx8lJibK09NToaGhMgxD27Zt0++//66FCxfqhx9+ULVq1bIdM7ffcQDADWQAAFDKSTIkGfPmzSvUfrGxsfZ9Dx06ZN8eERFh337fffcZaWlphmEYxunTpw3DMIyZM2cakozQ0FAjPj7evl9qaqoxduxYQ5JRoUIFIzU1Nd9z5Sc8PNyQZDzyyCM5ts+bN8+QZAQGBjps79Spk/1899xzj3H8+HF72/Llyw13d3dDkjF9+vQCHc8wDCMwMNB+zC+//NK+/cyZM4ZhGMamTZsMDw8PQ5Lx6KOPGn/99Ze9z/bt243Q0FBDkhEWFmZcvnw53+e+Z88eo0KFCoYk49VXX7Vvt1qtRtu2bQ1JRvv27Y39+/fb286fP28MGzbMkGQEBAQYJ06csLetXbvWXn/ZsmWNzz77zGG/bt26GZKMypUrF6i+q1+vmjVrOmy3/dzq169vHDx40KHthx9+MPz8/AxJxrPPPmvffujQIXt9u3btynauzz//3JBkNG7c2L4tISHBqFy5sv01T0pKsrfFxMQYt912myHJ6NChg8Ox8vsdBwDcOEynBwDg/w0dOjTP6didO3cu9DGnTZtmvz68SpUqkqQdO3ZIknr06OEwOuzj46O3335bd955p/r166fExMTrf1LXqWrVqlqyZImqV69u39a7d2/17NlTkrJN/S+IDh066MEHH7T/d0BAgCQpIiJCmZmZuvPOOzV79myH2+i1aNFCP/zwg3x9fRUdHZ3vlO1Tp06pZ8+e+uuvv/TAAw8oMjLS3rZixQpt2LBBNWrU0A8//KAGDRrY2ypUqKCPP/5Yt912m86ePavp06fnePyJEyfq4YcfdtjvzTfflCQlJibqwIEDBX9B/uby5cv69ddfZbFYNH36dIWEhDi033XXXfrHP/4hSdq1a5d9e/369dWxY0dJ0sKFC7Mdd8GCBZIcR+GnTZumxMRE9e7dW7Nnz5a/v7+9rV69elq+fLnKly+v9evXa9WqVTnWm9PvOADgxiHEAwDw/0JCQtSuXbtcH82aNSvU8apXr666devmeB5Jmjt3rmbOnOlwbbm3t7d+/PFHffLJJ6pZs+b1PaEicMcdd8jHxyfbdtvK8+fPny/0Mdu3b59tW3JystauXStJeuqpp3LcLzg4WPfdd58kadmyZbkePyUlRb169VJ8fLxuvfVWLViwQBaLxd6+dOlSSVLfvn1VpkyZbPtfvYbAt99+m+M5evXqlW3b1avxX8vrYuPp6akjR44oJSVF99xzT7Z2wzDsdaekpDi0DRs2TNKVyxKsVqt9+4kTJ/TTTz/Jw8NDgwYNsm+3vRa5rZlw0003qXv37pJyfi1y+x0HANw4XBMPAMD/K+x94vNz9ej11UaMGKG5c+dq7969GjNmjMaOHasWLVrojjvu0N13362OHTvKw8M1/onO7YsEX19fSVJmZmahj5nT63LkyBFlZGRIunLNf25uueUWff7557mOdFutVj388MPaunWrateureXLl2f7EmL37t2SroRS26yIv0tKSpJ0ZZE9wzAcvgSQcn5dbK+JdG2vy9/5+Pjo1KlT2rx5sw4ePKjY2Fjt379f27dvt9d3dVCXpAEDBuiJJ57Qn3/+qZ9//ll33HGHJGnRokXKyspSr1697GsUXLp0SfHx8ZKkSZMm6d13382xjri4OEnS/v37s7Xl9jsOALhxXOMTAgAAJdDVoe5q5cuX16ZNmzRt2jR9/vnniomJ0fbt27V9+3a99dZbqlq1qiZPnqxHH320mCvOzsvLK892wzAKfcycXpcLFy7Y/39eC/bZpthfvHgxx/ZnnnlGy5cvV5kyZbRixYocF2P766+/JEnHjh3TsWPH8qw1KytLFy9edJjaL92Y1+VqJ0+e1OjRo7VixQqHoO7n56dbb71VmZmZ+u2337Lt5+fnpwcffFAff/yxFi5caA/xOU2lt70O0v++2MhLTrMLcvsdBwDcOEynBwDACcqVK6eJEyfq0KFDOnTokGbPnq2HHnpI/v7+On36tEaOHKlvvvmmyM6XW6hMTk4usnNcj3Llytn//9Xh8u9sI9BX97d5//339e6778rNzU2fffaZWrRokeMxbFPR33//fRmGke/j7wH+RktLS1PXrl21bNky+fv7a8KECVq2bJkOHDigCxcuaO3atfZr33Nim1L/zTffKDU1Vdu3b9eePXsUEBCge++9197v6ksJdu3ale/rEB0dfeOeNACgwAjxAAAUs9OnT2v9+vU6e/aspCsLkj366KP6/PPPdezYMfut2XJanKywbNPy09PTc2y/+l7jzlSvXj17rdu2bcu1n+02e39f7O3bb7+1X0v/r3/9S3369Mn1GLaF7Pbs2ZNrn2PHjmnTpk1OeX2WLVumffv2ycPDQ5s2bdJrr72mPn36KDQ0VO7u7pKkhISEXPdv06aNGjVqpEuXLmnNmjUO1717enra+/n7+9un1uf1WuzatUt//PGH/QsUAIBzEeIBAChmd911lzp27Kj58+dnaytbtqxuv/12SVemctu4uf3vn+zCTNW2rfye0/XMWVlZWrFiRYGPdSOVKVNGXbt2lSS98847OfY5cuSIvd4ePXrYt2/btk0PPfSQrFarhg4dqueeey7Pc9kWpfvyyy91+vTpHPsMGzZMbdq00UMPPVTYp3LdYmNjJV2ZbfD3LyukKyvvr1y5UlLu197bRuOXLl2q5cuXS3KcSm9jG5n/4IMPsl1fL12ZFdGlSxe1bNky158LAKB4EeIBAChmttXBJ06cqB9++MGh7bfffrOPwNtu4yZdCfc2tsXICsK2EvyePXv03nvv2b8ASEpK0rBhwwp0LXRxiYyMlIeHh1avXq2RI0c6XPe+Y8cO9ezZU2lpaWrevLkGDx4sSTp69Kh69eql5ORk3XXXXfroo4/yPc+DDz6oZs2a6fz587rrrrscRqEvXryoMWPG6KeffpLFYtGLL75Y9E80Hw0bNpR05Wf07rvvOnxps2nTJt1xxx06d+6cpOyr09sMGjRIHh4eWrJkiXbu3KmwsDDdfPPN2fq98MILKlu2rH777TcNHDjQPjtEuvJ7ds899ygxMVEVKlTQ2LFji/JpAgCuESEeAIBiNm7cOPXo0UOXLl1Sjx49VLNmTbVu3VpBQUHq0KGD/vrrL/Xu3VsjRoyw71OpUiUFBgZKku677z61bt062xcAOenZs6c6dOhgP29gYKDCwsJUs2ZNffbZZ5o4ceKNeZLXoE2bNpo7d668vLw0Z84c3XTTTWrVqpUaNmyoFi1a6MCBA2rWrJmWLl1qvy/5448/rhMnTki6MkOhd+/e6tSpk9q3b5/t8cQTT0i6cgu3ZcuWKTQ0VH/88YeaNm2qhg0bKiwsTNWrV9fMmTMlSdOnT9fdd99d7K9D79691bZtW0lXbrdn+/2oVauW2rRpoz179tgXrDt+/HiOMzNuuukm3XPPPbp06ZKknEfhpSuXcixevFjlypXTF198oRo1aqhly5Zq0qSJ6tevr6ioKJUpU0bff/+9qlateoOeMQCgMAjxAAAUM3d3dy1btkzvvPOO2rZtq5SUFP3xxx9KSUnRnXf+X3t36NJoGAdw/HdzKBbzooomGdhErHpgVwSDbcWg7H+wyKLajCYxaFPBoDKDQdBXgzIEkyCzCIviJccdHifc6c1nfD6w9rD3eVn6vu94ft9jY2Mjtre334yZ29raitHR0Xh+fo6bm5uo1WrvXiuXy8Xe3l4sLS3F0NBQPDw8xN3dXYyPj0e1Wo3Z2dnPus2/Mjc3F+fn51EqlaJQKMTl5WXU6/UYGxuL1dXVOD09jb6+vub6n0+139/fj93d3Tg6Oopqtfrmk2VZc21/f3+cnZ1FpVKJkZGRuL+/jyzLoqenJ6ampuLw8DAWFxf/672/6ujoiIODg1heXo7h4eFoNBqRZVnk8/mYmZmJ4+Pj5ui8x8fHODk5+e33vI5L7Orq+uPvPDk5GVdXV1Eul2NgYCCur6+jVqtFb29vzM/PR5ZlzYcKALTet5d/nYECAMCXs7KyEgsLCzE9PR2bm5ut3g4AH8SbeACANrS+vh4REaVSqcU7AeAj5d9fAgDAV9doNOL29ja6u7ujUqnExcVFFIvFmJiYaPXWAPhAIh4AoA08PT39cgJ9LpeLtbW1Fu4IgM/g7/QAAG2gUCjE4OBgdHZ2RrFYjJ2dneZkAgDah4PtAAAAIBHexAMAAEAiRDwAAAAkQsQDAABAIkQ8AAAAJELEAwAAQCJEPAAAACRCxAMAAEAiRDwAAAAkQsQDAABAIkQ8AAAAJELEAwAAQCJEPAAAACRCxAMAAEAiRDwAAAAkQsQDAABAIkQ8AAAAJELEAwAAQCJEPAAAACRCxAMAAEAiRDwAAAAkQsQDAABAIkQ8AAAAJELEAwAAQCJEPAAAACRCxAMAAEAiRDwAAAAkQsQDAABAIkQ8AAAAJOIHDatN4n9qVEkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 975x690 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3.25, 2.3), dpi=300, layout=\"constrained\")\n",
    "\n",
    "model_name = \"MPNet\"\n",
    "\n",
    "for first_unfrozen_layer in np.arange(13):\n",
    "    ## save losses and accuracies\n",
    "    saving_path = (\n",
    "        Path(\"embeddings_\" + model_name.lower())\n",
    "        / Path(\"updated_dataset\")\n",
    "        / Path(\"freezing_experiment\")\n",
    "    )\n",
    "\n",
    "    # saving_name_losses = Path(\n",
    "    #     \"losses_first_unfrozen_layer_\" + str(first_unfrozen_layer) + \"_v1\"\n",
    "    # )\n",
    "    saving_name_accuracies = Path(\n",
    "        \"knn_accuracies_first_unfrozen_layer_\"\n",
    "        + str(first_unfrozen_layer)\n",
    "        + \"_v1.npy\"\n",
    "    )\n",
    "\n",
    "    # np.load(variables_path / saving_path / saving_name_losses, losses)\n",
    "    knn_accuracies = np.load(\n",
    "        variables_path / saving_path / saving_name_accuracies,\n",
    "    )\n",
    "\n",
    "    ax.scatter(\n",
    "        first_unfrozen_layer, knn_accuracies[0, 0], c=\"k\", marker=\".\", s=3\n",
    "    )\n",
    "ax.hlines(0.374, 0, 13, color=\"k\", linestyle=\"--\")\n",
    "ax.text(\n",
    "    0,\n",
    "    0.374 + 0.001,\n",
    "    f\"Before fine-tuning\",\n",
    "    fontsize=5,\n",
    "    va=\"bottom\",\n",
    "    ha=\"left\",\n",
    ")\n",
    "ax.hlines(0.589, 0, 13, color=\"k\", linestyle=\"--\")\n",
    "ax.hlines(0.58, 0, 13, color=\"r\", linestyle=\"-\")\n",
    "ax.text(\n",
    "    0,\n",
    "    0.589 + 0.001,\n",
    "    f\"After fine-tuning\",\n",
    "    fontsize=5,\n",
    "    va=\"bottom\",\n",
    "    ha=\"left\",\n",
    ")\n",
    "ax.set_xlabel(\"First unfrozen layer\")\n",
    "ax.set_ylabel(\"kNN accuracy\")\n",
    "\n",
    "\n",
    "fig.savefig(figures_path / \"freezing_experiment_mpnet_v1.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
