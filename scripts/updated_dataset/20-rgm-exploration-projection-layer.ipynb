{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import gc\n",
    "\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import memory_profiler\n",
    "\n",
    "%load_ext memory_profiler\n",
    "\n",
    "from pathlib import Path\n",
    "import distro\n",
    "\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1+cu121'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from text_embeddings_src.model_stuff import (\n",
    "    train_loop,\n",
    "    train_loop_with_projection_head,\n",
    ")\n",
    "from text_embeddings_src.data_stuff import (\n",
    "    MultOverlappingSentencesPairDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(line_length=79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_path = Path(\"../../results/variables\")\n",
    "figures_path = Path(\"../../results/figures/updated_dataset\")\n",
    "data_path = Path(\"../../data\")\n",
    "# berenslab_data_path = Path(\"/gpfs01/berens/data/data/GPT_wiki_intro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"../matplotlib_style.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Rita González-Márquez\n",
      "\n",
      "Last updated: 2023-12-15 18:29:13CET\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.5\n",
      "IPython version      : 8.18.1\n",
      "\n",
      "transformers: 4.35.2\n",
      "openTSNE    : 1.0.0\n",
      "\n",
      "Compiler    : GCC 11.2.0\n",
      "OS          : Linux\n",
      "Release     : 3.10.0-1160.el7.x86_64\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 64\n",
      "Architecture: 64bit\n",
      "\n",
      "Hostname: rgonzalesmarquez_GPU0-llm_gber7\n",
      "\n",
      "distro         : 1.8.0\n",
      "numpy          : 1.26.2\n",
      "matplotlib     : 3.8.2\n",
      "pandas         : 2.1.3\n",
      "torch          : 2.1.1\n",
      "scipy          : 1.11.4\n",
      "jupyter_black  : 0.3.4\n",
      "memory_profiler: 0.61.0\n",
      "\n",
      "Watermark: 2.4.3\n",
      "\n",
      "Ubuntu 22.04.3 LTS\n"
     ]
    }
   ],
   "source": [
    "%watermark -a 'Rita González-Márquez' -t -d -tz -u -v -iv -w -m -h -p transformers,openTSNE\n",
    "print(distro.name(pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 293 ms, sys: 76.9 ms, total: 370 ms\n",
      "Wall time: 556 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iclr2024 = pd.read_parquet(\n",
    "    data_path / \"iclr2024.parquet.gzip\",\n",
    "    # index=False,\n",
    "    engine=\"pyarrow\",\n",
    "    # compression=\"gzip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iclr2024.keywords = iclr2024.keywords.transform(lambda x: list(x))\n",
    "iclr2024.scores = iclr2024.scores.transform(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>decision</th>\n",
       "      <th>scores</th>\n",
       "      <th>keywords</th>\n",
       "      <th>gender-first</th>\n",
       "      <th>gender-last</th>\n",
       "      <th>t-SNE x</th>\n",
       "      <th>t-SNE y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>S1VaB4cex</td>\n",
       "      <td>FractalNet: Ultra-Deep Neural Networks without...</td>\n",
       "      <td>We introduce a design strategy for neural netw...</td>\n",
       "      <td>Gustav Larsson, Michael Maire, Gregory Shakhna...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "      <td>[5, 7, 6, 6]</td>\n",
       "      <td>[]</td>\n",
       "      <td>male</td>\n",
       "      <td>male</td>\n",
       "      <td>-28.117955</td>\n",
       "      <td>-20.418127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>H1W1UN9gg</td>\n",
       "      <td>Deep Information Propagation</td>\n",
       "      <td>We study the behavior of untrained neural netw...</td>\n",
       "      <td>Samuel S. Schoenholz, Justin Gilmer, Surya Gan...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "      <td>[8, 9, 8]</td>\n",
       "      <td>[theory, deep learning]</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>-32.466820</td>\n",
       "      <td>-10.791123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>r1GKzP5xx</td>\n",
       "      <td>Recurrent Normalization Propagation</td>\n",
       "      <td>We propose a LSTM parametrization  that preser...</td>\n",
       "      <td>César Laurent, Nicolas Ballas, Pascal Vincent</td>\n",
       "      <td>Invite to Workshop Track</td>\n",
       "      <td>[4, 6, 6]</td>\n",
       "      <td>[deep learning, optimization]</td>\n",
       "      <td>None</td>\n",
       "      <td>male</td>\n",
       "      <td>3.504240</td>\n",
       "      <td>19.946053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2017</td>\n",
       "      <td>S1J0E-71l</td>\n",
       "      <td>Surprisal-Driven Feedback in Recurrent Networks</td>\n",
       "      <td>Recurrent neural nets are widely used for pred...</td>\n",
       "      <td>K, a, m, i, l,  , R, o, c, k, i</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[3, 4, 3]</td>\n",
       "      <td>[unsupervised learning, applications, deep lea...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4.553473</td>\n",
       "      <td>16.037763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>SJGCiw5gl</td>\n",
       "      <td>Pruning Convolutional Neural Networks for Reso...</td>\n",
       "      <td>We propose a new formulation for pruning convo...</td>\n",
       "      <td>Pavlo Molchanov, Stephen Tyree, Tero Karras, T...</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "      <td>[6, 7, 9]</td>\n",
       "      <td>[deep learning, transfer learning]</td>\n",
       "      <td>None</td>\n",
       "      <td>male</td>\n",
       "      <td>-25.827705</td>\n",
       "      <td>-37.891772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24342</th>\n",
       "      <td>7299</td>\n",
       "      <td>2024</td>\n",
       "      <td>1bbPQShCT2</td>\n",
       "      <td>I-PHYRE: Interactive Physical Reasoning</td>\n",
       "      <td>Current evaluation protocols predominantly ass...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[intuitive physics, physical reasoning]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>43.137120</td>\n",
       "      <td>44.316133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24343</th>\n",
       "      <td>7300</td>\n",
       "      <td>2024</td>\n",
       "      <td>Ny150AblPu</td>\n",
       "      <td>EXPOSING TEXT-IMAGE INCONSISTENCY USING DIFFUS...</td>\n",
       "      <td>In the battle against widespread online misinf...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[mis-contextualization, media forensic]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>59.742172</td>\n",
       "      <td>-22.673627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24344</th>\n",
       "      <td>7301</td>\n",
       "      <td>2024</td>\n",
       "      <td>ZGBOfAQrMl</td>\n",
       "      <td>Video Super-Resolution Transformer with Masked...</td>\n",
       "      <td>Recently, Vision Transformer has achieved grea...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[video super-resolution, adaptive, memory and ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>57.933273</td>\n",
       "      <td>-3.932825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24345</th>\n",
       "      <td>7302</td>\n",
       "      <td>2024</td>\n",
       "      <td>J2kRjUAOLh</td>\n",
       "      <td>Contrastive Predict-and-Search for Mixed Integ...</td>\n",
       "      <td>Mixed integer linear programs  (MILP) are flex...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[mixed integer programs; contrastive learning]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-11.437999</td>\n",
       "      <td>21.289523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24346</th>\n",
       "      <td>7303</td>\n",
       "      <td>2024</td>\n",
       "      <td>U0P622bfUN</td>\n",
       "      <td>Federated Generative Learning with Foundation ...</td>\n",
       "      <td>Existing federated learning solutions focus on...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[federated learning, non-iid data]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-65.112587</td>\n",
       "      <td>18.746354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24347 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  year          id  \\\n",
       "0          0  2017   S1VaB4cex   \n",
       "1          1  2017   H1W1UN9gg   \n",
       "2          2  2017   r1GKzP5xx   \n",
       "3          3  2017   S1J0E-71l   \n",
       "4          4  2017   SJGCiw5gl   \n",
       "...      ...   ...         ...   \n",
       "24342   7299  2024  1bbPQShCT2   \n",
       "24343   7300  2024  Ny150AblPu   \n",
       "24344   7301  2024  ZGBOfAQrMl   \n",
       "24345   7302  2024  J2kRjUAOLh   \n",
       "24346   7303  2024  U0P622bfUN   \n",
       "\n",
       "                                                   title  \\\n",
       "0      FractalNet: Ultra-Deep Neural Networks without...   \n",
       "1                           Deep Information Propagation   \n",
       "2                    Recurrent Normalization Propagation   \n",
       "3        Surprisal-Driven Feedback in Recurrent Networks   \n",
       "4      Pruning Convolutional Neural Networks for Reso...   \n",
       "...                                                  ...   \n",
       "24342            I-PHYRE: Interactive Physical Reasoning   \n",
       "24343  EXPOSING TEXT-IMAGE INCONSISTENCY USING DIFFUS...   \n",
       "24344  Video Super-Resolution Transformer with Masked...   \n",
       "24345  Contrastive Predict-and-Search for Mixed Integ...   \n",
       "24346  Federated Generative Learning with Foundation ...   \n",
       "\n",
       "                                                abstract  \\\n",
       "0      We introduce a design strategy for neural netw...   \n",
       "1      We study the behavior of untrained neural netw...   \n",
       "2      We propose a LSTM parametrization  that preser...   \n",
       "3      Recurrent neural nets are widely used for pred...   \n",
       "4      We propose a new formulation for pruning convo...   \n",
       "...                                                  ...   \n",
       "24342  Current evaluation protocols predominantly ass...   \n",
       "24343  In the battle against widespread online misinf...   \n",
       "24344  Recently, Vision Transformer has achieved grea...   \n",
       "24345  Mixed integer linear programs  (MILP) are flex...   \n",
       "24346  Existing federated learning solutions focus on...   \n",
       "\n",
       "                                                 authors  \\\n",
       "0      Gustav Larsson, Michael Maire, Gregory Shakhna...   \n",
       "1      Samuel S. Schoenholz, Justin Gilmer, Surya Gan...   \n",
       "2          César Laurent, Nicolas Ballas, Pascal Vincent   \n",
       "3                        K, a, m, i, l,  , R, o, c, k, i   \n",
       "4      Pavlo Molchanov, Stephen Tyree, Tero Karras, T...   \n",
       "...                                                  ...   \n",
       "24342                                                      \n",
       "24343                                                      \n",
       "24344                                                      \n",
       "24345                                                      \n",
       "24346                                                      \n",
       "\n",
       "                       decision        scores  \\\n",
       "0               Accept (Poster)  [5, 7, 6, 6]   \n",
       "1               Accept (Poster)     [8, 9, 8]   \n",
       "2      Invite to Workshop Track     [4, 6, 6]   \n",
       "3                        Reject     [3, 4, 3]   \n",
       "4               Accept (Poster)     [6, 7, 9]   \n",
       "...                         ...           ...   \n",
       "24342                                      []   \n",
       "24343                                      []   \n",
       "24344                                      []   \n",
       "24345                                      []   \n",
       "24346                                      []   \n",
       "\n",
       "                                                keywords gender-first  \\\n",
       "0                                                     []         male   \n",
       "1                                [theory, deep learning]         male   \n",
       "2                          [deep learning, optimization]         None   \n",
       "3      [unsupervised learning, applications, deep lea...         None   \n",
       "4                     [deep learning, transfer learning]         None   \n",
       "...                                                  ...          ...   \n",
       "24342            [intuitive physics, physical reasoning]         None   \n",
       "24343            [mis-contextualization, media forensic]         None   \n",
       "24344  [video super-resolution, adaptive, memory and ...         None   \n",
       "24345     [mixed integer programs; contrastive learning]         None   \n",
       "24346                 [federated learning, non-iid data]         None   \n",
       "\n",
       "      gender-last    t-SNE x    t-SNE y  \n",
       "0            male -28.117955 -20.418127  \n",
       "1            None -32.466820 -10.791123  \n",
       "2            male   3.504240  19.946053  \n",
       "3            None   4.553473  16.037763  \n",
       "4            male -25.827705 -37.891772  \n",
       "...           ...        ...        ...  \n",
       "24342        None  43.137120  44.316133  \n",
       "24343        None  59.742172 -22.673627  \n",
       "24344        None  57.933273  -3.932825  \n",
       "24345        None -11.437999  21.289523  \n",
       "24346        None -65.112587  18.746354  \n",
       "\n",
       "[24347 rows x 13 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iclr2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_iclr = np.load(variables_path / \"updated_dataset\" / \"labels_iclr.npy\")\n",
    "colors_iclr = np.load(variables_path / \"updated_dataset\" / \"colors_iclr.npy\")\n",
    "\n",
    "pickle_in = open(\n",
    "    variables_path / \"updated_dataset\" / \"dict_label_to_color.pkl\", \"rb\"\n",
    ")\n",
    "dict_label_to_color = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"BERT\",\n",
    "    \"MPNet\",\n",
    "    \"SBERT\",\n",
    "    \"SciBERT\",\n",
    "    \"SPECTER\",\n",
    "    \"SciNCL\",\n",
    "    \"SimCSE\",\n",
    "    \"DeCLUTR\",\n",
    "    \"DeCLUTR-sci\",\n",
    "    \"SPECTER2\",\n",
    "]\n",
    "\n",
    "\n",
    "model_paths = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"microsoft/mpnet-base\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    \"allenai/specter\",\n",
    "    \"malteos/scincl\",\n",
    "    \"princeton-nlp/unsup-simcse-bert-base-uncased\",\n",
    "    \"johngiorgi/declutr-base\",\n",
    "    \"johngiorgi/declutr-sci-base\",\n",
    "    \"allenai/specter2_base\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set the random seed\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/mpnet-base\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "i = 1\n",
    "\n",
    "# random_state = random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on device: {}\".format(device))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_paths[i])\n",
    "model = AutoModel.from_pretrained(model_paths[i])\n",
    "print(model_paths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight: True\n",
      "embeddings.position_embeddings.weight: True\n",
      "embeddings.LayerNorm.weight: True\n",
      "embeddings.LayerNorm.bias: True\n",
      "encoder.layer.0.attention.attn.q.weight: True\n",
      "encoder.layer.0.attention.attn.q.bias: True\n",
      "encoder.layer.0.attention.attn.k.weight: True\n",
      "encoder.layer.0.attention.attn.k.bias: True\n",
      "encoder.layer.0.attention.attn.v.weight: True\n",
      "encoder.layer.0.attention.attn.v.bias: True\n",
      "encoder.layer.0.attention.attn.o.weight: True\n",
      "encoder.layer.0.attention.attn.o.bias: True\n",
      "encoder.layer.0.attention.LayerNorm.weight: True\n",
      "encoder.layer.0.attention.LayerNorm.bias: True\n",
      "encoder.layer.0.intermediate.dense.weight: True\n",
      "encoder.layer.0.intermediate.dense.bias: True\n",
      "encoder.layer.0.output.dense.weight: True\n",
      "encoder.layer.0.output.dense.bias: True\n",
      "encoder.layer.0.output.LayerNorm.weight: True\n",
      "encoder.layer.0.output.LayerNorm.bias: True\n",
      "encoder.layer.1.attention.attn.q.weight: True\n",
      "encoder.layer.1.attention.attn.q.bias: True\n",
      "encoder.layer.1.attention.attn.k.weight: True\n",
      "encoder.layer.1.attention.attn.k.bias: True\n",
      "encoder.layer.1.attention.attn.v.weight: True\n",
      "encoder.layer.1.attention.attn.v.bias: True\n",
      "encoder.layer.1.attention.attn.o.weight: True\n",
      "encoder.layer.1.attention.attn.o.bias: True\n",
      "encoder.layer.1.attention.LayerNorm.weight: True\n",
      "encoder.layer.1.attention.LayerNorm.bias: True\n",
      "encoder.layer.1.intermediate.dense.weight: True\n",
      "encoder.layer.1.intermediate.dense.bias: True\n",
      "encoder.layer.1.output.dense.weight: True\n",
      "encoder.layer.1.output.dense.bias: True\n",
      "encoder.layer.1.output.LayerNorm.weight: True\n",
      "encoder.layer.1.output.LayerNorm.bias: True\n",
      "encoder.layer.2.attention.attn.q.weight: True\n",
      "encoder.layer.2.attention.attn.q.bias: True\n",
      "encoder.layer.2.attention.attn.k.weight: True\n",
      "encoder.layer.2.attention.attn.k.bias: True\n",
      "encoder.layer.2.attention.attn.v.weight: True\n",
      "encoder.layer.2.attention.attn.v.bias: True\n",
      "encoder.layer.2.attention.attn.o.weight: True\n",
      "encoder.layer.2.attention.attn.o.bias: True\n",
      "encoder.layer.2.attention.LayerNorm.weight: True\n",
      "encoder.layer.2.attention.LayerNorm.bias: True\n",
      "encoder.layer.2.intermediate.dense.weight: True\n",
      "encoder.layer.2.intermediate.dense.bias: True\n",
      "encoder.layer.2.output.dense.weight: True\n",
      "encoder.layer.2.output.dense.bias: True\n",
      "encoder.layer.2.output.LayerNorm.weight: True\n",
      "encoder.layer.2.output.LayerNorm.bias: True\n",
      "encoder.layer.3.attention.attn.q.weight: True\n",
      "encoder.layer.3.attention.attn.q.bias: True\n",
      "encoder.layer.3.attention.attn.k.weight: True\n",
      "encoder.layer.3.attention.attn.k.bias: True\n",
      "encoder.layer.3.attention.attn.v.weight: True\n",
      "encoder.layer.3.attention.attn.v.bias: True\n",
      "encoder.layer.3.attention.attn.o.weight: True\n",
      "encoder.layer.3.attention.attn.o.bias: True\n",
      "encoder.layer.3.attention.LayerNorm.weight: True\n",
      "encoder.layer.3.attention.LayerNorm.bias: True\n",
      "encoder.layer.3.intermediate.dense.weight: True\n",
      "encoder.layer.3.intermediate.dense.bias: True\n",
      "encoder.layer.3.output.dense.weight: True\n",
      "encoder.layer.3.output.dense.bias: True\n",
      "encoder.layer.3.output.LayerNorm.weight: True\n",
      "encoder.layer.3.output.LayerNorm.bias: True\n",
      "encoder.layer.4.attention.attn.q.weight: True\n",
      "encoder.layer.4.attention.attn.q.bias: True\n",
      "encoder.layer.4.attention.attn.k.weight: True\n",
      "encoder.layer.4.attention.attn.k.bias: True\n",
      "encoder.layer.4.attention.attn.v.weight: True\n",
      "encoder.layer.4.attention.attn.v.bias: True\n",
      "encoder.layer.4.attention.attn.o.weight: True\n",
      "encoder.layer.4.attention.attn.o.bias: True\n",
      "encoder.layer.4.attention.LayerNorm.weight: True\n",
      "encoder.layer.4.attention.LayerNorm.bias: True\n",
      "encoder.layer.4.intermediate.dense.weight: True\n",
      "encoder.layer.4.intermediate.dense.bias: True\n",
      "encoder.layer.4.output.dense.weight: True\n",
      "encoder.layer.4.output.dense.bias: True\n",
      "encoder.layer.4.output.LayerNorm.weight: True\n",
      "encoder.layer.4.output.LayerNorm.bias: True\n",
      "encoder.layer.5.attention.attn.q.weight: True\n",
      "encoder.layer.5.attention.attn.q.bias: True\n",
      "encoder.layer.5.attention.attn.k.weight: True\n",
      "encoder.layer.5.attention.attn.k.bias: True\n",
      "encoder.layer.5.attention.attn.v.weight: True\n",
      "encoder.layer.5.attention.attn.v.bias: True\n",
      "encoder.layer.5.attention.attn.o.weight: True\n",
      "encoder.layer.5.attention.attn.o.bias: True\n",
      "encoder.layer.5.attention.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.attn.q.weight: True\n",
      "encoder.layer.6.attention.attn.q.bias: True\n",
      "encoder.layer.6.attention.attn.k.weight: True\n",
      "encoder.layer.6.attention.attn.k.bias: True\n",
      "encoder.layer.6.attention.attn.v.weight: True\n",
      "encoder.layer.6.attention.attn.v.bias: True\n",
      "encoder.layer.6.attention.attn.o.weight: True\n",
      "encoder.layer.6.attention.attn.o.bias: True\n",
      "encoder.layer.6.attention.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.attn.q.weight: True\n",
      "encoder.layer.7.attention.attn.q.bias: True\n",
      "encoder.layer.7.attention.attn.k.weight: True\n",
      "encoder.layer.7.attention.attn.k.bias: True\n",
      "encoder.layer.7.attention.attn.v.weight: True\n",
      "encoder.layer.7.attention.attn.v.bias: True\n",
      "encoder.layer.7.attention.attn.o.weight: True\n",
      "encoder.layer.7.attention.attn.o.bias: True\n",
      "encoder.layer.7.attention.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.attn.q.weight: True\n",
      "encoder.layer.8.attention.attn.q.bias: True\n",
      "encoder.layer.8.attention.attn.k.weight: True\n",
      "encoder.layer.8.attention.attn.k.bias: True\n",
      "encoder.layer.8.attention.attn.v.weight: True\n",
      "encoder.layer.8.attention.attn.v.bias: True\n",
      "encoder.layer.8.attention.attn.o.weight: True\n",
      "encoder.layer.8.attention.attn.o.bias: True\n",
      "encoder.layer.8.attention.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.attn.q.weight: True\n",
      "encoder.layer.9.attention.attn.q.bias: True\n",
      "encoder.layer.9.attention.attn.k.weight: True\n",
      "encoder.layer.9.attention.attn.k.bias: True\n",
      "encoder.layer.9.attention.attn.v.weight: True\n",
      "encoder.layer.9.attention.attn.v.bias: True\n",
      "encoder.layer.9.attention.attn.o.weight: True\n",
      "encoder.layer.9.attention.attn.o.bias: True\n",
      "encoder.layer.9.attention.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.attn.q.weight: True\n",
      "encoder.layer.10.attention.attn.q.bias: True\n",
      "encoder.layer.10.attention.attn.k.weight: True\n",
      "encoder.layer.10.attention.attn.k.bias: True\n",
      "encoder.layer.10.attention.attn.v.weight: True\n",
      "encoder.layer.10.attention.attn.v.bias: True\n",
      "encoder.layer.10.attention.attn.o.weight: True\n",
      "encoder.layer.10.attention.attn.o.bias: True\n",
      "encoder.layer.10.attention.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.attn.q.weight: True\n",
      "encoder.layer.11.attention.attn.q.bias: True\n",
      "encoder.layer.11.attention.attn.k.weight: True\n",
      "encoder.layer.11.attention.attn.k.bias: True\n",
      "encoder.layer.11.attention.attn.v.weight: True\n",
      "encoder.layer.11.attention.attn.v.bias: True\n",
      "encoder.layer.11.attention.attn.o.weight: True\n",
      "encoder.layer.11.attention.attn.o.bias: True\n",
      "encoder.layer.11.attention.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "encoder.relative_attention_bias.weight: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT\n",
      "embeddings.word_embeddings.weight: True\n",
      "embeddings.position_embeddings.weight: True\n",
      "embeddings.token_type_embeddings.weight: True\n",
      "embeddings.LayerNorm.weight: True\n",
      "embeddings.LayerNorm.bias: True\n",
      "encoder.layer.0.attention.self.query.weight: True\n",
      "encoder.layer.0.attention.self.query.bias: True\n",
      "encoder.layer.0.attention.self.key.weight: True\n",
      "encoder.layer.0.attention.self.key.bias: True\n",
      "encoder.layer.0.attention.self.value.weight: True\n",
      "encoder.layer.0.attention.self.value.bias: True\n",
      "encoder.layer.0.attention.output.dense.weight: True\n",
      "encoder.layer.0.attention.output.dense.bias: True\n",
      "encoder.layer.0.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.0.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.0.intermediate.dense.weight: True\n",
      "encoder.layer.0.intermediate.dense.bias: True\n",
      "encoder.layer.0.output.dense.weight: True\n",
      "encoder.layer.0.output.dense.bias: True\n",
      "encoder.layer.0.output.LayerNorm.weight: True\n",
      "encoder.layer.0.output.LayerNorm.bias: True\n",
      "encoder.layer.1.attention.self.query.weight: True\n",
      "encoder.layer.1.attention.self.query.bias: True\n",
      "encoder.layer.1.attention.self.key.weight: True\n",
      "encoder.layer.1.attention.self.key.bias: True\n",
      "encoder.layer.1.attention.self.value.weight: True\n",
      "encoder.layer.1.attention.self.value.bias: True\n",
      "encoder.layer.1.attention.output.dense.weight: True\n",
      "encoder.layer.1.attention.output.dense.bias: True\n",
      "encoder.layer.1.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.1.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.1.intermediate.dense.weight: True\n",
      "encoder.layer.1.intermediate.dense.bias: True\n",
      "encoder.layer.1.output.dense.weight: True\n",
      "encoder.layer.1.output.dense.bias: True\n",
      "encoder.layer.1.output.LayerNorm.weight: True\n",
      "encoder.layer.1.output.LayerNorm.bias: True\n",
      "encoder.layer.2.attention.self.query.weight: True\n",
      "encoder.layer.2.attention.self.query.bias: True\n",
      "encoder.layer.2.attention.self.key.weight: True\n",
      "encoder.layer.2.attention.self.key.bias: True\n",
      "encoder.layer.2.attention.self.value.weight: True\n",
      "encoder.layer.2.attention.self.value.bias: True\n",
      "encoder.layer.2.attention.output.dense.weight: True\n",
      "encoder.layer.2.attention.output.dense.bias: True\n",
      "encoder.layer.2.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.2.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.2.intermediate.dense.weight: True\n",
      "encoder.layer.2.intermediate.dense.bias: True\n",
      "encoder.layer.2.output.dense.weight: True\n",
      "encoder.layer.2.output.dense.bias: True\n",
      "encoder.layer.2.output.LayerNorm.weight: True\n",
      "encoder.layer.2.output.LayerNorm.bias: True\n",
      "encoder.layer.3.attention.self.query.weight: True\n",
      "encoder.layer.3.attention.self.query.bias: True\n",
      "encoder.layer.3.attention.self.key.weight: True\n",
      "encoder.layer.3.attention.self.key.bias: True\n",
      "encoder.layer.3.attention.self.value.weight: True\n",
      "encoder.layer.3.attention.self.value.bias: True\n",
      "encoder.layer.3.attention.output.dense.weight: True\n",
      "encoder.layer.3.attention.output.dense.bias: True\n",
      "encoder.layer.3.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.3.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.3.intermediate.dense.weight: True\n",
      "encoder.layer.3.intermediate.dense.bias: True\n",
      "encoder.layer.3.output.dense.weight: True\n",
      "encoder.layer.3.output.dense.bias: True\n",
      "encoder.layer.3.output.LayerNorm.weight: True\n",
      "encoder.layer.3.output.LayerNorm.bias: True\n",
      "encoder.layer.4.attention.self.query.weight: True\n",
      "encoder.layer.4.attention.self.query.bias: True\n",
      "encoder.layer.4.attention.self.key.weight: True\n",
      "encoder.layer.4.attention.self.key.bias: True\n",
      "encoder.layer.4.attention.self.value.weight: True\n",
      "encoder.layer.4.attention.self.value.bias: True\n",
      "encoder.layer.4.attention.output.dense.weight: True\n",
      "encoder.layer.4.attention.output.dense.bias: True\n",
      "encoder.layer.4.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.4.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.4.intermediate.dense.weight: True\n",
      "encoder.layer.4.intermediate.dense.bias: True\n",
      "encoder.layer.4.output.dense.weight: True\n",
      "encoder.layer.4.output.dense.bias: True\n",
      "encoder.layer.4.output.LayerNorm.weight: True\n",
      "encoder.layer.4.output.LayerNorm.bias: True\n",
      "encoder.layer.5.attention.self.query.weight: True\n",
      "encoder.layer.5.attention.self.query.bias: True\n",
      "encoder.layer.5.attention.self.key.weight: True\n",
      "encoder.layer.5.attention.self.key.bias: True\n",
      "encoder.layer.5.attention.self.value.weight: True\n",
      "encoder.layer.5.attention.self.value.bias: True\n",
      "encoder.layer.5.attention.output.dense.weight: True\n",
      "encoder.layer.5.attention.output.dense.bias: True\n",
      "encoder.layer.5.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.self.query.weight: True\n",
      "encoder.layer.6.attention.self.query.bias: True\n",
      "encoder.layer.6.attention.self.key.weight: True\n",
      "encoder.layer.6.attention.self.key.bias: True\n",
      "encoder.layer.6.attention.self.value.weight: True\n",
      "encoder.layer.6.attention.self.value.bias: True\n",
      "encoder.layer.6.attention.output.dense.weight: True\n",
      "encoder.layer.6.attention.output.dense.bias: True\n",
      "encoder.layer.6.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.self.query.weight: True\n",
      "encoder.layer.7.attention.self.query.bias: True\n",
      "encoder.layer.7.attention.self.key.weight: True\n",
      "encoder.layer.7.attention.self.key.bias: True\n",
      "encoder.layer.7.attention.self.value.weight: True\n",
      "encoder.layer.7.attention.self.value.bias: True\n",
      "encoder.layer.7.attention.output.dense.weight: True\n",
      "encoder.layer.7.attention.output.dense.bias: True\n",
      "encoder.layer.7.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.self.query.weight: True\n",
      "encoder.layer.8.attention.self.query.bias: True\n",
      "encoder.layer.8.attention.self.key.weight: True\n",
      "encoder.layer.8.attention.self.key.bias: True\n",
      "encoder.layer.8.attention.self.value.weight: True\n",
      "encoder.layer.8.attention.self.value.bias: True\n",
      "encoder.layer.8.attention.output.dense.weight: True\n",
      "encoder.layer.8.attention.output.dense.bias: True\n",
      "encoder.layer.8.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.self.query.weight: True\n",
      "encoder.layer.9.attention.self.query.bias: True\n",
      "encoder.layer.9.attention.self.key.weight: True\n",
      "encoder.layer.9.attention.self.key.bias: True\n",
      "encoder.layer.9.attention.self.value.weight: True\n",
      "encoder.layer.9.attention.self.value.bias: True\n",
      "encoder.layer.9.attention.output.dense.weight: True\n",
      "encoder.layer.9.attention.output.dense.bias: True\n",
      "encoder.layer.9.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.self.query.weight: True\n",
      "encoder.layer.10.attention.self.query.bias: True\n",
      "encoder.layer.10.attention.self.key.weight: True\n",
      "encoder.layer.10.attention.self.key.bias: True\n",
      "encoder.layer.10.attention.self.value.weight: True\n",
      "encoder.layer.10.attention.self.value.bias: True\n",
      "encoder.layer.10.attention.output.dense.weight: True\n",
      "encoder.layer.10.attention.output.dense.bias: True\n",
      "encoder.layer.10.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.self.query.weight: True\n",
      "encoder.layer.11.attention.self.query.bias: True\n",
      "encoder.layer.11.attention.self.key.weight: True\n",
      "encoder.layer.11.attention.self.key.bias: True\n",
      "encoder.layer.11.attention.self.value.weight: True\n",
      "encoder.layer.11.attention.self.value.bias: True\n",
      "encoder.layer.11.attention.output.dense.weight: True\n",
      "encoder.layer.11.attention.output.dense.bias: True\n",
      "encoder.layer.11.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n",
      "--------------------------------------\n",
      "MPNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.bias', 'mpnet.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight: True\n",
      "embeddings.position_embeddings.weight: True\n",
      "embeddings.LayerNorm.weight: True\n",
      "embeddings.LayerNorm.bias: True\n",
      "encoder.layer.0.attention.attn.q.weight: True\n",
      "encoder.layer.0.attention.attn.q.bias: True\n",
      "encoder.layer.0.attention.attn.k.weight: True\n",
      "encoder.layer.0.attention.attn.k.bias: True\n",
      "encoder.layer.0.attention.attn.v.weight: True\n",
      "encoder.layer.0.attention.attn.v.bias: True\n",
      "encoder.layer.0.attention.attn.o.weight: True\n",
      "encoder.layer.0.attention.attn.o.bias: True\n",
      "encoder.layer.0.attention.LayerNorm.weight: True\n",
      "encoder.layer.0.attention.LayerNorm.bias: True\n",
      "encoder.layer.0.intermediate.dense.weight: True\n",
      "encoder.layer.0.intermediate.dense.bias: True\n",
      "encoder.layer.0.output.dense.weight: True\n",
      "encoder.layer.0.output.dense.bias: True\n",
      "encoder.layer.0.output.LayerNorm.weight: True\n",
      "encoder.layer.0.output.LayerNorm.bias: True\n",
      "encoder.layer.1.attention.attn.q.weight: True\n",
      "encoder.layer.1.attention.attn.q.bias: True\n",
      "encoder.layer.1.attention.attn.k.weight: True\n",
      "encoder.layer.1.attention.attn.k.bias: True\n",
      "encoder.layer.1.attention.attn.v.weight: True\n",
      "encoder.layer.1.attention.attn.v.bias: True\n",
      "encoder.layer.1.attention.attn.o.weight: True\n",
      "encoder.layer.1.attention.attn.o.bias: True\n",
      "encoder.layer.1.attention.LayerNorm.weight: True\n",
      "encoder.layer.1.attention.LayerNorm.bias: True\n",
      "encoder.layer.1.intermediate.dense.weight: True\n",
      "encoder.layer.1.intermediate.dense.bias: True\n",
      "encoder.layer.1.output.dense.weight: True\n",
      "encoder.layer.1.output.dense.bias: True\n",
      "encoder.layer.1.output.LayerNorm.weight: True\n",
      "encoder.layer.1.output.LayerNorm.bias: True\n",
      "encoder.layer.2.attention.attn.q.weight: True\n",
      "encoder.layer.2.attention.attn.q.bias: True\n",
      "encoder.layer.2.attention.attn.k.weight: True\n",
      "encoder.layer.2.attention.attn.k.bias: True\n",
      "encoder.layer.2.attention.attn.v.weight: True\n",
      "encoder.layer.2.attention.attn.v.bias: True\n",
      "encoder.layer.2.attention.attn.o.weight: True\n",
      "encoder.layer.2.attention.attn.o.bias: True\n",
      "encoder.layer.2.attention.LayerNorm.weight: True\n",
      "encoder.layer.2.attention.LayerNorm.bias: True\n",
      "encoder.layer.2.intermediate.dense.weight: True\n",
      "encoder.layer.2.intermediate.dense.bias: True\n",
      "encoder.layer.2.output.dense.weight: True\n",
      "encoder.layer.2.output.dense.bias: True\n",
      "encoder.layer.2.output.LayerNorm.weight: True\n",
      "encoder.layer.2.output.LayerNorm.bias: True\n",
      "encoder.layer.3.attention.attn.q.weight: True\n",
      "encoder.layer.3.attention.attn.q.bias: True\n",
      "encoder.layer.3.attention.attn.k.weight: True\n",
      "encoder.layer.3.attention.attn.k.bias: True\n",
      "encoder.layer.3.attention.attn.v.weight: True\n",
      "encoder.layer.3.attention.attn.v.bias: True\n",
      "encoder.layer.3.attention.attn.o.weight: True\n",
      "encoder.layer.3.attention.attn.o.bias: True\n",
      "encoder.layer.3.attention.LayerNorm.weight: True\n",
      "encoder.layer.3.attention.LayerNorm.bias: True\n",
      "encoder.layer.3.intermediate.dense.weight: True\n",
      "encoder.layer.3.intermediate.dense.bias: True\n",
      "encoder.layer.3.output.dense.weight: True\n",
      "encoder.layer.3.output.dense.bias: True\n",
      "encoder.layer.3.output.LayerNorm.weight: True\n",
      "encoder.layer.3.output.LayerNorm.bias: True\n",
      "encoder.layer.4.attention.attn.q.weight: True\n",
      "encoder.layer.4.attention.attn.q.bias: True\n",
      "encoder.layer.4.attention.attn.k.weight: True\n",
      "encoder.layer.4.attention.attn.k.bias: True\n",
      "encoder.layer.4.attention.attn.v.weight: True\n",
      "encoder.layer.4.attention.attn.v.bias: True\n",
      "encoder.layer.4.attention.attn.o.weight: True\n",
      "encoder.layer.4.attention.attn.o.bias: True\n",
      "encoder.layer.4.attention.LayerNorm.weight: True\n",
      "encoder.layer.4.attention.LayerNorm.bias: True\n",
      "encoder.layer.4.intermediate.dense.weight: True\n",
      "encoder.layer.4.intermediate.dense.bias: True\n",
      "encoder.layer.4.output.dense.weight: True\n",
      "encoder.layer.4.output.dense.bias: True\n",
      "encoder.layer.4.output.LayerNorm.weight: True\n",
      "encoder.layer.4.output.LayerNorm.bias: True\n",
      "encoder.layer.5.attention.attn.q.weight: True\n",
      "encoder.layer.5.attention.attn.q.bias: True\n",
      "encoder.layer.5.attention.attn.k.weight: True\n",
      "encoder.layer.5.attention.attn.k.bias: True\n",
      "encoder.layer.5.attention.attn.v.weight: True\n",
      "encoder.layer.5.attention.attn.v.bias: True\n",
      "encoder.layer.5.attention.attn.o.weight: True\n",
      "encoder.layer.5.attention.attn.o.bias: True\n",
      "encoder.layer.5.attention.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.attn.q.weight: True\n",
      "encoder.layer.6.attention.attn.q.bias: True\n",
      "encoder.layer.6.attention.attn.k.weight: True\n",
      "encoder.layer.6.attention.attn.k.bias: True\n",
      "encoder.layer.6.attention.attn.v.weight: True\n",
      "encoder.layer.6.attention.attn.v.bias: True\n",
      "encoder.layer.6.attention.attn.o.weight: True\n",
      "encoder.layer.6.attention.attn.o.bias: True\n",
      "encoder.layer.6.attention.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.attn.q.weight: True\n",
      "encoder.layer.7.attention.attn.q.bias: True\n",
      "encoder.layer.7.attention.attn.k.weight: True\n",
      "encoder.layer.7.attention.attn.k.bias: True\n",
      "encoder.layer.7.attention.attn.v.weight: True\n",
      "encoder.layer.7.attention.attn.v.bias: True\n",
      "encoder.layer.7.attention.attn.o.weight: True\n",
      "encoder.layer.7.attention.attn.o.bias: True\n",
      "encoder.layer.7.attention.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.attn.q.weight: True\n",
      "encoder.layer.8.attention.attn.q.bias: True\n",
      "encoder.layer.8.attention.attn.k.weight: True\n",
      "encoder.layer.8.attention.attn.k.bias: True\n",
      "encoder.layer.8.attention.attn.v.weight: True\n",
      "encoder.layer.8.attention.attn.v.bias: True\n",
      "encoder.layer.8.attention.attn.o.weight: True\n",
      "encoder.layer.8.attention.attn.o.bias: True\n",
      "encoder.layer.8.attention.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.attn.q.weight: True\n",
      "encoder.layer.9.attention.attn.q.bias: True\n",
      "encoder.layer.9.attention.attn.k.weight: True\n",
      "encoder.layer.9.attention.attn.k.bias: True\n",
      "encoder.layer.9.attention.attn.v.weight: True\n",
      "encoder.layer.9.attention.attn.v.bias: True\n",
      "encoder.layer.9.attention.attn.o.weight: True\n",
      "encoder.layer.9.attention.attn.o.bias: True\n",
      "encoder.layer.9.attention.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.attn.q.weight: True\n",
      "encoder.layer.10.attention.attn.q.bias: True\n",
      "encoder.layer.10.attention.attn.k.weight: True\n",
      "encoder.layer.10.attention.attn.k.bias: True\n",
      "encoder.layer.10.attention.attn.v.weight: True\n",
      "encoder.layer.10.attention.attn.v.bias: True\n",
      "encoder.layer.10.attention.attn.o.weight: True\n",
      "encoder.layer.10.attention.attn.o.bias: True\n",
      "encoder.layer.10.attention.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.attn.q.weight: True\n",
      "encoder.layer.11.attention.attn.q.bias: True\n",
      "encoder.layer.11.attention.attn.k.weight: True\n",
      "encoder.layer.11.attention.attn.k.bias: True\n",
      "encoder.layer.11.attention.attn.v.weight: True\n",
      "encoder.layer.11.attention.attn.v.bias: True\n",
      "encoder.layer.11.attention.attn.o.weight: True\n",
      "encoder.layer.11.attention.attn.o.bias: True\n",
      "encoder.layer.11.attention.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "encoder.relative_attention_bias.weight: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n",
      "--------------------------------------\n",
      "SBERT\n",
      "embeddings.word_embeddings.weight: True\n",
      "embeddings.position_embeddings.weight: True\n",
      "embeddings.LayerNorm.weight: True\n",
      "embeddings.LayerNorm.bias: True\n",
      "encoder.layer.0.attention.attn.q.weight: True\n",
      "encoder.layer.0.attention.attn.q.bias: True\n",
      "encoder.layer.0.attention.attn.k.weight: True\n",
      "encoder.layer.0.attention.attn.k.bias: True\n",
      "encoder.layer.0.attention.attn.v.weight: True\n",
      "encoder.layer.0.attention.attn.v.bias: True\n",
      "encoder.layer.0.attention.attn.o.weight: True\n",
      "encoder.layer.0.attention.attn.o.bias: True\n",
      "encoder.layer.0.attention.LayerNorm.weight: True\n",
      "encoder.layer.0.attention.LayerNorm.bias: True\n",
      "encoder.layer.0.intermediate.dense.weight: True\n",
      "encoder.layer.0.intermediate.dense.bias: True\n",
      "encoder.layer.0.output.dense.weight: True\n",
      "encoder.layer.0.output.dense.bias: True\n",
      "encoder.layer.0.output.LayerNorm.weight: True\n",
      "encoder.layer.0.output.LayerNorm.bias: True\n",
      "encoder.layer.1.attention.attn.q.weight: True\n",
      "encoder.layer.1.attention.attn.q.bias: True\n",
      "encoder.layer.1.attention.attn.k.weight: True\n",
      "encoder.layer.1.attention.attn.k.bias: True\n",
      "encoder.layer.1.attention.attn.v.weight: True\n",
      "encoder.layer.1.attention.attn.v.bias: True\n",
      "encoder.layer.1.attention.attn.o.weight: True\n",
      "encoder.layer.1.attention.attn.o.bias: True\n",
      "encoder.layer.1.attention.LayerNorm.weight: True\n",
      "encoder.layer.1.attention.LayerNorm.bias: True\n",
      "encoder.layer.1.intermediate.dense.weight: True\n",
      "encoder.layer.1.intermediate.dense.bias: True\n",
      "encoder.layer.1.output.dense.weight: True\n",
      "encoder.layer.1.output.dense.bias: True\n",
      "encoder.layer.1.output.LayerNorm.weight: True\n",
      "encoder.layer.1.output.LayerNorm.bias: True\n",
      "encoder.layer.2.attention.attn.q.weight: True\n",
      "encoder.layer.2.attention.attn.q.bias: True\n",
      "encoder.layer.2.attention.attn.k.weight: True\n",
      "encoder.layer.2.attention.attn.k.bias: True\n",
      "encoder.layer.2.attention.attn.v.weight: True\n",
      "encoder.layer.2.attention.attn.v.bias: True\n",
      "encoder.layer.2.attention.attn.o.weight: True\n",
      "encoder.layer.2.attention.attn.o.bias: True\n",
      "encoder.layer.2.attention.LayerNorm.weight: True\n",
      "encoder.layer.2.attention.LayerNorm.bias: True\n",
      "encoder.layer.2.intermediate.dense.weight: True\n",
      "encoder.layer.2.intermediate.dense.bias: True\n",
      "encoder.layer.2.output.dense.weight: True\n",
      "encoder.layer.2.output.dense.bias: True\n",
      "encoder.layer.2.output.LayerNorm.weight: True\n",
      "encoder.layer.2.output.LayerNorm.bias: True\n",
      "encoder.layer.3.attention.attn.q.weight: True\n",
      "encoder.layer.3.attention.attn.q.bias: True\n",
      "encoder.layer.3.attention.attn.k.weight: True\n",
      "encoder.layer.3.attention.attn.k.bias: True\n",
      "encoder.layer.3.attention.attn.v.weight: True\n",
      "encoder.layer.3.attention.attn.v.bias: True\n",
      "encoder.layer.3.attention.attn.o.weight: True\n",
      "encoder.layer.3.attention.attn.o.bias: True\n",
      "encoder.layer.3.attention.LayerNorm.weight: True\n",
      "encoder.layer.3.attention.LayerNorm.bias: True\n",
      "encoder.layer.3.intermediate.dense.weight: True\n",
      "encoder.layer.3.intermediate.dense.bias: True\n",
      "encoder.layer.3.output.dense.weight: True\n",
      "encoder.layer.3.output.dense.bias: True\n",
      "encoder.layer.3.output.LayerNorm.weight: True\n",
      "encoder.layer.3.output.LayerNorm.bias: True\n",
      "encoder.layer.4.attention.attn.q.weight: True\n",
      "encoder.layer.4.attention.attn.q.bias: True\n",
      "encoder.layer.4.attention.attn.k.weight: True\n",
      "encoder.layer.4.attention.attn.k.bias: True\n",
      "encoder.layer.4.attention.attn.v.weight: True\n",
      "encoder.layer.4.attention.attn.v.bias: True\n",
      "encoder.layer.4.attention.attn.o.weight: True\n",
      "encoder.layer.4.attention.attn.o.bias: True\n",
      "encoder.layer.4.attention.LayerNorm.weight: True\n",
      "encoder.layer.4.attention.LayerNorm.bias: True\n",
      "encoder.layer.4.intermediate.dense.weight: True\n",
      "encoder.layer.4.intermediate.dense.bias: True\n",
      "encoder.layer.4.output.dense.weight: True\n",
      "encoder.layer.4.output.dense.bias: True\n",
      "encoder.layer.4.output.LayerNorm.weight: True\n",
      "encoder.layer.4.output.LayerNorm.bias: True\n",
      "encoder.layer.5.attention.attn.q.weight: True\n",
      "encoder.layer.5.attention.attn.q.bias: True\n",
      "encoder.layer.5.attention.attn.k.weight: True\n",
      "encoder.layer.5.attention.attn.k.bias: True\n",
      "encoder.layer.5.attention.attn.v.weight: True\n",
      "encoder.layer.5.attention.attn.v.bias: True\n",
      "encoder.layer.5.attention.attn.o.weight: True\n",
      "encoder.layer.5.attention.attn.o.bias: True\n",
      "encoder.layer.5.attention.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.attn.q.weight: True\n",
      "encoder.layer.6.attention.attn.q.bias: True\n",
      "encoder.layer.6.attention.attn.k.weight: True\n",
      "encoder.layer.6.attention.attn.k.bias: True\n",
      "encoder.layer.6.attention.attn.v.weight: True\n",
      "encoder.layer.6.attention.attn.v.bias: True\n",
      "encoder.layer.6.attention.attn.o.weight: True\n",
      "encoder.layer.6.attention.attn.o.bias: True\n",
      "encoder.layer.6.attention.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.attn.q.weight: True\n",
      "encoder.layer.7.attention.attn.q.bias: True\n",
      "encoder.layer.7.attention.attn.k.weight: True\n",
      "encoder.layer.7.attention.attn.k.bias: True\n",
      "encoder.layer.7.attention.attn.v.weight: True\n",
      "encoder.layer.7.attention.attn.v.bias: True\n",
      "encoder.layer.7.attention.attn.o.weight: True\n",
      "encoder.layer.7.attention.attn.o.bias: True\n",
      "encoder.layer.7.attention.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.attn.q.weight: True\n",
      "encoder.layer.8.attention.attn.q.bias: True\n",
      "encoder.layer.8.attention.attn.k.weight: True\n",
      "encoder.layer.8.attention.attn.k.bias: True\n",
      "encoder.layer.8.attention.attn.v.weight: True\n",
      "encoder.layer.8.attention.attn.v.bias: True\n",
      "encoder.layer.8.attention.attn.o.weight: True\n",
      "encoder.layer.8.attention.attn.o.bias: True\n",
      "encoder.layer.8.attention.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.attn.q.weight: True\n",
      "encoder.layer.9.attention.attn.q.bias: True\n",
      "encoder.layer.9.attention.attn.k.weight: True\n",
      "encoder.layer.9.attention.attn.k.bias: True\n",
      "encoder.layer.9.attention.attn.v.weight: True\n",
      "encoder.layer.9.attention.attn.v.bias: True\n",
      "encoder.layer.9.attention.attn.o.weight: True\n",
      "encoder.layer.9.attention.attn.o.bias: True\n",
      "encoder.layer.9.attention.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.attn.q.weight: True\n",
      "encoder.layer.10.attention.attn.q.bias: True\n",
      "encoder.layer.10.attention.attn.k.weight: True\n",
      "encoder.layer.10.attention.attn.k.bias: True\n",
      "encoder.layer.10.attention.attn.v.weight: True\n",
      "encoder.layer.10.attention.attn.v.bias: True\n",
      "encoder.layer.10.attention.attn.o.weight: True\n",
      "encoder.layer.10.attention.attn.o.bias: True\n",
      "encoder.layer.10.attention.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.attn.q.weight: True\n",
      "encoder.layer.11.attention.attn.q.bias: True\n",
      "encoder.layer.11.attention.attn.k.weight: True\n",
      "encoder.layer.11.attention.attn.k.bias: True\n",
      "encoder.layer.11.attention.attn.v.weight: True\n",
      "encoder.layer.11.attention.attn.v.bias: True\n",
      "encoder.layer.11.attention.attn.o.weight: True\n",
      "encoder.layer.11.attention.attn.o.bias: True\n",
      "encoder.layer.11.attention.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "encoder.relative_attention_bias.weight: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n",
      "--------------------------------------\n",
      "SciBERT\n",
      "embeddings.word_embeddings.weight: True\n",
      "embeddings.position_embeddings.weight: True\n",
      "embeddings.token_type_embeddings.weight: True\n",
      "embeddings.LayerNorm.weight: True\n",
      "embeddings.LayerNorm.bias: True\n",
      "encoder.layer.0.attention.self.query.weight: True\n",
      "encoder.layer.0.attention.self.query.bias: True\n",
      "encoder.layer.0.attention.self.key.weight: True\n",
      "encoder.layer.0.attention.self.key.bias: True\n",
      "encoder.layer.0.attention.self.value.weight: True\n",
      "encoder.layer.0.attention.self.value.bias: True\n",
      "encoder.layer.0.attention.output.dense.weight: True\n",
      "encoder.layer.0.attention.output.dense.bias: True\n",
      "encoder.layer.0.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.0.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.0.intermediate.dense.weight: True\n",
      "encoder.layer.0.intermediate.dense.bias: True\n",
      "encoder.layer.0.output.dense.weight: True\n",
      "encoder.layer.0.output.dense.bias: True\n",
      "encoder.layer.0.output.LayerNorm.weight: True\n",
      "encoder.layer.0.output.LayerNorm.bias: True\n",
      "encoder.layer.1.attention.self.query.weight: True\n",
      "encoder.layer.1.attention.self.query.bias: True\n",
      "encoder.layer.1.attention.self.key.weight: True\n",
      "encoder.layer.1.attention.self.key.bias: True\n",
      "encoder.layer.1.attention.self.value.weight: True\n",
      "encoder.layer.1.attention.self.value.bias: True\n",
      "encoder.layer.1.attention.output.dense.weight: True\n",
      "encoder.layer.1.attention.output.dense.bias: True\n",
      "encoder.layer.1.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.1.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.1.intermediate.dense.weight: True\n",
      "encoder.layer.1.intermediate.dense.bias: True\n",
      "encoder.layer.1.output.dense.weight: True\n",
      "encoder.layer.1.output.dense.bias: True\n",
      "encoder.layer.1.output.LayerNorm.weight: True\n",
      "encoder.layer.1.output.LayerNorm.bias: True\n",
      "encoder.layer.2.attention.self.query.weight: True\n",
      "encoder.layer.2.attention.self.query.bias: True\n",
      "encoder.layer.2.attention.self.key.weight: True\n",
      "encoder.layer.2.attention.self.key.bias: True\n",
      "encoder.layer.2.attention.self.value.weight: True\n",
      "encoder.layer.2.attention.self.value.bias: True\n",
      "encoder.layer.2.attention.output.dense.weight: True\n",
      "encoder.layer.2.attention.output.dense.bias: True\n",
      "encoder.layer.2.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.2.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.2.intermediate.dense.weight: True\n",
      "encoder.layer.2.intermediate.dense.bias: True\n",
      "encoder.layer.2.output.dense.weight: True\n",
      "encoder.layer.2.output.dense.bias: True\n",
      "encoder.layer.2.output.LayerNorm.weight: True\n",
      "encoder.layer.2.output.LayerNorm.bias: True\n",
      "encoder.layer.3.attention.self.query.weight: True\n",
      "encoder.layer.3.attention.self.query.bias: True\n",
      "encoder.layer.3.attention.self.key.weight: True\n",
      "encoder.layer.3.attention.self.key.bias: True\n",
      "encoder.layer.3.attention.self.value.weight: True\n",
      "encoder.layer.3.attention.self.value.bias: True\n",
      "encoder.layer.3.attention.output.dense.weight: True\n",
      "encoder.layer.3.attention.output.dense.bias: True\n",
      "encoder.layer.3.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.3.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.3.intermediate.dense.weight: True\n",
      "encoder.layer.3.intermediate.dense.bias: True\n",
      "encoder.layer.3.output.dense.weight: True\n",
      "encoder.layer.3.output.dense.bias: True\n",
      "encoder.layer.3.output.LayerNorm.weight: True\n",
      "encoder.layer.3.output.LayerNorm.bias: True\n",
      "encoder.layer.4.attention.self.query.weight: True\n",
      "encoder.layer.4.attention.self.query.bias: True\n",
      "encoder.layer.4.attention.self.key.weight: True\n",
      "encoder.layer.4.attention.self.key.bias: True\n",
      "encoder.layer.4.attention.self.value.weight: True\n",
      "encoder.layer.4.attention.self.value.bias: True\n",
      "encoder.layer.4.attention.output.dense.weight: True\n",
      "encoder.layer.4.attention.output.dense.bias: True\n",
      "encoder.layer.4.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.4.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.4.intermediate.dense.weight: True\n",
      "encoder.layer.4.intermediate.dense.bias: True\n",
      "encoder.layer.4.output.dense.weight: True\n",
      "encoder.layer.4.output.dense.bias: True\n",
      "encoder.layer.4.output.LayerNorm.weight: True\n",
      "encoder.layer.4.output.LayerNorm.bias: True\n",
      "encoder.layer.5.attention.self.query.weight: True\n",
      "encoder.layer.5.attention.self.query.bias: True\n",
      "encoder.layer.5.attention.self.key.weight: True\n",
      "encoder.layer.5.attention.self.key.bias: True\n",
      "encoder.layer.5.attention.self.value.weight: True\n",
      "encoder.layer.5.attention.self.value.bias: True\n",
      "encoder.layer.5.attention.output.dense.weight: True\n",
      "encoder.layer.5.attention.output.dense.bias: True\n",
      "encoder.layer.5.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.self.query.weight: True\n",
      "encoder.layer.6.attention.self.query.bias: True\n",
      "encoder.layer.6.attention.self.key.weight: True\n",
      "encoder.layer.6.attention.self.key.bias: True\n",
      "encoder.layer.6.attention.self.value.weight: True\n",
      "encoder.layer.6.attention.self.value.bias: True\n",
      "encoder.layer.6.attention.output.dense.weight: True\n",
      "encoder.layer.6.attention.output.dense.bias: True\n",
      "encoder.layer.6.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.self.query.weight: True\n",
      "encoder.layer.7.attention.self.query.bias: True\n",
      "encoder.layer.7.attention.self.key.weight: True\n",
      "encoder.layer.7.attention.self.key.bias: True\n",
      "encoder.layer.7.attention.self.value.weight: True\n",
      "encoder.layer.7.attention.self.value.bias: True\n",
      "encoder.layer.7.attention.output.dense.weight: True\n",
      "encoder.layer.7.attention.output.dense.bias: True\n",
      "encoder.layer.7.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.self.query.weight: True\n",
      "encoder.layer.8.attention.self.query.bias: True\n",
      "encoder.layer.8.attention.self.key.weight: True\n",
      "encoder.layer.8.attention.self.key.bias: True\n",
      "encoder.layer.8.attention.self.value.weight: True\n",
      "encoder.layer.8.attention.self.value.bias: True\n",
      "encoder.layer.8.attention.output.dense.weight: True\n",
      "encoder.layer.8.attention.output.dense.bias: True\n",
      "encoder.layer.8.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.self.query.weight: True\n",
      "encoder.layer.9.attention.self.query.bias: True\n",
      "encoder.layer.9.attention.self.key.weight: True\n",
      "encoder.layer.9.attention.self.key.bias: True\n",
      "encoder.layer.9.attention.self.value.weight: True\n",
      "encoder.layer.9.attention.self.value.bias: True\n",
      "encoder.layer.9.attention.output.dense.weight: True\n",
      "encoder.layer.9.attention.output.dense.bias: True\n",
      "encoder.layer.9.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.self.query.weight: True\n",
      "encoder.layer.10.attention.self.query.bias: True\n",
      "encoder.layer.10.attention.self.key.weight: True\n",
      "encoder.layer.10.attention.self.key.bias: True\n",
      "encoder.layer.10.attention.self.value.weight: True\n",
      "encoder.layer.10.attention.self.value.bias: True\n",
      "encoder.layer.10.attention.output.dense.weight: True\n",
      "encoder.layer.10.attention.output.dense.bias: True\n",
      "encoder.layer.10.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.self.query.weight: True\n",
      "encoder.layer.11.attention.self.query.bias: True\n",
      "encoder.layer.11.attention.self.key.weight: True\n",
      "encoder.layer.11.attention.self.key.bias: True\n",
      "encoder.layer.11.attention.self.value.weight: True\n",
      "encoder.layer.11.attention.self.value.bias: True\n",
      "encoder.layer.11.attention.output.dense.weight: True\n",
      "encoder.layer.11.attention.output.dense.bias: True\n",
      "encoder.layer.11.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n",
      "--------------------------------------\n",
      "SPECTER\n",
      "embeddings.word_embeddings.weight: True\n",
      "embeddings.position_embeddings.weight: True\n",
      "embeddings.token_type_embeddings.weight: True\n",
      "embeddings.LayerNorm.weight: True\n",
      "embeddings.LayerNorm.bias: True\n",
      "encoder.layer.0.attention.self.query.weight: True\n",
      "encoder.layer.0.attention.self.query.bias: True\n",
      "encoder.layer.0.attention.self.key.weight: True\n",
      "encoder.layer.0.attention.self.key.bias: True\n",
      "encoder.layer.0.attention.self.value.weight: True\n",
      "encoder.layer.0.attention.self.value.bias: True\n",
      "encoder.layer.0.attention.output.dense.weight: True\n",
      "encoder.layer.0.attention.output.dense.bias: True\n",
      "encoder.layer.0.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.0.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.0.intermediate.dense.weight: True\n",
      "encoder.layer.0.intermediate.dense.bias: True\n",
      "encoder.layer.0.output.dense.weight: True\n",
      "encoder.layer.0.output.dense.bias: True\n",
      "encoder.layer.0.output.LayerNorm.weight: True\n",
      "encoder.layer.0.output.LayerNorm.bias: True\n",
      "encoder.layer.1.attention.self.query.weight: True\n",
      "encoder.layer.1.attention.self.query.bias: True\n",
      "encoder.layer.1.attention.self.key.weight: True\n",
      "encoder.layer.1.attention.self.key.bias: True\n",
      "encoder.layer.1.attention.self.value.weight: True\n",
      "encoder.layer.1.attention.self.value.bias: True\n",
      "encoder.layer.1.attention.output.dense.weight: True\n",
      "encoder.layer.1.attention.output.dense.bias: True\n",
      "encoder.layer.1.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.1.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.1.intermediate.dense.weight: True\n",
      "encoder.layer.1.intermediate.dense.bias: True\n",
      "encoder.layer.1.output.dense.weight: True\n",
      "encoder.layer.1.output.dense.bias: True\n",
      "encoder.layer.1.output.LayerNorm.weight: True\n",
      "encoder.layer.1.output.LayerNorm.bias: True\n",
      "encoder.layer.2.attention.self.query.weight: True\n",
      "encoder.layer.2.attention.self.query.bias: True\n",
      "encoder.layer.2.attention.self.key.weight: True\n",
      "encoder.layer.2.attention.self.key.bias: True\n",
      "encoder.layer.2.attention.self.value.weight: True\n",
      "encoder.layer.2.attention.self.value.bias: True\n",
      "encoder.layer.2.attention.output.dense.weight: True\n",
      "encoder.layer.2.attention.output.dense.bias: True\n",
      "encoder.layer.2.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.2.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.2.intermediate.dense.weight: True\n",
      "encoder.layer.2.intermediate.dense.bias: True\n",
      "encoder.layer.2.output.dense.weight: True\n",
      "encoder.layer.2.output.dense.bias: True\n",
      "encoder.layer.2.output.LayerNorm.weight: True\n",
      "encoder.layer.2.output.LayerNorm.bias: True\n",
      "encoder.layer.3.attention.self.query.weight: True\n",
      "encoder.layer.3.attention.self.query.bias: True\n",
      "encoder.layer.3.attention.self.key.weight: True\n",
      "encoder.layer.3.attention.self.key.bias: True\n",
      "encoder.layer.3.attention.self.value.weight: True\n",
      "encoder.layer.3.attention.self.value.bias: True\n",
      "encoder.layer.3.attention.output.dense.weight: True\n",
      "encoder.layer.3.attention.output.dense.bias: True\n",
      "encoder.layer.3.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.3.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.3.intermediate.dense.weight: True\n",
      "encoder.layer.3.intermediate.dense.bias: True\n",
      "encoder.layer.3.output.dense.weight: True\n",
      "encoder.layer.3.output.dense.bias: True\n",
      "encoder.layer.3.output.LayerNorm.weight: True\n",
      "encoder.layer.3.output.LayerNorm.bias: True\n",
      "encoder.layer.4.attention.self.query.weight: True\n",
      "encoder.layer.4.attention.self.query.bias: True\n",
      "encoder.layer.4.attention.self.key.weight: True\n",
      "encoder.layer.4.attention.self.key.bias: True\n",
      "encoder.layer.4.attention.self.value.weight: True\n",
      "encoder.layer.4.attention.self.value.bias: True\n",
      "encoder.layer.4.attention.output.dense.weight: True\n",
      "encoder.layer.4.attention.output.dense.bias: True\n",
      "encoder.layer.4.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.4.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.4.intermediate.dense.weight: True\n",
      "encoder.layer.4.intermediate.dense.bias: True\n",
      "encoder.layer.4.output.dense.weight: True\n",
      "encoder.layer.4.output.dense.bias: True\n",
      "encoder.layer.4.output.LayerNorm.weight: True\n",
      "encoder.layer.4.output.LayerNorm.bias: True\n",
      "encoder.layer.5.attention.self.query.weight: True\n",
      "encoder.layer.5.attention.self.query.bias: True\n",
      "encoder.layer.5.attention.self.key.weight: True\n",
      "encoder.layer.5.attention.self.key.bias: True\n",
      "encoder.layer.5.attention.self.value.weight: True\n",
      "encoder.layer.5.attention.self.value.bias: True\n",
      "encoder.layer.5.attention.output.dense.weight: True\n",
      "encoder.layer.5.attention.output.dense.bias: True\n",
      "encoder.layer.5.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.self.query.weight: True\n",
      "encoder.layer.6.attention.self.query.bias: True\n",
      "encoder.layer.6.attention.self.key.weight: True\n",
      "encoder.layer.6.attention.self.key.bias: True\n",
      "encoder.layer.6.attention.self.value.weight: True\n",
      "encoder.layer.6.attention.self.value.bias: True\n",
      "encoder.layer.6.attention.output.dense.weight: True\n",
      "encoder.layer.6.attention.output.dense.bias: True\n",
      "encoder.layer.6.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.self.query.weight: True\n",
      "encoder.layer.7.attention.self.query.bias: True\n",
      "encoder.layer.7.attention.self.key.weight: True\n",
      "encoder.layer.7.attention.self.key.bias: True\n",
      "encoder.layer.7.attention.self.value.weight: True\n",
      "encoder.layer.7.attention.self.value.bias: True\n",
      "encoder.layer.7.attention.output.dense.weight: True\n",
      "encoder.layer.7.attention.output.dense.bias: True\n",
      "encoder.layer.7.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.self.query.weight: True\n",
      "encoder.layer.8.attention.self.query.bias: True\n",
      "encoder.layer.8.attention.self.key.weight: True\n",
      "encoder.layer.8.attention.self.key.bias: True\n",
      "encoder.layer.8.attention.self.value.weight: True\n",
      "encoder.layer.8.attention.self.value.bias: True\n",
      "encoder.layer.8.attention.output.dense.weight: True\n",
      "encoder.layer.8.attention.output.dense.bias: True\n",
      "encoder.layer.8.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.self.query.weight: True\n",
      "encoder.layer.9.attention.self.query.bias: True\n",
      "encoder.layer.9.attention.self.key.weight: True\n",
      "encoder.layer.9.attention.self.key.bias: True\n",
      "encoder.layer.9.attention.self.value.weight: True\n",
      "encoder.layer.9.attention.self.value.bias: True\n",
      "encoder.layer.9.attention.output.dense.weight: True\n",
      "encoder.layer.9.attention.output.dense.bias: True\n",
      "encoder.layer.9.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.self.query.weight: True\n",
      "encoder.layer.10.attention.self.query.bias: True\n",
      "encoder.layer.10.attention.self.key.weight: True\n",
      "encoder.layer.10.attention.self.key.bias: True\n",
      "encoder.layer.10.attention.self.value.weight: True\n",
      "encoder.layer.10.attention.self.value.bias: True\n",
      "encoder.layer.10.attention.output.dense.weight: True\n",
      "encoder.layer.10.attention.output.dense.bias: True\n",
      "encoder.layer.10.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.self.query.weight: True\n",
      "encoder.layer.11.attention.self.query.bias: True\n",
      "encoder.layer.11.attention.self.key.weight: True\n",
      "encoder.layer.11.attention.self.key.bias: True\n",
      "encoder.layer.11.attention.self.value.weight: True\n",
      "encoder.layer.11.attention.self.value.bias: True\n",
      "encoder.layer.11.attention.output.dense.weight: True\n",
      "encoder.layer.11.attention.output.dense.bias: True\n",
      "encoder.layer.11.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n",
      "--------------------------------------\n",
      "SciNCL\n",
      "embeddings.word_embeddings.weight: True\n",
      "embeddings.position_embeddings.weight: True\n",
      "embeddings.token_type_embeddings.weight: True\n",
      "embeddings.LayerNorm.weight: True\n",
      "embeddings.LayerNorm.bias: True\n",
      "encoder.layer.0.attention.self.query.weight: True\n",
      "encoder.layer.0.attention.self.query.bias: True\n",
      "encoder.layer.0.attention.self.key.weight: True\n",
      "encoder.layer.0.attention.self.key.bias: True\n",
      "encoder.layer.0.attention.self.value.weight: True\n",
      "encoder.layer.0.attention.self.value.bias: True\n",
      "encoder.layer.0.attention.output.dense.weight: True\n",
      "encoder.layer.0.attention.output.dense.bias: True\n",
      "encoder.layer.0.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.0.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.0.intermediate.dense.weight: True\n",
      "encoder.layer.0.intermediate.dense.bias: True\n",
      "encoder.layer.0.output.dense.weight: True\n",
      "encoder.layer.0.output.dense.bias: True\n",
      "encoder.layer.0.output.LayerNorm.weight: True\n",
      "encoder.layer.0.output.LayerNorm.bias: True\n",
      "encoder.layer.1.attention.self.query.weight: True\n",
      "encoder.layer.1.attention.self.query.bias: True\n",
      "encoder.layer.1.attention.self.key.weight: True\n",
      "encoder.layer.1.attention.self.key.bias: True\n",
      "encoder.layer.1.attention.self.value.weight: True\n",
      "encoder.layer.1.attention.self.value.bias: True\n",
      "encoder.layer.1.attention.output.dense.weight: True\n",
      "encoder.layer.1.attention.output.dense.bias: True\n",
      "encoder.layer.1.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.1.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.1.intermediate.dense.weight: True\n",
      "encoder.layer.1.intermediate.dense.bias: True\n",
      "encoder.layer.1.output.dense.weight: True\n",
      "encoder.layer.1.output.dense.bias: True\n",
      "encoder.layer.1.output.LayerNorm.weight: True\n",
      "encoder.layer.1.output.LayerNorm.bias: True\n",
      "encoder.layer.2.attention.self.query.weight: True\n",
      "encoder.layer.2.attention.self.query.bias: True\n",
      "encoder.layer.2.attention.self.key.weight: True\n",
      "encoder.layer.2.attention.self.key.bias: True\n",
      "encoder.layer.2.attention.self.value.weight: True\n",
      "encoder.layer.2.attention.self.value.bias: True\n",
      "encoder.layer.2.attention.output.dense.weight: True\n",
      "encoder.layer.2.attention.output.dense.bias: True\n",
      "encoder.layer.2.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.2.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.2.intermediate.dense.weight: True\n",
      "encoder.layer.2.intermediate.dense.bias: True\n",
      "encoder.layer.2.output.dense.weight: True\n",
      "encoder.layer.2.output.dense.bias: True\n",
      "encoder.layer.2.output.LayerNorm.weight: True\n",
      "encoder.layer.2.output.LayerNorm.bias: True\n",
      "encoder.layer.3.attention.self.query.weight: True\n",
      "encoder.layer.3.attention.self.query.bias: True\n",
      "encoder.layer.3.attention.self.key.weight: True\n",
      "encoder.layer.3.attention.self.key.bias: True\n",
      "encoder.layer.3.attention.self.value.weight: True\n",
      "encoder.layer.3.attention.self.value.bias: True\n",
      "encoder.layer.3.attention.output.dense.weight: True\n",
      "encoder.layer.3.attention.output.dense.bias: True\n",
      "encoder.layer.3.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.3.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.3.intermediate.dense.weight: True\n",
      "encoder.layer.3.intermediate.dense.bias: True\n",
      "encoder.layer.3.output.dense.weight: True\n",
      "encoder.layer.3.output.dense.bias: True\n",
      "encoder.layer.3.output.LayerNorm.weight: True\n",
      "encoder.layer.3.output.LayerNorm.bias: True\n",
      "encoder.layer.4.attention.self.query.weight: True\n",
      "encoder.layer.4.attention.self.query.bias: True\n",
      "encoder.layer.4.attention.self.key.weight: True\n",
      "encoder.layer.4.attention.self.key.bias: True\n",
      "encoder.layer.4.attention.self.value.weight: True\n",
      "encoder.layer.4.attention.self.value.bias: True\n",
      "encoder.layer.4.attention.output.dense.weight: True\n",
      "encoder.layer.4.attention.output.dense.bias: True\n",
      "encoder.layer.4.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.4.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.4.intermediate.dense.weight: True\n",
      "encoder.layer.4.intermediate.dense.bias: True\n",
      "encoder.layer.4.output.dense.weight: True\n",
      "encoder.layer.4.output.dense.bias: True\n",
      "encoder.layer.4.output.LayerNorm.weight: True\n",
      "encoder.layer.4.output.LayerNorm.bias: True\n",
      "encoder.layer.5.attention.self.query.weight: True\n",
      "encoder.layer.5.attention.self.query.bias: True\n",
      "encoder.layer.5.attention.self.key.weight: True\n",
      "encoder.layer.5.attention.self.key.bias: True\n",
      "encoder.layer.5.attention.self.value.weight: True\n",
      "encoder.layer.5.attention.self.value.bias: True\n",
      "encoder.layer.5.attention.output.dense.weight: True\n",
      "encoder.layer.5.attention.output.dense.bias: True\n",
      "encoder.layer.5.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.self.query.weight: True\n",
      "encoder.layer.6.attention.self.query.bias: True\n",
      "encoder.layer.6.attention.self.key.weight: True\n",
      "encoder.layer.6.attention.self.key.bias: True\n",
      "encoder.layer.6.attention.self.value.weight: True\n",
      "encoder.layer.6.attention.self.value.bias: True\n",
      "encoder.layer.6.attention.output.dense.weight: True\n",
      "encoder.layer.6.attention.output.dense.bias: True\n",
      "encoder.layer.6.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.self.query.weight: True\n",
      "encoder.layer.7.attention.self.query.bias: True\n",
      "encoder.layer.7.attention.self.key.weight: True\n",
      "encoder.layer.7.attention.self.key.bias: True\n",
      "encoder.layer.7.attention.self.value.weight: True\n",
      "encoder.layer.7.attention.self.value.bias: True\n",
      "encoder.layer.7.attention.output.dense.weight: True\n",
      "encoder.layer.7.attention.output.dense.bias: True\n",
      "encoder.layer.7.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.self.query.weight: True\n",
      "encoder.layer.8.attention.self.query.bias: True\n",
      "encoder.layer.8.attention.self.key.weight: True\n",
      "encoder.layer.8.attention.self.key.bias: True\n",
      "encoder.layer.8.attention.self.value.weight: True\n",
      "encoder.layer.8.attention.self.value.bias: True\n",
      "encoder.layer.8.attention.output.dense.weight: True\n",
      "encoder.layer.8.attention.output.dense.bias: True\n",
      "encoder.layer.8.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.self.query.weight: True\n",
      "encoder.layer.9.attention.self.query.bias: True\n",
      "encoder.layer.9.attention.self.key.weight: True\n",
      "encoder.layer.9.attention.self.key.bias: True\n",
      "encoder.layer.9.attention.self.value.weight: True\n",
      "encoder.layer.9.attention.self.value.bias: True\n",
      "encoder.layer.9.attention.output.dense.weight: True\n",
      "encoder.layer.9.attention.output.dense.bias: True\n",
      "encoder.layer.9.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.self.query.weight: True\n",
      "encoder.layer.10.attention.self.query.bias: True\n",
      "encoder.layer.10.attention.self.key.weight: True\n",
      "encoder.layer.10.attention.self.key.bias: True\n",
      "encoder.layer.10.attention.self.value.weight: True\n",
      "encoder.layer.10.attention.self.value.bias: True\n",
      "encoder.layer.10.attention.output.dense.weight: True\n",
      "encoder.layer.10.attention.output.dense.bias: True\n",
      "encoder.layer.10.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.self.query.weight: True\n",
      "encoder.layer.11.attention.self.query.bias: True\n",
      "encoder.layer.11.attention.self.key.weight: True\n",
      "encoder.layer.11.attention.self.key.bias: True\n",
      "encoder.layer.11.attention.self.value.weight: True\n",
      "encoder.layer.11.attention.self.value.bias: True\n",
      "encoder.layer.11.attention.output.dense.weight: True\n",
      "encoder.layer.11.attention.output.dense.bias: True\n",
      "encoder.layer.11.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n",
      "--------------------------------------\n",
      "SimCSE\n",
      "embeddings.word_embeddings.weight: True\n",
      "embeddings.position_embeddings.weight: True\n",
      "embeddings.token_type_embeddings.weight: True\n",
      "embeddings.LayerNorm.weight: True\n",
      "embeddings.LayerNorm.bias: True\n",
      "encoder.layer.0.attention.self.query.weight: True\n",
      "encoder.layer.0.attention.self.query.bias: True\n",
      "encoder.layer.0.attention.self.key.weight: True\n",
      "encoder.layer.0.attention.self.key.bias: True\n",
      "encoder.layer.0.attention.self.value.weight: True\n",
      "encoder.layer.0.attention.self.value.bias: True\n",
      "encoder.layer.0.attention.output.dense.weight: True\n",
      "encoder.layer.0.attention.output.dense.bias: True\n",
      "encoder.layer.0.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.0.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.0.intermediate.dense.weight: True\n",
      "encoder.layer.0.intermediate.dense.bias: True\n",
      "encoder.layer.0.output.dense.weight: True\n",
      "encoder.layer.0.output.dense.bias: True\n",
      "encoder.layer.0.output.LayerNorm.weight: True\n",
      "encoder.layer.0.output.LayerNorm.bias: True\n",
      "encoder.layer.1.attention.self.query.weight: True\n",
      "encoder.layer.1.attention.self.query.bias: True\n",
      "encoder.layer.1.attention.self.key.weight: True\n",
      "encoder.layer.1.attention.self.key.bias: True\n",
      "encoder.layer.1.attention.self.value.weight: True\n",
      "encoder.layer.1.attention.self.value.bias: True\n",
      "encoder.layer.1.attention.output.dense.weight: True\n",
      "encoder.layer.1.attention.output.dense.bias: True\n",
      "encoder.layer.1.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.1.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.1.intermediate.dense.weight: True\n",
      "encoder.layer.1.intermediate.dense.bias: True\n",
      "encoder.layer.1.output.dense.weight: True\n",
      "encoder.layer.1.output.dense.bias: True\n",
      "encoder.layer.1.output.LayerNorm.weight: True\n",
      "encoder.layer.1.output.LayerNorm.bias: True\n",
      "encoder.layer.2.attention.self.query.weight: True\n",
      "encoder.layer.2.attention.self.query.bias: True\n",
      "encoder.layer.2.attention.self.key.weight: True\n",
      "encoder.layer.2.attention.self.key.bias: True\n",
      "encoder.layer.2.attention.self.value.weight: True\n",
      "encoder.layer.2.attention.self.value.bias: True\n",
      "encoder.layer.2.attention.output.dense.weight: True\n",
      "encoder.layer.2.attention.output.dense.bias: True\n",
      "encoder.layer.2.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.2.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.2.intermediate.dense.weight: True\n",
      "encoder.layer.2.intermediate.dense.bias: True\n",
      "encoder.layer.2.output.dense.weight: True\n",
      "encoder.layer.2.output.dense.bias: True\n",
      "encoder.layer.2.output.LayerNorm.weight: True\n",
      "encoder.layer.2.output.LayerNorm.bias: True\n",
      "encoder.layer.3.attention.self.query.weight: True\n",
      "encoder.layer.3.attention.self.query.bias: True\n",
      "encoder.layer.3.attention.self.key.weight: True\n",
      "encoder.layer.3.attention.self.key.bias: True\n",
      "encoder.layer.3.attention.self.value.weight: True\n",
      "encoder.layer.3.attention.self.value.bias: True\n",
      "encoder.layer.3.attention.output.dense.weight: True\n",
      "encoder.layer.3.attention.output.dense.bias: True\n",
      "encoder.layer.3.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.3.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.3.intermediate.dense.weight: True\n",
      "encoder.layer.3.intermediate.dense.bias: True\n",
      "encoder.layer.3.output.dense.weight: True\n",
      "encoder.layer.3.output.dense.bias: True\n",
      "encoder.layer.3.output.LayerNorm.weight: True\n",
      "encoder.layer.3.output.LayerNorm.bias: True\n",
      "encoder.layer.4.attention.self.query.weight: True\n",
      "encoder.layer.4.attention.self.query.bias: True\n",
      "encoder.layer.4.attention.self.key.weight: True\n",
      "encoder.layer.4.attention.self.key.bias: True\n",
      "encoder.layer.4.attention.self.value.weight: True\n",
      "encoder.layer.4.attention.self.value.bias: True\n",
      "encoder.layer.4.attention.output.dense.weight: True\n",
      "encoder.layer.4.attention.output.dense.bias: True\n",
      "encoder.layer.4.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.4.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.4.intermediate.dense.weight: True\n",
      "encoder.layer.4.intermediate.dense.bias: True\n",
      "encoder.layer.4.output.dense.weight: True\n",
      "encoder.layer.4.output.dense.bias: True\n",
      "encoder.layer.4.output.LayerNorm.weight: True\n",
      "encoder.layer.4.output.LayerNorm.bias: True\n",
      "encoder.layer.5.attention.self.query.weight: True\n",
      "encoder.layer.5.attention.self.query.bias: True\n",
      "encoder.layer.5.attention.self.key.weight: True\n",
      "encoder.layer.5.attention.self.key.bias: True\n",
      "encoder.layer.5.attention.self.value.weight: True\n",
      "encoder.layer.5.attention.self.value.bias: True\n",
      "encoder.layer.5.attention.output.dense.weight: True\n",
      "encoder.layer.5.attention.output.dense.bias: True\n",
      "encoder.layer.5.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.self.query.weight: True\n",
      "encoder.layer.6.attention.self.query.bias: True\n",
      "encoder.layer.6.attention.self.key.weight: True\n",
      "encoder.layer.6.attention.self.key.bias: True\n",
      "encoder.layer.6.attention.self.value.weight: True\n",
      "encoder.layer.6.attention.self.value.bias: True\n",
      "encoder.layer.6.attention.output.dense.weight: True\n",
      "encoder.layer.6.attention.output.dense.bias: True\n",
      "encoder.layer.6.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.self.query.weight: True\n",
      "encoder.layer.7.attention.self.query.bias: True\n",
      "encoder.layer.7.attention.self.key.weight: True\n",
      "encoder.layer.7.attention.self.key.bias: True\n",
      "encoder.layer.7.attention.self.value.weight: True\n",
      "encoder.layer.7.attention.self.value.bias: True\n",
      "encoder.layer.7.attention.output.dense.weight: True\n",
      "encoder.layer.7.attention.output.dense.bias: True\n",
      "encoder.layer.7.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.self.query.weight: True\n",
      "encoder.layer.8.attention.self.query.bias: True\n",
      "encoder.layer.8.attention.self.key.weight: True\n",
      "encoder.layer.8.attention.self.key.bias: True\n",
      "encoder.layer.8.attention.self.value.weight: True\n",
      "encoder.layer.8.attention.self.value.bias: True\n",
      "encoder.layer.8.attention.output.dense.weight: True\n",
      "encoder.layer.8.attention.output.dense.bias: True\n",
      "encoder.layer.8.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.self.query.weight: True\n",
      "encoder.layer.9.attention.self.query.bias: True\n",
      "encoder.layer.9.attention.self.key.weight: True\n",
      "encoder.layer.9.attention.self.key.bias: True\n",
      "encoder.layer.9.attention.self.value.weight: True\n",
      "encoder.layer.9.attention.self.value.bias: True\n",
      "encoder.layer.9.attention.output.dense.weight: True\n",
      "encoder.layer.9.attention.output.dense.bias: True\n",
      "encoder.layer.9.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.self.query.weight: True\n",
      "encoder.layer.10.attention.self.query.bias: True\n",
      "encoder.layer.10.attention.self.key.weight: True\n",
      "encoder.layer.10.attention.self.key.bias: True\n",
      "encoder.layer.10.attention.self.value.weight: True\n",
      "encoder.layer.10.attention.self.value.bias: True\n",
      "encoder.layer.10.attention.output.dense.weight: True\n",
      "encoder.layer.10.attention.output.dense.bias: True\n",
      "encoder.layer.10.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.self.query.weight: True\n",
      "encoder.layer.11.attention.self.query.bias: True\n",
      "encoder.layer.11.attention.self.key.weight: True\n",
      "encoder.layer.11.attention.self.key.bias: True\n",
      "encoder.layer.11.attention.self.value.weight: True\n",
      "encoder.layer.11.attention.self.value.bias: True\n",
      "encoder.layer.11.attention.output.dense.weight: True\n",
      "encoder.layer.11.attention.output.dense.bias: True\n",
      "encoder.layer.11.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n",
      "--------------------------------------\n",
      "DeCLUTR\n",
      "embeddings.word_embeddings.weight: True\n",
      "embeddings.position_embeddings.weight: True\n",
      "embeddings.token_type_embeddings.weight: True\n",
      "embeddings.LayerNorm.weight: True\n",
      "embeddings.LayerNorm.bias: True\n",
      "encoder.layer.0.attention.self.query.weight: True\n",
      "encoder.layer.0.attention.self.query.bias: True\n",
      "encoder.layer.0.attention.self.key.weight: True\n",
      "encoder.layer.0.attention.self.key.bias: True\n",
      "encoder.layer.0.attention.self.value.weight: True\n",
      "encoder.layer.0.attention.self.value.bias: True\n",
      "encoder.layer.0.attention.output.dense.weight: True\n",
      "encoder.layer.0.attention.output.dense.bias: True\n",
      "encoder.layer.0.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.0.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.0.intermediate.dense.weight: True\n",
      "encoder.layer.0.intermediate.dense.bias: True\n",
      "encoder.layer.0.output.dense.weight: True\n",
      "encoder.layer.0.output.dense.bias: True\n",
      "encoder.layer.0.output.LayerNorm.weight: True\n",
      "encoder.layer.0.output.LayerNorm.bias: True\n",
      "encoder.layer.1.attention.self.query.weight: True\n",
      "encoder.layer.1.attention.self.query.bias: True\n",
      "encoder.layer.1.attention.self.key.weight: True\n",
      "encoder.layer.1.attention.self.key.bias: True\n",
      "encoder.layer.1.attention.self.value.weight: True\n",
      "encoder.layer.1.attention.self.value.bias: True\n",
      "encoder.layer.1.attention.output.dense.weight: True\n",
      "encoder.layer.1.attention.output.dense.bias: True\n",
      "encoder.layer.1.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.1.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.1.intermediate.dense.weight: True\n",
      "encoder.layer.1.intermediate.dense.bias: True\n",
      "encoder.layer.1.output.dense.weight: True\n",
      "encoder.layer.1.output.dense.bias: True\n",
      "encoder.layer.1.output.LayerNorm.weight: True\n",
      "encoder.layer.1.output.LayerNorm.bias: True\n",
      "encoder.layer.2.attention.self.query.weight: True\n",
      "encoder.layer.2.attention.self.query.bias: True\n",
      "encoder.layer.2.attention.self.key.weight: True\n",
      "encoder.layer.2.attention.self.key.bias: True\n",
      "encoder.layer.2.attention.self.value.weight: True\n",
      "encoder.layer.2.attention.self.value.bias: True\n",
      "encoder.layer.2.attention.output.dense.weight: True\n",
      "encoder.layer.2.attention.output.dense.bias: True\n",
      "encoder.layer.2.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.2.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.2.intermediate.dense.weight: True\n",
      "encoder.layer.2.intermediate.dense.bias: True\n",
      "encoder.layer.2.output.dense.weight: True\n",
      "encoder.layer.2.output.dense.bias: True\n",
      "encoder.layer.2.output.LayerNorm.weight: True\n",
      "encoder.layer.2.output.LayerNorm.bias: True\n",
      "encoder.layer.3.attention.self.query.weight: True\n",
      "encoder.layer.3.attention.self.query.bias: True\n",
      "encoder.layer.3.attention.self.key.weight: True\n",
      "encoder.layer.3.attention.self.key.bias: True\n",
      "encoder.layer.3.attention.self.value.weight: True\n",
      "encoder.layer.3.attention.self.value.bias: True\n",
      "encoder.layer.3.attention.output.dense.weight: True\n",
      "encoder.layer.3.attention.output.dense.bias: True\n",
      "encoder.layer.3.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.3.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.3.intermediate.dense.weight: True\n",
      "encoder.layer.3.intermediate.dense.bias: True\n",
      "encoder.layer.3.output.dense.weight: True\n",
      "encoder.layer.3.output.dense.bias: True\n",
      "encoder.layer.3.output.LayerNorm.weight: True\n",
      "encoder.layer.3.output.LayerNorm.bias: True\n",
      "encoder.layer.4.attention.self.query.weight: True\n",
      "encoder.layer.4.attention.self.query.bias: True\n",
      "encoder.layer.4.attention.self.key.weight: True\n",
      "encoder.layer.4.attention.self.key.bias: True\n",
      "encoder.layer.4.attention.self.value.weight: True\n",
      "encoder.layer.4.attention.self.value.bias: True\n",
      "encoder.layer.4.attention.output.dense.weight: True\n",
      "encoder.layer.4.attention.output.dense.bias: True\n",
      "encoder.layer.4.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.4.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.4.intermediate.dense.weight: True\n",
      "encoder.layer.4.intermediate.dense.bias: True\n",
      "encoder.layer.4.output.dense.weight: True\n",
      "encoder.layer.4.output.dense.bias: True\n",
      "encoder.layer.4.output.LayerNorm.weight: True\n",
      "encoder.layer.4.output.LayerNorm.bias: True\n",
      "encoder.layer.5.attention.self.query.weight: True\n",
      "encoder.layer.5.attention.self.query.bias: True\n",
      "encoder.layer.5.attention.self.key.weight: True\n",
      "encoder.layer.5.attention.self.key.bias: True\n",
      "encoder.layer.5.attention.self.value.weight: True\n",
      "encoder.layer.5.attention.self.value.bias: True\n",
      "encoder.layer.5.attention.output.dense.weight: True\n",
      "encoder.layer.5.attention.output.dense.bias: True\n",
      "encoder.layer.5.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.self.query.weight: True\n",
      "encoder.layer.6.attention.self.query.bias: True\n",
      "encoder.layer.6.attention.self.key.weight: True\n",
      "encoder.layer.6.attention.self.key.bias: True\n",
      "encoder.layer.6.attention.self.value.weight: True\n",
      "encoder.layer.6.attention.self.value.bias: True\n",
      "encoder.layer.6.attention.output.dense.weight: True\n",
      "encoder.layer.6.attention.output.dense.bias: True\n",
      "encoder.layer.6.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.self.query.weight: True\n",
      "encoder.layer.7.attention.self.query.bias: True\n",
      "encoder.layer.7.attention.self.key.weight: True\n",
      "encoder.layer.7.attention.self.key.bias: True\n",
      "encoder.layer.7.attention.self.value.weight: True\n",
      "encoder.layer.7.attention.self.value.bias: True\n",
      "encoder.layer.7.attention.output.dense.weight: True\n",
      "encoder.layer.7.attention.output.dense.bias: True\n",
      "encoder.layer.7.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.self.query.weight: True\n",
      "encoder.layer.8.attention.self.query.bias: True\n",
      "encoder.layer.8.attention.self.key.weight: True\n",
      "encoder.layer.8.attention.self.key.bias: True\n",
      "encoder.layer.8.attention.self.value.weight: True\n",
      "encoder.layer.8.attention.self.value.bias: True\n",
      "encoder.layer.8.attention.output.dense.weight: True\n",
      "encoder.layer.8.attention.output.dense.bias: True\n",
      "encoder.layer.8.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.self.query.weight: True\n",
      "encoder.layer.9.attention.self.query.bias: True\n",
      "encoder.layer.9.attention.self.key.weight: True\n",
      "encoder.layer.9.attention.self.key.bias: True\n",
      "encoder.layer.9.attention.self.value.weight: True\n",
      "encoder.layer.9.attention.self.value.bias: True\n",
      "encoder.layer.9.attention.output.dense.weight: True\n",
      "encoder.layer.9.attention.output.dense.bias: True\n",
      "encoder.layer.9.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.self.query.weight: True\n",
      "encoder.layer.10.attention.self.query.bias: True\n",
      "encoder.layer.10.attention.self.key.weight: True\n",
      "encoder.layer.10.attention.self.key.bias: True\n",
      "encoder.layer.10.attention.self.value.weight: True\n",
      "encoder.layer.10.attention.self.value.bias: True\n",
      "encoder.layer.10.attention.output.dense.weight: True\n",
      "encoder.layer.10.attention.output.dense.bias: True\n",
      "encoder.layer.10.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.self.query.weight: True\n",
      "encoder.layer.11.attention.self.query.bias: True\n",
      "encoder.layer.11.attention.self.key.weight: True\n",
      "encoder.layer.11.attention.self.key.bias: True\n",
      "encoder.layer.11.attention.self.value.weight: True\n",
      "encoder.layer.11.attention.self.value.bias: True\n",
      "encoder.layer.11.attention.output.dense.weight: True\n",
      "encoder.layer.11.attention.output.dense.bias: True\n",
      "encoder.layer.11.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n",
      "--------------------------------------\n",
      "DeCLUTR-sci\n",
      "embeddings.word_embeddings.weight: True\n",
      "embeddings.position_embeddings.weight: True\n",
      "embeddings.token_type_embeddings.weight: True\n",
      "embeddings.LayerNorm.weight: True\n",
      "embeddings.LayerNorm.bias: True\n",
      "encoder.layer.0.attention.self.query.weight: True\n",
      "encoder.layer.0.attention.self.query.bias: True\n",
      "encoder.layer.0.attention.self.key.weight: True\n",
      "encoder.layer.0.attention.self.key.bias: True\n",
      "encoder.layer.0.attention.self.value.weight: True\n",
      "encoder.layer.0.attention.self.value.bias: True\n",
      "encoder.layer.0.attention.output.dense.weight: True\n",
      "encoder.layer.0.attention.output.dense.bias: True\n",
      "encoder.layer.0.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.0.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.0.intermediate.dense.weight: True\n",
      "encoder.layer.0.intermediate.dense.bias: True\n",
      "encoder.layer.0.output.dense.weight: True\n",
      "encoder.layer.0.output.dense.bias: True\n",
      "encoder.layer.0.output.LayerNorm.weight: True\n",
      "encoder.layer.0.output.LayerNorm.bias: True\n",
      "encoder.layer.1.attention.self.query.weight: True\n",
      "encoder.layer.1.attention.self.query.bias: True\n",
      "encoder.layer.1.attention.self.key.weight: True\n",
      "encoder.layer.1.attention.self.key.bias: True\n",
      "encoder.layer.1.attention.self.value.weight: True\n",
      "encoder.layer.1.attention.self.value.bias: True\n",
      "encoder.layer.1.attention.output.dense.weight: True\n",
      "encoder.layer.1.attention.output.dense.bias: True\n",
      "encoder.layer.1.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.1.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.1.intermediate.dense.weight: True\n",
      "encoder.layer.1.intermediate.dense.bias: True\n",
      "encoder.layer.1.output.dense.weight: True\n",
      "encoder.layer.1.output.dense.bias: True\n",
      "encoder.layer.1.output.LayerNorm.weight: True\n",
      "encoder.layer.1.output.LayerNorm.bias: True\n",
      "encoder.layer.2.attention.self.query.weight: True\n",
      "encoder.layer.2.attention.self.query.bias: True\n",
      "encoder.layer.2.attention.self.key.weight: True\n",
      "encoder.layer.2.attention.self.key.bias: True\n",
      "encoder.layer.2.attention.self.value.weight: True\n",
      "encoder.layer.2.attention.self.value.bias: True\n",
      "encoder.layer.2.attention.output.dense.weight: True\n",
      "encoder.layer.2.attention.output.dense.bias: True\n",
      "encoder.layer.2.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.2.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.2.intermediate.dense.weight: True\n",
      "encoder.layer.2.intermediate.dense.bias: True\n",
      "encoder.layer.2.output.dense.weight: True\n",
      "encoder.layer.2.output.dense.bias: True\n",
      "encoder.layer.2.output.LayerNorm.weight: True\n",
      "encoder.layer.2.output.LayerNorm.bias: True\n",
      "encoder.layer.3.attention.self.query.weight: True\n",
      "encoder.layer.3.attention.self.query.bias: True\n",
      "encoder.layer.3.attention.self.key.weight: True\n",
      "encoder.layer.3.attention.self.key.bias: True\n",
      "encoder.layer.3.attention.self.value.weight: True\n",
      "encoder.layer.3.attention.self.value.bias: True\n",
      "encoder.layer.3.attention.output.dense.weight: True\n",
      "encoder.layer.3.attention.output.dense.bias: True\n",
      "encoder.layer.3.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.3.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.3.intermediate.dense.weight: True\n",
      "encoder.layer.3.intermediate.dense.bias: True\n",
      "encoder.layer.3.output.dense.weight: True\n",
      "encoder.layer.3.output.dense.bias: True\n",
      "encoder.layer.3.output.LayerNorm.weight: True\n",
      "encoder.layer.3.output.LayerNorm.bias: True\n",
      "encoder.layer.4.attention.self.query.weight: True\n",
      "encoder.layer.4.attention.self.query.bias: True\n",
      "encoder.layer.4.attention.self.key.weight: True\n",
      "encoder.layer.4.attention.self.key.bias: True\n",
      "encoder.layer.4.attention.self.value.weight: True\n",
      "encoder.layer.4.attention.self.value.bias: True\n",
      "encoder.layer.4.attention.output.dense.weight: True\n",
      "encoder.layer.4.attention.output.dense.bias: True\n",
      "encoder.layer.4.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.4.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.4.intermediate.dense.weight: True\n",
      "encoder.layer.4.intermediate.dense.bias: True\n",
      "encoder.layer.4.output.dense.weight: True\n",
      "encoder.layer.4.output.dense.bias: True\n",
      "encoder.layer.4.output.LayerNorm.weight: True\n",
      "encoder.layer.4.output.LayerNorm.bias: True\n",
      "encoder.layer.5.attention.self.query.weight: True\n",
      "encoder.layer.5.attention.self.query.bias: True\n",
      "encoder.layer.5.attention.self.key.weight: True\n",
      "encoder.layer.5.attention.self.key.bias: True\n",
      "encoder.layer.5.attention.self.value.weight: True\n",
      "encoder.layer.5.attention.self.value.bias: True\n",
      "encoder.layer.5.attention.output.dense.weight: True\n",
      "encoder.layer.5.attention.output.dense.bias: True\n",
      "encoder.layer.5.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.self.query.weight: True\n",
      "encoder.layer.6.attention.self.query.bias: True\n",
      "encoder.layer.6.attention.self.key.weight: True\n",
      "encoder.layer.6.attention.self.key.bias: True\n",
      "encoder.layer.6.attention.self.value.weight: True\n",
      "encoder.layer.6.attention.self.value.bias: True\n",
      "encoder.layer.6.attention.output.dense.weight: True\n",
      "encoder.layer.6.attention.output.dense.bias: True\n",
      "encoder.layer.6.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.self.query.weight: True\n",
      "encoder.layer.7.attention.self.query.bias: True\n",
      "encoder.layer.7.attention.self.key.weight: True\n",
      "encoder.layer.7.attention.self.key.bias: True\n",
      "encoder.layer.7.attention.self.value.weight: True\n",
      "encoder.layer.7.attention.self.value.bias: True\n",
      "encoder.layer.7.attention.output.dense.weight: True\n",
      "encoder.layer.7.attention.output.dense.bias: True\n",
      "encoder.layer.7.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.self.query.weight: True\n",
      "encoder.layer.8.attention.self.query.bias: True\n",
      "encoder.layer.8.attention.self.key.weight: True\n",
      "encoder.layer.8.attention.self.key.bias: True\n",
      "encoder.layer.8.attention.self.value.weight: True\n",
      "encoder.layer.8.attention.self.value.bias: True\n",
      "encoder.layer.8.attention.output.dense.weight: True\n",
      "encoder.layer.8.attention.output.dense.bias: True\n",
      "encoder.layer.8.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.self.query.weight: True\n",
      "encoder.layer.9.attention.self.query.bias: True\n",
      "encoder.layer.9.attention.self.key.weight: True\n",
      "encoder.layer.9.attention.self.key.bias: True\n",
      "encoder.layer.9.attention.self.value.weight: True\n",
      "encoder.layer.9.attention.self.value.bias: True\n",
      "encoder.layer.9.attention.output.dense.weight: True\n",
      "encoder.layer.9.attention.output.dense.bias: True\n",
      "encoder.layer.9.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.self.query.weight: True\n",
      "encoder.layer.10.attention.self.query.bias: True\n",
      "encoder.layer.10.attention.self.key.weight: True\n",
      "encoder.layer.10.attention.self.key.bias: True\n",
      "encoder.layer.10.attention.self.value.weight: True\n",
      "encoder.layer.10.attention.self.value.bias: True\n",
      "encoder.layer.10.attention.output.dense.weight: True\n",
      "encoder.layer.10.attention.output.dense.bias: True\n",
      "encoder.layer.10.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.self.query.weight: True\n",
      "encoder.layer.11.attention.self.query.bias: True\n",
      "encoder.layer.11.attention.self.key.weight: True\n",
      "encoder.layer.11.attention.self.key.bias: True\n",
      "encoder.layer.11.attention.self.value.weight: True\n",
      "encoder.layer.11.attention.self.value.bias: True\n",
      "encoder.layer.11.attention.output.dense.weight: True\n",
      "encoder.layer.11.attention.output.dense.bias: True\n",
      "encoder.layer.11.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n",
      "--------------------------------------\n",
      "SPECTER2\n",
      "embeddings.word_embeddings.weight: True\n",
      "embeddings.position_embeddings.weight: True\n",
      "embeddings.token_type_embeddings.weight: True\n",
      "embeddings.LayerNorm.weight: True\n",
      "embeddings.LayerNorm.bias: True\n",
      "encoder.layer.0.attention.self.query.weight: True\n",
      "encoder.layer.0.attention.self.query.bias: True\n",
      "encoder.layer.0.attention.self.key.weight: True\n",
      "encoder.layer.0.attention.self.key.bias: True\n",
      "encoder.layer.0.attention.self.value.weight: True\n",
      "encoder.layer.0.attention.self.value.bias: True\n",
      "encoder.layer.0.attention.output.dense.weight: True\n",
      "encoder.layer.0.attention.output.dense.bias: True\n",
      "encoder.layer.0.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.0.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.0.intermediate.dense.weight: True\n",
      "encoder.layer.0.intermediate.dense.bias: True\n",
      "encoder.layer.0.output.dense.weight: True\n",
      "encoder.layer.0.output.dense.bias: True\n",
      "encoder.layer.0.output.LayerNorm.weight: True\n",
      "encoder.layer.0.output.LayerNorm.bias: True\n",
      "encoder.layer.1.attention.self.query.weight: True\n",
      "encoder.layer.1.attention.self.query.bias: True\n",
      "encoder.layer.1.attention.self.key.weight: True\n",
      "encoder.layer.1.attention.self.key.bias: True\n",
      "encoder.layer.1.attention.self.value.weight: True\n",
      "encoder.layer.1.attention.self.value.bias: True\n",
      "encoder.layer.1.attention.output.dense.weight: True\n",
      "encoder.layer.1.attention.output.dense.bias: True\n",
      "encoder.layer.1.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.1.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.1.intermediate.dense.weight: True\n",
      "encoder.layer.1.intermediate.dense.bias: True\n",
      "encoder.layer.1.output.dense.weight: True\n",
      "encoder.layer.1.output.dense.bias: True\n",
      "encoder.layer.1.output.LayerNorm.weight: True\n",
      "encoder.layer.1.output.LayerNorm.bias: True\n",
      "encoder.layer.2.attention.self.query.weight: True\n",
      "encoder.layer.2.attention.self.query.bias: True\n",
      "encoder.layer.2.attention.self.key.weight: True\n",
      "encoder.layer.2.attention.self.key.bias: True\n",
      "encoder.layer.2.attention.self.value.weight: True\n",
      "encoder.layer.2.attention.self.value.bias: True\n",
      "encoder.layer.2.attention.output.dense.weight: True\n",
      "encoder.layer.2.attention.output.dense.bias: True\n",
      "encoder.layer.2.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.2.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.2.intermediate.dense.weight: True\n",
      "encoder.layer.2.intermediate.dense.bias: True\n",
      "encoder.layer.2.output.dense.weight: True\n",
      "encoder.layer.2.output.dense.bias: True\n",
      "encoder.layer.2.output.LayerNorm.weight: True\n",
      "encoder.layer.2.output.LayerNorm.bias: True\n",
      "encoder.layer.3.attention.self.query.weight: True\n",
      "encoder.layer.3.attention.self.query.bias: True\n",
      "encoder.layer.3.attention.self.key.weight: True\n",
      "encoder.layer.3.attention.self.key.bias: True\n",
      "encoder.layer.3.attention.self.value.weight: True\n",
      "encoder.layer.3.attention.self.value.bias: True\n",
      "encoder.layer.3.attention.output.dense.weight: True\n",
      "encoder.layer.3.attention.output.dense.bias: True\n",
      "encoder.layer.3.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.3.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.3.intermediate.dense.weight: True\n",
      "encoder.layer.3.intermediate.dense.bias: True\n",
      "encoder.layer.3.output.dense.weight: True\n",
      "encoder.layer.3.output.dense.bias: True\n",
      "encoder.layer.3.output.LayerNorm.weight: True\n",
      "encoder.layer.3.output.LayerNorm.bias: True\n",
      "encoder.layer.4.attention.self.query.weight: True\n",
      "encoder.layer.4.attention.self.query.bias: True\n",
      "encoder.layer.4.attention.self.key.weight: True\n",
      "encoder.layer.4.attention.self.key.bias: True\n",
      "encoder.layer.4.attention.self.value.weight: True\n",
      "encoder.layer.4.attention.self.value.bias: True\n",
      "encoder.layer.4.attention.output.dense.weight: True\n",
      "encoder.layer.4.attention.output.dense.bias: True\n",
      "encoder.layer.4.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.4.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.4.intermediate.dense.weight: True\n",
      "encoder.layer.4.intermediate.dense.bias: True\n",
      "encoder.layer.4.output.dense.weight: True\n",
      "encoder.layer.4.output.dense.bias: True\n",
      "encoder.layer.4.output.LayerNorm.weight: True\n",
      "encoder.layer.4.output.LayerNorm.bias: True\n",
      "encoder.layer.5.attention.self.query.weight: True\n",
      "encoder.layer.5.attention.self.query.bias: True\n",
      "encoder.layer.5.attention.self.key.weight: True\n",
      "encoder.layer.5.attention.self.key.bias: True\n",
      "encoder.layer.5.attention.self.value.weight: True\n",
      "encoder.layer.5.attention.self.value.bias: True\n",
      "encoder.layer.5.attention.output.dense.weight: True\n",
      "encoder.layer.5.attention.output.dense.bias: True\n",
      "encoder.layer.5.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.self.query.weight: True\n",
      "encoder.layer.6.attention.self.query.bias: True\n",
      "encoder.layer.6.attention.self.key.weight: True\n",
      "encoder.layer.6.attention.self.key.bias: True\n",
      "encoder.layer.6.attention.self.value.weight: True\n",
      "encoder.layer.6.attention.self.value.bias: True\n",
      "encoder.layer.6.attention.output.dense.weight: True\n",
      "encoder.layer.6.attention.output.dense.bias: True\n",
      "encoder.layer.6.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.self.query.weight: True\n",
      "encoder.layer.7.attention.self.query.bias: True\n",
      "encoder.layer.7.attention.self.key.weight: True\n",
      "encoder.layer.7.attention.self.key.bias: True\n",
      "encoder.layer.7.attention.self.value.weight: True\n",
      "encoder.layer.7.attention.self.value.bias: True\n",
      "encoder.layer.7.attention.output.dense.weight: True\n",
      "encoder.layer.7.attention.output.dense.bias: True\n",
      "encoder.layer.7.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.self.query.weight: True\n",
      "encoder.layer.8.attention.self.query.bias: True\n",
      "encoder.layer.8.attention.self.key.weight: True\n",
      "encoder.layer.8.attention.self.key.bias: True\n",
      "encoder.layer.8.attention.self.value.weight: True\n",
      "encoder.layer.8.attention.self.value.bias: True\n",
      "encoder.layer.8.attention.output.dense.weight: True\n",
      "encoder.layer.8.attention.output.dense.bias: True\n",
      "encoder.layer.8.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.self.query.weight: True\n",
      "encoder.layer.9.attention.self.query.bias: True\n",
      "encoder.layer.9.attention.self.key.weight: True\n",
      "encoder.layer.9.attention.self.key.bias: True\n",
      "encoder.layer.9.attention.self.value.weight: True\n",
      "encoder.layer.9.attention.self.value.bias: True\n",
      "encoder.layer.9.attention.output.dense.weight: True\n",
      "encoder.layer.9.attention.output.dense.bias: True\n",
      "encoder.layer.9.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.self.query.weight: True\n",
      "encoder.layer.10.attention.self.query.bias: True\n",
      "encoder.layer.10.attention.self.key.weight: True\n",
      "encoder.layer.10.attention.self.key.bias: True\n",
      "encoder.layer.10.attention.self.value.weight: True\n",
      "encoder.layer.10.attention.self.value.bias: True\n",
      "encoder.layer.10.attention.output.dense.weight: True\n",
      "encoder.layer.10.attention.output.dense.bias: True\n",
      "encoder.layer.10.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.self.query.weight: True\n",
      "encoder.layer.11.attention.self.query.bias: True\n",
      "encoder.layer.11.attention.self.key.weight: True\n",
      "encoder.layer.11.attention.self.key.bias: True\n",
      "encoder.layer.11.attention.self.value.weight: True\n",
      "encoder.layer.11.attention.self.value.bias: True\n",
      "encoder.layer.11.attention.output.dense.weight: True\n",
      "encoder.layer.11.attention.output.dense.bias: True\n",
      "encoder.layer.11.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # print(\"Running on device: {}\".format(device))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_paths[i])\n",
    "    model = AutoModel.from_pretrained(model_paths[i])\n",
    "    # print(model_paths[i])\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.requires_grad}\")\n",
    "\n",
    "    print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layer[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetPooler(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pooler  # .layer[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = [\n",
    "    model.embeddings,\n",
    "    *model.encoder.layer[:2],\n",
    "]  # Replace 5 by what you want\n",
    "for module in modules:\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight: False\n",
      "embeddings.position_embeddings.weight: False\n",
      "embeddings.token_type_embeddings.weight: False\n",
      "embeddings.LayerNorm.weight: False\n",
      "embeddings.LayerNorm.bias: False\n",
      "encoder.layer.0.attention.self.query.weight: False\n",
      "encoder.layer.0.attention.self.query.bias: False\n",
      "encoder.layer.0.attention.self.key.weight: False\n",
      "encoder.layer.0.attention.self.key.bias: False\n",
      "encoder.layer.0.attention.self.value.weight: False\n",
      "encoder.layer.0.attention.self.value.bias: False\n",
      "encoder.layer.0.attention.output.dense.weight: False\n",
      "encoder.layer.0.attention.output.dense.bias: False\n",
      "encoder.layer.0.attention.output.LayerNorm.weight: False\n",
      "encoder.layer.0.attention.output.LayerNorm.bias: False\n",
      "encoder.layer.0.intermediate.dense.weight: False\n",
      "encoder.layer.0.intermediate.dense.bias: False\n",
      "encoder.layer.0.output.dense.weight: False\n",
      "encoder.layer.0.output.dense.bias: False\n",
      "encoder.layer.0.output.LayerNorm.weight: False\n",
      "encoder.layer.0.output.LayerNorm.bias: False\n",
      "encoder.layer.1.attention.self.query.weight: False\n",
      "encoder.layer.1.attention.self.query.bias: False\n",
      "encoder.layer.1.attention.self.key.weight: False\n",
      "encoder.layer.1.attention.self.key.bias: False\n",
      "encoder.layer.1.attention.self.value.weight: False\n",
      "encoder.layer.1.attention.self.value.bias: False\n",
      "encoder.layer.1.attention.output.dense.weight: False\n",
      "encoder.layer.1.attention.output.dense.bias: False\n",
      "encoder.layer.1.attention.output.LayerNorm.weight: False\n",
      "encoder.layer.1.attention.output.LayerNorm.bias: False\n",
      "encoder.layer.1.intermediate.dense.weight: False\n",
      "encoder.layer.1.intermediate.dense.bias: False\n",
      "encoder.layer.1.output.dense.weight: False\n",
      "encoder.layer.1.output.dense.bias: False\n",
      "encoder.layer.1.output.LayerNorm.weight: False\n",
      "encoder.layer.1.output.LayerNorm.bias: False\n",
      "encoder.layer.2.attention.self.query.weight: True\n",
      "encoder.layer.2.attention.self.query.bias: True\n",
      "encoder.layer.2.attention.self.key.weight: True\n",
      "encoder.layer.2.attention.self.key.bias: True\n",
      "encoder.layer.2.attention.self.value.weight: True\n",
      "encoder.layer.2.attention.self.value.bias: True\n",
      "encoder.layer.2.attention.output.dense.weight: True\n",
      "encoder.layer.2.attention.output.dense.bias: True\n",
      "encoder.layer.2.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.2.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.2.intermediate.dense.weight: True\n",
      "encoder.layer.2.intermediate.dense.bias: True\n",
      "encoder.layer.2.output.dense.weight: True\n",
      "encoder.layer.2.output.dense.bias: True\n",
      "encoder.layer.2.output.LayerNorm.weight: True\n",
      "encoder.layer.2.output.LayerNorm.bias: True\n",
      "encoder.layer.3.attention.self.query.weight: True\n",
      "encoder.layer.3.attention.self.query.bias: True\n",
      "encoder.layer.3.attention.self.key.weight: True\n",
      "encoder.layer.3.attention.self.key.bias: True\n",
      "encoder.layer.3.attention.self.value.weight: True\n",
      "encoder.layer.3.attention.self.value.bias: True\n",
      "encoder.layer.3.attention.output.dense.weight: True\n",
      "encoder.layer.3.attention.output.dense.bias: True\n",
      "encoder.layer.3.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.3.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.3.intermediate.dense.weight: True\n",
      "encoder.layer.3.intermediate.dense.bias: True\n",
      "encoder.layer.3.output.dense.weight: True\n",
      "encoder.layer.3.output.dense.bias: True\n",
      "encoder.layer.3.output.LayerNorm.weight: True\n",
      "encoder.layer.3.output.LayerNorm.bias: True\n",
      "encoder.layer.4.attention.self.query.weight: True\n",
      "encoder.layer.4.attention.self.query.bias: True\n",
      "encoder.layer.4.attention.self.key.weight: True\n",
      "encoder.layer.4.attention.self.key.bias: True\n",
      "encoder.layer.4.attention.self.value.weight: True\n",
      "encoder.layer.4.attention.self.value.bias: True\n",
      "encoder.layer.4.attention.output.dense.weight: True\n",
      "encoder.layer.4.attention.output.dense.bias: True\n",
      "encoder.layer.4.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.4.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.4.intermediate.dense.weight: True\n",
      "encoder.layer.4.intermediate.dense.bias: True\n",
      "encoder.layer.4.output.dense.weight: True\n",
      "encoder.layer.4.output.dense.bias: True\n",
      "encoder.layer.4.output.LayerNorm.weight: True\n",
      "encoder.layer.4.output.LayerNorm.bias: True\n",
      "encoder.layer.5.attention.self.query.weight: True\n",
      "encoder.layer.5.attention.self.query.bias: True\n",
      "encoder.layer.5.attention.self.key.weight: True\n",
      "encoder.layer.5.attention.self.key.bias: True\n",
      "encoder.layer.5.attention.self.value.weight: True\n",
      "encoder.layer.5.attention.self.value.bias: True\n",
      "encoder.layer.5.attention.output.dense.weight: True\n",
      "encoder.layer.5.attention.output.dense.bias: True\n",
      "encoder.layer.5.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.5.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.5.intermediate.dense.weight: True\n",
      "encoder.layer.5.intermediate.dense.bias: True\n",
      "encoder.layer.5.output.dense.weight: True\n",
      "encoder.layer.5.output.dense.bias: True\n",
      "encoder.layer.5.output.LayerNorm.weight: True\n",
      "encoder.layer.5.output.LayerNorm.bias: True\n",
      "encoder.layer.6.attention.self.query.weight: True\n",
      "encoder.layer.6.attention.self.query.bias: True\n",
      "encoder.layer.6.attention.self.key.weight: True\n",
      "encoder.layer.6.attention.self.key.bias: True\n",
      "encoder.layer.6.attention.self.value.weight: True\n",
      "encoder.layer.6.attention.self.value.bias: True\n",
      "encoder.layer.6.attention.output.dense.weight: True\n",
      "encoder.layer.6.attention.output.dense.bias: True\n",
      "encoder.layer.6.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.6.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.6.intermediate.dense.weight: True\n",
      "encoder.layer.6.intermediate.dense.bias: True\n",
      "encoder.layer.6.output.dense.weight: True\n",
      "encoder.layer.6.output.dense.bias: True\n",
      "encoder.layer.6.output.LayerNorm.weight: True\n",
      "encoder.layer.6.output.LayerNorm.bias: True\n",
      "encoder.layer.7.attention.self.query.weight: True\n",
      "encoder.layer.7.attention.self.query.bias: True\n",
      "encoder.layer.7.attention.self.key.weight: True\n",
      "encoder.layer.7.attention.self.key.bias: True\n",
      "encoder.layer.7.attention.self.value.weight: True\n",
      "encoder.layer.7.attention.self.value.bias: True\n",
      "encoder.layer.7.attention.output.dense.weight: True\n",
      "encoder.layer.7.attention.output.dense.bias: True\n",
      "encoder.layer.7.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.7.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.7.intermediate.dense.weight: True\n",
      "encoder.layer.7.intermediate.dense.bias: True\n",
      "encoder.layer.7.output.dense.weight: True\n",
      "encoder.layer.7.output.dense.bias: True\n",
      "encoder.layer.7.output.LayerNorm.weight: True\n",
      "encoder.layer.7.output.LayerNorm.bias: True\n",
      "encoder.layer.8.attention.self.query.weight: True\n",
      "encoder.layer.8.attention.self.query.bias: True\n",
      "encoder.layer.8.attention.self.key.weight: True\n",
      "encoder.layer.8.attention.self.key.bias: True\n",
      "encoder.layer.8.attention.self.value.weight: True\n",
      "encoder.layer.8.attention.self.value.bias: True\n",
      "encoder.layer.8.attention.output.dense.weight: True\n",
      "encoder.layer.8.attention.output.dense.bias: True\n",
      "encoder.layer.8.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.8.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.8.intermediate.dense.weight: True\n",
      "encoder.layer.8.intermediate.dense.bias: True\n",
      "encoder.layer.8.output.dense.weight: True\n",
      "encoder.layer.8.output.dense.bias: True\n",
      "encoder.layer.8.output.LayerNorm.weight: True\n",
      "encoder.layer.8.output.LayerNorm.bias: True\n",
      "encoder.layer.9.attention.self.query.weight: True\n",
      "encoder.layer.9.attention.self.query.bias: True\n",
      "encoder.layer.9.attention.self.key.weight: True\n",
      "encoder.layer.9.attention.self.key.bias: True\n",
      "encoder.layer.9.attention.self.value.weight: True\n",
      "encoder.layer.9.attention.self.value.bias: True\n",
      "encoder.layer.9.attention.output.dense.weight: True\n",
      "encoder.layer.9.attention.output.dense.bias: True\n",
      "encoder.layer.9.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.9.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.9.intermediate.dense.weight: True\n",
      "encoder.layer.9.intermediate.dense.bias: True\n",
      "encoder.layer.9.output.dense.weight: True\n",
      "encoder.layer.9.output.dense.bias: True\n",
      "encoder.layer.9.output.LayerNorm.weight: True\n",
      "encoder.layer.9.output.LayerNorm.bias: True\n",
      "encoder.layer.10.attention.self.query.weight: True\n",
      "encoder.layer.10.attention.self.query.bias: True\n",
      "encoder.layer.10.attention.self.key.weight: True\n",
      "encoder.layer.10.attention.self.key.bias: True\n",
      "encoder.layer.10.attention.self.value.weight: True\n",
      "encoder.layer.10.attention.self.value.bias: True\n",
      "encoder.layer.10.attention.output.dense.weight: True\n",
      "encoder.layer.10.attention.output.dense.bias: True\n",
      "encoder.layer.10.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.10.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.10.intermediate.dense.weight: True\n",
      "encoder.layer.10.intermediate.dense.bias: True\n",
      "encoder.layer.10.output.dense.weight: True\n",
      "encoder.layer.10.output.dense.bias: True\n",
      "encoder.layer.10.output.LayerNorm.weight: True\n",
      "encoder.layer.10.output.LayerNorm.bias: True\n",
      "encoder.layer.11.attention.self.query.weight: True\n",
      "encoder.layer.11.attention.self.query.bias: True\n",
      "encoder.layer.11.attention.self.key.weight: True\n",
      "encoder.layer.11.attention.self.key.bias: True\n",
      "encoder.layer.11.attention.self.value.weight: True\n",
      "encoder.layer.11.attention.self.value.bias: True\n",
      "encoder.layer.11.attention.output.dense.weight: True\n",
      "encoder.layer.11.attention.output.dense.bias: True\n",
      "encoder.layer.11.attention.output.LayerNorm.weight: True\n",
      "encoder.layer.11.attention.output.LayerNorm.bias: True\n",
      "encoder.layer.11.intermediate.dense.weight: True\n",
      "encoder.layer.11.intermediate.dense.bias: True\n",
      "encoder.layer.11.output.dense.weight: True\n",
      "encoder.layer.11.output.dense.bias: True\n",
      "encoder.layer.11.output.LayerNorm.weight: True\n",
      "encoder.layer.11.output.LayerNorm.bias: True\n",
      "pooler.dense.weight: True\n",
      "pooler.dense.bias: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exploration output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [\"Hi my name is Rita.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the input\n",
    "inputs = tokenizer(\n",
    "    test_text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=512,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  7636,  2030,  2175,  2007, 11624,  1016,     2]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPooling(last_hidden_state=tensor([[[-0.1944,  0.2234, -0.0105,  ...,  0.1301, -0.0548,  0.0220],\n",
       "         [-0.0627, -0.2211, -0.0483,  ...,  0.0552, -0.0270, -0.0104],\n",
       "         [-0.1352,  0.0082,  0.0685,  ..., -0.0541,  0.0621,  0.0774],\n",
       "         ...,\n",
       "         [ 0.0301,  0.4014,  0.0188,  ...,  0.1003, -0.0244, -0.0517],\n",
       "         [ 0.0797,  0.0526,  0.0305,  ...,  0.0518, -0.1234, -0.0463],\n",
       "         [ 0.0103,  0.4551,  0.0095,  ...,  0.0741, -0.1618,  0.0528]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.1421, -0.0403,  0.0245, -0.0105,  0.0191, -0.1390, -0.0840,  0.0648,\n",
       "          0.0033,  0.0307, -0.0077,  0.0265, -0.0257, -0.0424,  0.0317, -0.0537,\n",
       "         -0.1393, -0.1608, -0.0270, -0.0185,  0.0355, -0.0515, -0.0617,  0.0763,\n",
       "         -0.0769, -0.0263, -0.0543,  0.1073,  0.0768, -0.0264, -0.0008,  0.0354,\n",
       "          0.0311,  0.0679,  0.0668,  0.0498, -0.0157, -0.0352,  0.0495, -0.0952,\n",
       "         -0.0282,  0.0049, -0.0543,  0.0640, -0.2114,  0.0520, -0.0730, -0.0357,\n",
       "         -0.1309, -0.0453, -0.0828,  0.0501, -0.0918,  0.0209, -0.0158,  0.0761,\n",
       "          0.0339,  0.0435, -0.0234,  0.0396,  0.0032, -0.0703, -0.1093, -0.0256,\n",
       "          0.1445,  0.0199,  0.0917,  0.0056,  0.0797,  0.0415,  0.0135, -0.0534,\n",
       "          0.0519, -0.1556, -0.0298,  0.0572,  0.0722, -0.0711, -0.0485,  0.0610,\n",
       "         -0.0735, -0.1043, -0.0572, -0.0228,  0.0148,  0.1789, -0.0158, -0.0657,\n",
       "          0.0013,  0.1009,  0.0343,  0.0340,  0.0700, -0.1347,  0.1805,  0.0445,\n",
       "         -0.0407, -0.1547,  0.0031,  0.1559,  0.1008,  0.0969, -0.1276,  0.1010,\n",
       "          0.0099,  0.0726,  0.0048, -0.1608,  0.0251,  0.0333,  0.1835, -0.0075,\n",
       "         -0.0474,  0.0366, -0.0801,  0.0236,  0.0981,  0.0837, -0.0564,  0.0065,\n",
       "          0.0264, -0.0360,  0.1455, -0.1227,  0.0546, -0.0431, -0.0164,  0.0020,\n",
       "          0.1706,  0.0285, -0.0605, -0.0398,  0.0166, -0.0476, -0.0662,  0.0299,\n",
       "         -0.1312,  0.0126,  0.0034, -0.0385,  0.0431,  0.0019,  0.0509,  0.0101,\n",
       "         -0.0100,  0.0755, -0.0030,  0.0354, -0.0033,  0.0274,  0.0215,  0.1205,\n",
       "          0.0045,  0.1168,  0.0523,  0.0315, -0.0136,  0.0935,  0.1187, -0.1395,\n",
       "          0.0190, -0.0004, -0.0128,  0.2218, -0.1237, -0.0751,  0.0671, -0.1381,\n",
       "         -0.0941, -0.1302,  0.0585,  0.0961, -0.1612, -0.0665, -0.0126, -0.0871,\n",
       "         -0.0054,  0.1587,  0.0767,  0.1390, -0.1884,  0.0159,  0.1341, -0.0366,\n",
       "          0.0588, -0.0510,  0.0071, -0.0142,  0.0603, -0.0750,  0.0993, -0.0192,\n",
       "         -0.0385,  0.2382, -0.0419, -0.0098, -0.0113,  0.0124,  0.0895,  0.1235,\n",
       "          0.0877,  0.0699,  0.0710, -0.0682,  0.0496, -0.1342,  0.0394, -0.1375,\n",
       "          0.0941, -0.0195, -0.2167, -0.1001,  0.0598,  0.1611,  0.1003, -0.0237,\n",
       "         -0.1245, -0.0577,  0.1759, -0.0300, -0.0275,  0.0216, -0.1289, -0.1862,\n",
       "          0.1940,  0.0238,  0.0460,  0.0174, -0.1143,  0.1745,  0.0275,  0.1125,\n",
       "         -0.0528, -0.0302, -0.0620, -0.0091, -0.1816,  0.0550, -0.0655, -0.0775,\n",
       "         -0.0489, -0.0638, -0.0212, -0.0838,  0.0027,  0.1730,  0.0196,  0.0175,\n",
       "          0.0979,  0.1651, -0.0994, -0.0792, -0.0836, -0.0398, -0.0717, -0.1884,\n",
       "         -0.0393, -0.0571,  0.1119,  0.0609,  0.1086, -0.0135, -0.1643, -0.1243,\n",
       "          0.1393,  0.0777,  0.1153, -0.0155,  0.0923, -0.0149, -0.0207, -0.1347,\n",
       "          0.1185, -0.0656,  0.0814, -0.1777,  0.0933, -0.1135, -0.1081,  0.1256,\n",
       "         -0.0823, -0.0104,  0.2535,  0.0828,  0.0039, -0.0120, -0.0724, -0.0353,\n",
       "          0.0101,  0.1069, -0.0445, -0.0196, -0.0666,  0.0009,  0.0704,  0.1395,\n",
       "         -0.1232,  0.0861,  0.0479,  0.0267, -0.0220, -0.0342, -0.0823, -0.0566,\n",
       "         -0.0373, -0.0425, -0.0819,  0.0263, -0.0496,  0.0043, -0.0215,  0.0131,\n",
       "          0.0477, -0.0668, -0.0956, -0.1017,  0.0024,  0.0783,  0.0015,  0.0740,\n",
       "         -0.1393,  0.1586, -0.0637,  0.0013,  0.0962,  0.0130,  0.0218, -0.1094,\n",
       "         -0.0729, -0.1168, -0.1141,  0.0987,  0.0074, -0.0692,  0.1126, -0.0206,\n",
       "          0.0465, -0.0475,  0.0965, -0.0418, -0.1275, -0.0127, -0.0154,  0.0655,\n",
       "         -0.0542, -0.1018,  0.0062, -0.2282,  0.0326, -0.0929,  0.1053, -0.0852,\n",
       "         -0.0522,  0.0264,  0.0709,  0.1841, -0.0597,  0.0179, -0.0615,  0.1383,\n",
       "          0.0011, -0.0530,  0.0907, -0.1475,  0.0592, -0.2172,  0.1627, -0.0703,\n",
       "          0.0142, -0.0349, -0.1525,  0.0429,  0.0547, -0.0919, -0.1510,  0.0467,\n",
       "          0.0105, -0.0282, -0.0085,  0.0763,  0.0460,  0.0100,  0.0350, -0.0673,\n",
       "         -0.0292,  0.0866, -0.1003,  0.2292, -0.0063, -0.0668,  0.0103, -0.0313,\n",
       "          0.1103,  0.0291, -0.0355,  0.0020,  0.0418,  0.1994, -0.1300,  0.0406,\n",
       "         -0.1497, -0.0247,  0.0629, -0.1944,  0.0457,  0.0097, -0.0409, -0.1514,\n",
       "         -0.0035, -0.1071,  0.0826, -0.0120,  0.0296,  0.0137,  0.0538, -0.0692,\n",
       "          0.1513,  0.0629,  0.0053,  0.0955, -0.0254, -0.0982, -0.1417,  0.1291,\n",
       "         -0.0283, -0.0249,  0.0612,  0.1957,  0.0945,  0.0707, -0.0186,  0.0060,\n",
       "          0.0172, -0.0039, -0.1247, -0.0070,  0.1485,  0.0472,  0.0078, -0.1001,\n",
       "          0.0932, -0.1843, -0.0707,  0.1662, -0.0409, -0.0372,  0.0021, -0.0238,\n",
       "          0.1460,  0.0361,  0.0046,  0.0261, -0.0761,  0.0394,  0.0866,  0.0843,\n",
       "          0.0925,  0.2379,  0.1153, -0.0342,  0.1517,  0.0893, -0.0111, -0.1343,\n",
       "          0.0336, -0.1228,  0.0496, -0.1162, -0.0595,  0.0048, -0.1380,  0.1075,\n",
       "         -0.0808,  0.0237,  0.1208,  0.0508,  0.0653,  0.0258,  0.0145,  0.1558,\n",
       "         -0.0936,  0.0030, -0.0135,  0.0363,  0.0094, -0.0812, -0.0879, -0.0181,\n",
       "         -0.0367,  0.1210, -0.1120,  0.0955,  0.0737, -0.1657,  0.1203,  0.0138,\n",
       "          0.1063,  0.1290, -0.1021,  0.0311, -0.1008, -0.1161,  0.0740,  0.0475,\n",
       "         -0.0202,  0.0520, -0.2008,  0.0895, -0.1601,  0.0211,  0.0168,  0.0259,\n",
       "         -0.1260,  0.0598,  0.0795,  0.0638,  0.1386,  0.0247, -0.0551, -0.1412,\n",
       "         -0.0356, -0.0315,  0.0311, -0.0598, -0.0468,  0.0256, -0.0137, -0.0894,\n",
       "          0.1650, -0.0576, -0.0375, -0.1242, -0.0233,  0.1001,  0.0515,  0.0211,\n",
       "          0.1084, -0.0126, -0.0392,  0.0961, -0.1171,  0.0701, -0.0583,  0.0419,\n",
       "          0.1463, -0.0389, -0.0288,  0.0653,  0.0571,  0.0841, -0.1354, -0.1715,\n",
       "          0.0833,  0.2086, -0.0488, -0.0517,  0.1254, -0.0366,  0.1766,  0.0145,\n",
       "         -0.0841, -0.0852,  0.1139, -0.0861, -0.0811, -0.0662,  0.0178, -0.1225,\n",
       "          0.0121,  0.0929, -0.0460, -0.0309,  0.1019,  0.0464, -0.0576, -0.0589,\n",
       "          0.1333, -0.0455,  0.1334, -0.0543, -0.0566, -0.0383,  0.1297, -0.2168,\n",
       "         -0.0183,  0.0441,  0.0109,  0.0819, -0.0547, -0.0393,  0.0797, -0.0536,\n",
       "          0.0772, -0.0985, -0.0092,  0.0069, -0.0092,  0.0792,  0.0071,  0.0786,\n",
       "         -0.0477, -0.0105, -0.0040, -0.1331, -0.0656, -0.0177, -0.0132,  0.1376,\n",
       "          0.1012, -0.0302,  0.0601, -0.0639, -0.0744, -0.1595, -0.1647, -0.0567,\n",
       "         -0.1407,  0.1506,  0.0885, -0.0692, -0.0053,  0.1523, -0.0273,  0.0700,\n",
       "          0.1730, -0.0383,  0.0485,  0.0174, -0.1973, -0.0706,  0.1196, -0.1575,\n",
       "         -0.0197, -0.0043, -0.0589, -0.0389, -0.1272, -0.0688, -0.0396,  0.0579,\n",
       "         -0.0530,  0.0236,  0.0607,  0.0393, -0.0664,  0.1083, -0.0546,  0.0188,\n",
       "         -0.0087, -0.1253,  0.0729,  0.0193, -0.0543,  0.1816, -0.0384, -0.0670,\n",
       "         -0.0570, -0.0049, -0.0785,  0.0157, -0.0565,  0.0048,  0.0514, -0.0291,\n",
       "          0.0658,  0.0143, -0.0081,  0.0960, -0.1051, -0.0667,  0.0255,  0.0282,\n",
       "          0.0053,  0.0847,  0.0051, -0.2269, -0.1206, -0.1442,  0.0118, -0.0163,\n",
       "          0.0137, -0.0553,  0.0432, -0.1414, -0.1147, -0.0064,  0.0872, -0.0164,\n",
       "         -0.0126,  0.0620,  0.0763,  0.0454,  0.0727,  0.0473,  0.0171,  0.1674,\n",
       "          0.0808,  0.0534,  0.0350,  0.0239,  0.0027,  0.0337, -0.0938,  0.1103,\n",
       "         -0.0564, -0.0414,  0.0116,  0.0798, -0.0003, -0.0748, -0.0478, -0.0481,\n",
       "         -0.0085, -0.0087, -0.0086, -0.0350, -0.0307,  0.0410, -0.1184, -0.1844,\n",
       "         -0.0366, -0.0063,  0.0043, -0.0610,  0.0011,  0.0934, -0.1993,  0.0315,\n",
       "          0.1172,  0.0039,  0.0778, -0.0556, -0.0585, -0.1434,  0.0519, -0.0771,\n",
       "          0.1264,  0.0544,  0.0080, -0.0245, -0.1340,  0.0327, -0.1028, -0.1146,\n",
       "          0.1294,  0.0430,  0.0022,  0.1622, -0.0416,  0.0207,  0.0730, -0.0143,\n",
       "         -0.1324, -0.0622, -0.0272,  0.0006, -0.2071, -0.1142,  0.0628, -0.0497,\n",
       "         -0.0414, -0.0103,  0.0171, -0.1172,  0.0041, -0.1289, -0.0186,  0.0411]],\n",
       "       device='cuda:0', grad_fn=<TanhBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**inputs)  # [0].shape  # [1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # inference\n",
    "# outputs = model(**inputs)[0].cpu().detach() \n",
    "\n",
    "# embedding_av = torch.mean(outputs, [0, 1]).numpy()\n",
    "# embedding_sep = outputs[:, -1, :].numpy()\n",
    "# embedding_cls = outputs[:, 0, :].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooling functions\n",
    "def mean_pool(token_embeds, attention_mask):\n",
    "    # reshape attention_mask to cover 768-dimension embeddings\n",
    "    in_mask = attention_mask.unsqueeze(-1).expand(token_embeds.size())\n",
    "    # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
    "    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(\n",
    "        in_mask.sum(1), min=1e-9\n",
    "    )\n",
    "    return pool\n",
    "\n",
    "\n",
    "def sep_pool(token_embeds, attention_mask):\n",
    "    ix = attention_mask.sum(1) - 1\n",
    "    ix0 = torch.arange(attention_mask.size(0))\n",
    "    return token_embeds[ix0, ix, :]\n",
    "\n",
    "\n",
    "def cls_pool(token_embeds, attention_mask):\n",
    "    ix0 = torch.arange(attention_mask.size(0))\n",
    "    return token_embeds[ix0, 0, :]\n",
    "\n",
    "\n",
    "def seventh_pool(token_embeds, attention_mask):\n",
    "    ix0 = torch.arange(attention_mask.size(0))\n",
    "    return token_embeds[ix0, 7, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "import adapters\n",
    "from adapters import AutoAdapterModel\n",
    "\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, checkpoint, in_dim=768, feat_dim=128, hidden_dim=512):\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.feat_dim = feat_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Load Model with given checkpoint and extract its body\n",
    "        if checkpoint == \"allenai/specter2_base\":\n",
    "            self.model = AutoAdapterModel.from_pretrained(checkpoint)\n",
    "            # add adapter proximity\n",
    "            self.model.load_adapter(\n",
    "                \"allenai/specter2\",\n",
    "                source=\"hf\",\n",
    "                load_as=\"specter2\",\n",
    "                set_active=True,\n",
    "            )\n",
    "        else:\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                checkpoint\n",
    "            )  # ,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "\n",
    "        # add projection head\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, self.hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.hidden_dim, self.feat_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, optimized_rep=\"av\"):\n",
    "        # Extract outputs from the body\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        )[0]\n",
    "\n",
    "        # pooling\n",
    "        if optimized_rep == \"av\":\n",
    "            h = mean_pool(outputs, attention_mask)\n",
    "\n",
    "        elif optimized_rep == \"cls\":\n",
    "            h = cls_pool(outputs, attention_mask)\n",
    "\n",
    "        elif optimized_rep == \"sep\":\n",
    "            h = sep_pool(outputs, attention_mask)\n",
    "\n",
    "        elif optimized_rep == \"7th\":\n",
    "            h = seventh_pool(outputs, attention_mask)\n",
    "\n",
    "        # Add custom layers\n",
    "        z = self.projection_head(h)  # .view(-1,768)\n",
    "\n",
    "        return z, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = CustomModel(model_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (projection_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test  # .projection_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embeddings.word_embeddings.weight: True\n",
      "model.embeddings.position_embeddings.weight: True\n",
      "model.embeddings.token_type_embeddings.weight: True\n",
      "model.embeddings.LayerNorm.weight: True\n",
      "model.embeddings.LayerNorm.bias: True\n",
      "model.encoder.layer.0.attention.self.query.weight: True\n",
      "model.encoder.layer.0.attention.self.query.bias: True\n",
      "model.encoder.layer.0.attention.self.key.weight: True\n",
      "model.encoder.layer.0.attention.self.key.bias: True\n",
      "model.encoder.layer.0.attention.self.value.weight: True\n",
      "model.encoder.layer.0.attention.self.value.bias: True\n",
      "model.encoder.layer.0.attention.output.dense.weight: True\n",
      "model.encoder.layer.0.attention.output.dense.bias: True\n",
      "model.encoder.layer.0.attention.output.LayerNorm.weight: True\n",
      "model.encoder.layer.0.attention.output.LayerNorm.bias: True\n",
      "model.encoder.layer.0.intermediate.dense.weight: True\n",
      "model.encoder.layer.0.intermediate.dense.bias: True\n",
      "model.encoder.layer.0.output.dense.weight: True\n",
      "model.encoder.layer.0.output.dense.bias: True\n",
      "model.encoder.layer.0.output.LayerNorm.weight: True\n",
      "model.encoder.layer.0.output.LayerNorm.bias: True\n",
      "model.encoder.layer.1.attention.self.query.weight: True\n",
      "model.encoder.layer.1.attention.self.query.bias: True\n",
      "model.encoder.layer.1.attention.self.key.weight: True\n",
      "model.encoder.layer.1.attention.self.key.bias: True\n",
      "model.encoder.layer.1.attention.self.value.weight: True\n",
      "model.encoder.layer.1.attention.self.value.bias: True\n",
      "model.encoder.layer.1.attention.output.dense.weight: True\n",
      "model.encoder.layer.1.attention.output.dense.bias: True\n",
      "model.encoder.layer.1.attention.output.LayerNorm.weight: True\n",
      "model.encoder.layer.1.attention.output.LayerNorm.bias: True\n",
      "model.encoder.layer.1.intermediate.dense.weight: True\n",
      "model.encoder.layer.1.intermediate.dense.bias: True\n",
      "model.encoder.layer.1.output.dense.weight: True\n",
      "model.encoder.layer.1.output.dense.bias: True\n",
      "model.encoder.layer.1.output.LayerNorm.weight: True\n",
      "model.encoder.layer.1.output.LayerNorm.bias: True\n",
      "model.encoder.layer.2.attention.self.query.weight: True\n",
      "model.encoder.layer.2.attention.self.query.bias: True\n",
      "model.encoder.layer.2.attention.self.key.weight: True\n",
      "model.encoder.layer.2.attention.self.key.bias: True\n",
      "model.encoder.layer.2.attention.self.value.weight: True\n",
      "model.encoder.layer.2.attention.self.value.bias: True\n",
      "model.encoder.layer.2.attention.output.dense.weight: True\n",
      "model.encoder.layer.2.attention.output.dense.bias: True\n",
      "model.encoder.layer.2.attention.output.LayerNorm.weight: True\n",
      "model.encoder.layer.2.attention.output.LayerNorm.bias: True\n",
      "model.encoder.layer.2.intermediate.dense.weight: True\n",
      "model.encoder.layer.2.intermediate.dense.bias: True\n",
      "model.encoder.layer.2.output.dense.weight: True\n",
      "model.encoder.layer.2.output.dense.bias: True\n",
      "model.encoder.layer.2.output.LayerNorm.weight: True\n",
      "model.encoder.layer.2.output.LayerNorm.bias: True\n",
      "model.encoder.layer.3.attention.self.query.weight: True\n",
      "model.encoder.layer.3.attention.self.query.bias: True\n",
      "model.encoder.layer.3.attention.self.key.weight: True\n",
      "model.encoder.layer.3.attention.self.key.bias: True\n",
      "model.encoder.layer.3.attention.self.value.weight: True\n",
      "model.encoder.layer.3.attention.self.value.bias: True\n",
      "model.encoder.layer.3.attention.output.dense.weight: True\n",
      "model.encoder.layer.3.attention.output.dense.bias: True\n",
      "model.encoder.layer.3.attention.output.LayerNorm.weight: True\n",
      "model.encoder.layer.3.attention.output.LayerNorm.bias: True\n",
      "model.encoder.layer.3.intermediate.dense.weight: True\n",
      "model.encoder.layer.3.intermediate.dense.bias: True\n",
      "model.encoder.layer.3.output.dense.weight: True\n",
      "model.encoder.layer.3.output.dense.bias: True\n",
      "model.encoder.layer.3.output.LayerNorm.weight: True\n",
      "model.encoder.layer.3.output.LayerNorm.bias: True\n",
      "model.encoder.layer.4.attention.self.query.weight: True\n",
      "model.encoder.layer.4.attention.self.query.bias: True\n",
      "model.encoder.layer.4.attention.self.key.weight: True\n",
      "model.encoder.layer.4.attention.self.key.bias: True\n",
      "model.encoder.layer.4.attention.self.value.weight: True\n",
      "model.encoder.layer.4.attention.self.value.bias: True\n",
      "model.encoder.layer.4.attention.output.dense.weight: True\n",
      "model.encoder.layer.4.attention.output.dense.bias: True\n",
      "model.encoder.layer.4.attention.output.LayerNorm.weight: True\n",
      "model.encoder.layer.4.attention.output.LayerNorm.bias: True\n",
      "model.encoder.layer.4.intermediate.dense.weight: True\n",
      "model.encoder.layer.4.intermediate.dense.bias: True\n",
      "model.encoder.layer.4.output.dense.weight: True\n",
      "model.encoder.layer.4.output.dense.bias: True\n",
      "model.encoder.layer.4.output.LayerNorm.weight: True\n",
      "model.encoder.layer.4.output.LayerNorm.bias: True\n",
      "model.encoder.layer.5.attention.self.query.weight: True\n",
      "model.encoder.layer.5.attention.self.query.bias: True\n",
      "model.encoder.layer.5.attention.self.key.weight: True\n",
      "model.encoder.layer.5.attention.self.key.bias: True\n",
      "model.encoder.layer.5.attention.self.value.weight: True\n",
      "model.encoder.layer.5.attention.self.value.bias: True\n",
      "model.encoder.layer.5.attention.output.dense.weight: True\n",
      "model.encoder.layer.5.attention.output.dense.bias: True\n",
      "model.encoder.layer.5.attention.output.LayerNorm.weight: True\n",
      "model.encoder.layer.5.attention.output.LayerNorm.bias: True\n",
      "model.encoder.layer.5.intermediate.dense.weight: True\n",
      "model.encoder.layer.5.intermediate.dense.bias: True\n",
      "model.encoder.layer.5.output.dense.weight: True\n",
      "model.encoder.layer.5.output.dense.bias: True\n",
      "model.encoder.layer.5.output.LayerNorm.weight: True\n",
      "model.encoder.layer.5.output.LayerNorm.bias: True\n",
      "model.encoder.layer.6.attention.self.query.weight: True\n",
      "model.encoder.layer.6.attention.self.query.bias: True\n",
      "model.encoder.layer.6.attention.self.key.weight: True\n",
      "model.encoder.layer.6.attention.self.key.bias: True\n",
      "model.encoder.layer.6.attention.self.value.weight: True\n",
      "model.encoder.layer.6.attention.self.value.bias: True\n",
      "model.encoder.layer.6.attention.output.dense.weight: True\n",
      "model.encoder.layer.6.attention.output.dense.bias: True\n",
      "model.encoder.layer.6.attention.output.LayerNorm.weight: True\n",
      "model.encoder.layer.6.attention.output.LayerNorm.bias: True\n",
      "model.encoder.layer.6.intermediate.dense.weight: True\n",
      "model.encoder.layer.6.intermediate.dense.bias: True\n",
      "model.encoder.layer.6.output.dense.weight: True\n",
      "model.encoder.layer.6.output.dense.bias: True\n",
      "model.encoder.layer.6.output.LayerNorm.weight: True\n",
      "model.encoder.layer.6.output.LayerNorm.bias: True\n",
      "model.encoder.layer.7.attention.self.query.weight: True\n",
      "model.encoder.layer.7.attention.self.query.bias: True\n",
      "model.encoder.layer.7.attention.self.key.weight: True\n",
      "model.encoder.layer.7.attention.self.key.bias: True\n",
      "model.encoder.layer.7.attention.self.value.weight: True\n",
      "model.encoder.layer.7.attention.self.value.bias: True\n",
      "model.encoder.layer.7.attention.output.dense.weight: True\n",
      "model.encoder.layer.7.attention.output.dense.bias: True\n",
      "model.encoder.layer.7.attention.output.LayerNorm.weight: True\n",
      "model.encoder.layer.7.attention.output.LayerNorm.bias: True\n",
      "model.encoder.layer.7.intermediate.dense.weight: True\n",
      "model.encoder.layer.7.intermediate.dense.bias: True\n",
      "model.encoder.layer.7.output.dense.weight: True\n",
      "model.encoder.layer.7.output.dense.bias: True\n",
      "model.encoder.layer.7.output.LayerNorm.weight: True\n",
      "model.encoder.layer.7.output.LayerNorm.bias: True\n",
      "model.encoder.layer.8.attention.self.query.weight: True\n",
      "model.encoder.layer.8.attention.self.query.bias: True\n",
      "model.encoder.layer.8.attention.self.key.weight: True\n",
      "model.encoder.layer.8.attention.self.key.bias: True\n",
      "model.encoder.layer.8.attention.self.value.weight: True\n",
      "model.encoder.layer.8.attention.self.value.bias: True\n",
      "model.encoder.layer.8.attention.output.dense.weight: True\n",
      "model.encoder.layer.8.attention.output.dense.bias: True\n",
      "model.encoder.layer.8.attention.output.LayerNorm.weight: True\n",
      "model.encoder.layer.8.attention.output.LayerNorm.bias: True\n",
      "model.encoder.layer.8.intermediate.dense.weight: True\n",
      "model.encoder.layer.8.intermediate.dense.bias: True\n",
      "model.encoder.layer.8.output.dense.weight: True\n",
      "model.encoder.layer.8.output.dense.bias: True\n",
      "model.encoder.layer.8.output.LayerNorm.weight: True\n",
      "model.encoder.layer.8.output.LayerNorm.bias: True\n",
      "model.encoder.layer.9.attention.self.query.weight: True\n",
      "model.encoder.layer.9.attention.self.query.bias: True\n",
      "model.encoder.layer.9.attention.self.key.weight: True\n",
      "model.encoder.layer.9.attention.self.key.bias: True\n",
      "model.encoder.layer.9.attention.self.value.weight: True\n",
      "model.encoder.layer.9.attention.self.value.bias: True\n",
      "model.encoder.layer.9.attention.output.dense.weight: True\n",
      "model.encoder.layer.9.attention.output.dense.bias: True\n",
      "model.encoder.layer.9.attention.output.LayerNorm.weight: True\n",
      "model.encoder.layer.9.attention.output.LayerNorm.bias: True\n",
      "model.encoder.layer.9.intermediate.dense.weight: True\n",
      "model.encoder.layer.9.intermediate.dense.bias: True\n",
      "model.encoder.layer.9.output.dense.weight: True\n",
      "model.encoder.layer.9.output.dense.bias: True\n",
      "model.encoder.layer.9.output.LayerNorm.weight: True\n",
      "model.encoder.layer.9.output.LayerNorm.bias: True\n",
      "model.encoder.layer.10.attention.self.query.weight: True\n",
      "model.encoder.layer.10.attention.self.query.bias: True\n",
      "model.encoder.layer.10.attention.self.key.weight: True\n",
      "model.encoder.layer.10.attention.self.key.bias: True\n",
      "model.encoder.layer.10.attention.self.value.weight: True\n",
      "model.encoder.layer.10.attention.self.value.bias: True\n",
      "model.encoder.layer.10.attention.output.dense.weight: True\n",
      "model.encoder.layer.10.attention.output.dense.bias: True\n",
      "model.encoder.layer.10.attention.output.LayerNorm.weight: True\n",
      "model.encoder.layer.10.attention.output.LayerNorm.bias: True\n",
      "model.encoder.layer.10.intermediate.dense.weight: True\n",
      "model.encoder.layer.10.intermediate.dense.bias: True\n",
      "model.encoder.layer.10.output.dense.weight: True\n",
      "model.encoder.layer.10.output.dense.bias: True\n",
      "model.encoder.layer.10.output.LayerNorm.weight: True\n",
      "model.encoder.layer.10.output.LayerNorm.bias: True\n",
      "model.encoder.layer.11.attention.self.query.weight: True\n",
      "model.encoder.layer.11.attention.self.query.bias: True\n",
      "model.encoder.layer.11.attention.self.key.weight: True\n",
      "model.encoder.layer.11.attention.self.key.bias: True\n",
      "model.encoder.layer.11.attention.self.value.weight: True\n",
      "model.encoder.layer.11.attention.self.value.bias: True\n",
      "model.encoder.layer.11.attention.output.dense.weight: True\n",
      "model.encoder.layer.11.attention.output.dense.bias: True\n",
      "model.encoder.layer.11.attention.output.LayerNorm.weight: True\n",
      "model.encoder.layer.11.attention.output.LayerNorm.bias: True\n",
      "model.encoder.layer.11.intermediate.dense.weight: True\n",
      "model.encoder.layer.11.intermediate.dense.bias: True\n",
      "model.encoder.layer.11.output.dense.weight: True\n",
      "model.encoder.layer.11.output.dense.bias: True\n",
      "model.encoder.layer.11.output.LayerNorm.weight: True\n",
      "model.encoder.layer.11.output.LayerNorm.bias: True\n",
      "model.pooler.dense.weight: True\n",
      "model.pooler.dense.bias: True\n",
      "projection_head.0.weight: True\n",
      "projection_head.0.bias: True\n",
      "projection_head.2.weight: True\n",
      "projection_head.2.bias: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_test.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sanity check output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [\"Hi my name is Rita.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the input\n",
    "old_model = AutoModel.from_pretrained(model_paths[0])\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_paths[0])\n",
    "inputs = tokenizer(\n",
    "    test_text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=512,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7632,  2026,  2171,  2003, 11620,  1012,   102]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (projection_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1286,  0.2925,  0.1487,  ..., -0.2231,  0.3022,  0.9385],\n",
       "         [ 0.6235,  0.2482,  0.2557,  ..., -0.0574,  0.8209,  0.3311],\n",
       "         [-0.2326, -0.2783, -0.4162,  ..., -0.0133,  0.0898,  0.2336],\n",
       "         ...,\n",
       "         [ 0.5914, -0.2341,  0.6581,  ...,  0.2008,  1.1242,  1.4513],\n",
       "         [ 0.3857, -0.1003, -0.3692,  ...,  0.5074, -0.0274, -0.4099],\n",
       "         [ 0.8128, -0.1049, -0.2351,  ...,  0.4146, -0.3801, -0.1596]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "old_model(**inputs)[0]  # == model_test.model(**inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1286,  0.2925,  0.1487,  ..., -0.2231,  0.3022,  0.9385],\n",
       "         [ 0.6235,  0.2482,  0.2557,  ..., -0.0574,  0.8209,  0.3311],\n",
       "         [-0.2326, -0.2783, -0.4162,  ..., -0.0133,  0.0898,  0.2336],\n",
       "         ...,\n",
       "         [ 0.5914, -0.2341,  0.6581,  ...,  0.2008,  1.1242,  1.4513],\n",
       "         [ 0.3857, -0.1003, -0.3692,  ...,  0.5074, -0.0274, -0.4099],\n",
       "         [ 0.8128, -0.1049, -0.2351,  ...,  0.4146, -0.3801, -0.1596]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test.model(**inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "model_test(**inputs)[1] == mean_pool(\n",
    "    old_model(**inputs)[0], inputs[\"attention_mask\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"BERT\",\n",
    "    # \"MPNet\",\n",
    "    \"SBERT\",\n",
    "    \"SciBERT\",\n",
    "    \"SPECTER\",\n",
    "    \"SciNCL\",\n",
    "    \"SimCSE\",\n",
    "    \"DeCLUTR\",\n",
    "    \"DeCLUTR-sci\",\n",
    "    # \"SPECTER2\",\n",
    "]\n",
    "\n",
    "\n",
    "model_paths = [\n",
    "    \"bert-base-uncased\",\n",
    "    # \"microsoft/mpnet-base\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    \"allenai/specter\",\n",
    "    \"malteos/scincl\",\n",
    "    \"princeton-nlp/unsup-simcse-bert-base-uncased\",\n",
    "    \"johngiorgi/declutr-base\",\n",
    "    \"johngiorgi/declutr-sci-base\",\n",
    "    # \"allenai/specter2_base\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model (+ projection head):  BERT\n",
      "Running on device: cuda\n",
      "bert-base-uncased\n",
      "Training loader length:  368\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a0c870f5324bd19b61e4334c6a2722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "for i, model_name in enumerate(model_names):\n",
    "    ## fix random seeds\n",
    "    seed = 42\n",
    "    # Set the random seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "    # Set the random seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "    # Set the random seed\n",
    "    random.seed(seed)\n",
    "\n",
    "    # set up model\n",
    "    print(\"Model (+ projection head): \", model_name)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Running on device: {}\".format(device))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_paths[i])\n",
    "    model = CustomModel(model_paths[i])\n",
    "    print(model_paths[i])\n",
    "\n",
    "    # data\n",
    "    training_dataset = MultOverlappingSentencesPairDataset(\n",
    "        iclr2024.abstract, tokenizer, device, n_cons_sntcs=2, seed=42\n",
    "    )\n",
    "\n",
    "    gen = torch.Generator()\n",
    "    gen.manual_seed(seed)\n",
    "    training_loader = torch.utils.data.DataLoader(\n",
    "        training_dataset, batch_size=64, shuffle=True, generator=gen\n",
    "    )\n",
    "    print(\"Training loader length: \", len(training_loader))\n",
    "\n",
    "    losses, knn_accuracies = train_loop_with_projection_head(\n",
    "        model,\n",
    "        training_loader,\n",
    "        device,\n",
    "        iclr2024.abstract.to_list(),\n",
    "        tokenizer,\n",
    "        (labels_iclr != \"unlabeled\"),\n",
    "        labels_acc=labels_iclr[labels_iclr != \"unlabeled\"],\n",
    "        n_epochs=2,\n",
    "        lr=2e-5,\n",
    "    )\n",
    "\n",
    "    # save\n",
    "    saving_path = Path(\"embeddings_\" + model_name.lower()) / Path(\n",
    "        \"updated_dataset\"\n",
    "    )\n",
    "    (variables_path / saving_path).mkdir(exist_ok=True)\n",
    "\n",
    "    np.save(\n",
    "        variables_path / saving_path / \"losses_train_with_projector_head_run7\",\n",
    "        losses,\n",
    "    )\n",
    "    np.save(\n",
    "        variables_path\n",
    "        / saving_path\n",
    "        / \"knn_accuracies_train_with_projector_head_run7\",\n",
    "        knn_accuracies,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    # \"BERT\",\n",
    "    # # \"MPNet\",\n",
    "    # \"SBERT\",\n",
    "    # \"SciBERT\",\n",
    "    # \"SPECTER\",\n",
    "    # \"SciNCL\",\n",
    "    # \"SimCSE\",\n",
    "    # \"DeCLUTR\",\n",
    "    # \"DeCLUTR-sci\",\n",
    "    \"SPECTER2\",\n",
    "]\n",
    "\n",
    "\n",
    "model_paths = [\n",
    "    # \"bert-base-uncased\",\n",
    "    # # \"microsoft/mpnet-base\",\n",
    "    # \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    # \"allenai/scibert_scivocab_uncased\",\n",
    "    # \"allenai/specter\",\n",
    "    # \"malteos/scincl\",\n",
    "    # \"princeton-nlp/unsup-simcse-bert-base-uncased\",\n",
    "    # \"johngiorgi/declutr-base\",\n",
    "    # \"johngiorgi/declutr-sci-base\",\n",
    "    \"allenai/specter2_base\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model (+ projection head):  SPECTER2\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c55dcc112a43d7a8b580e4a7936116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eae4463c2884e418cca6014fececb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allenai/specter2_base\n",
      "Training loader length:  368\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a9b3e1074a49f9bb3aa2e7a9d844e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d371ecd7c448269156ce9227f759bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a6f01f7038468ab3b9875f0515da29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b222f846dc489bb87619adeea7aabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 51s, sys: 13min 42s, total: 22min 34s\n",
      "Wall time: 17min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, model_name in enumerate(model_names):\n",
    "    ## fix random seeds\n",
    "    seed = 42\n",
    "    # Set the random seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "    # Set the random seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "    # Set the random seed\n",
    "    random.seed(seed)\n",
    "\n",
    "    # set up model\n",
    "    print(\"Model (+ projection head): \", model_name)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Running on device: {}\".format(device))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_paths[i])\n",
    "    model = CustomModel(model_paths[i])\n",
    "    print(model_paths[i])\n",
    "\n",
    "    # data\n",
    "    training_dataset = MultOverlappingSentencesPairDataset(\n",
    "        iclr2024.abstract, tokenizer, device, n_cons_sntcs=2, seed=42\n",
    "    )\n",
    "\n",
    "    gen = torch.Generator()\n",
    "    gen.manual_seed(seed)\n",
    "    training_loader = torch.utils.data.DataLoader(\n",
    "        training_dataset, batch_size=64, shuffle=True, generator=gen\n",
    "    )\n",
    "    print(\"Training loader length: \", len(training_loader))\n",
    "\n",
    "    losses, knn_accuracies = train_loop_with_projection_head(\n",
    "        model,\n",
    "        training_loader,\n",
    "        device,\n",
    "        iclr2024.abstract.to_list(),\n",
    "        tokenizer,\n",
    "        (labels_iclr != \"unlabeled\"),\n",
    "        labels_acc=labels_iclr[labels_iclr != \"unlabeled\"],\n",
    "        n_epochs=2,\n",
    "        lr=2e-5,\n",
    "    )\n",
    "\n",
    "    # save\n",
    "    saving_path = Path(\"embeddings_\" + model_name.lower()) / Path(\n",
    "        \"updated_dataset\"\n",
    "    )\n",
    "    (variables_path / saving_path).mkdir(exist_ok=True)\n",
    "\n",
    "    np.save(\n",
    "        variables_path / saving_path / \"losses_train_with_projector_head_run7\",\n",
    "        losses,\n",
    "    )\n",
    "    np.save(\n",
    "        variables_path\n",
    "        / saving_path\n",
    "        / \"knn_accuracies_train_with_projector_head_run7\",\n",
    "        knn_accuracies,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model (+ projection head):  MPNet\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/mpnet-base\n",
      "Training loader length:  368\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd97ee2bef74337ad6ecf242e69c338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0347cba5295a4086859978660b1c83d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47c40b3f5084e8fa23715e3cc6e7f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92648e4c50bf4e50baf88ebd2a479b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 3s, sys: 14min 33s, total: 30min 37s\n",
      "Wall time: 17min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# WITH LR=2e-4\n",
    "\n",
    "i = 1\n",
    "model_name = model_names[i]\n",
    "\n",
    "## fix random seeds\n",
    "seed = 42\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed)\n",
    "# Set the random seed\n",
    "random.seed(seed)\n",
    "\n",
    "# set up model\n",
    "print(\"Model (+ projection head): \", model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on device: {}\".format(device))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_paths[i])\n",
    "model = CustomModel(model_paths[i])\n",
    "print(model_paths[i])\n",
    "\n",
    "# data\n",
    "training_dataset = MultOverlappingSentencesPairDataset(\n",
    "    iclr2024.abstract, tokenizer, device, n_cons_sntcs=2, seed=42\n",
    ")\n",
    "\n",
    "gen = torch.Generator()\n",
    "gen.manual_seed(seed)\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    training_dataset, batch_size=64, shuffle=True, generator=gen\n",
    ")\n",
    "print(\"Training loader length: \", len(training_loader))\n",
    "\n",
    "losses, knn_accuracies = train_loop_with_projection_head(\n",
    "    model,\n",
    "    training_loader,\n",
    "    device,\n",
    "    iclr2024.abstract.to_list(),\n",
    "    tokenizer,\n",
    "    (labels_iclr != \"unlabeled\"),\n",
    "    labels_acc=labels_iclr[labels_iclr != \"unlabeled\"],\n",
    "    n_epochs=2,\n",
    "    lr=2e-5,\n",
    ")\n",
    "\n",
    "# save\n",
    "saving_path = Path(\"embeddings_\" + model_name.lower()) / Path(\n",
    "    \"updated_dataset\"\n",
    ")\n",
    "(variables_path / saving_path).mkdir(exist_ok=True)\n",
    "\n",
    "np.save(\n",
    "    variables_path / saving_path / \"losses_train_with_projector_head_run7_v1\",\n",
    "    losses,\n",
    ")\n",
    "np.save(\n",
    "    variables_path\n",
    "    / saving_path\n",
    "    / \"knn_accuracies_train_with_projector_head_run7_v1\",\n",
    "    knn_accuracies,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"BERT\",\n",
    "    \"MPNet\",\n",
    "    \"SBERT\",\n",
    "    \"SciBERT\",\n",
    "    \"SPECTER\",\n",
    "    \"SciNCL\",\n",
    "    \"SimCSE\",\n",
    "    \"DeCLUTR\",\n",
    "    \"DeCLUTR-sci\",\n",
    "    # \"SPECTER2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(6, 4), dpi=200)\n",
    "n_epochs = 2\n",
    "colormap = plt.get_cmap(\"tab10\")\n",
    "\n",
    "# for i, model_name in enumerate(model_names):\n",
    "#     # load things\n",
    "saving_path = Path(\"embeddings_\" + model_name.lower()) / Path(\n",
    "    \"updated_dataset\"\n",
    ")\n",
    "\n",
    "losses = np.load(\n",
    "    variables_path / saving_path / \"losses_train_with_projector_head_run7.npy\"\n",
    ")\n",
    "knn_accuracies = np.load(\n",
    "    variables_path\n",
    "    / saving_path\n",
    "    / \"knn_accuracies__train_with_projector_head_run7.npy\"\n",
    ")\n",
    "\n",
    "saving_name_2 = Path(\"knn_accuracy_\" + model_name.lower() + \".npy\")\n",
    "knn_acc_highd = np.load(variables_path / \"updated_dataset\" / saving_name_2)\n",
    "\n",
    "color = colormap(i)\n",
    "\n",
    "axs[0, 0].plot(\n",
    "    np.arange(n_epochs),\n",
    "    np.mean(losses, axis=1),\n",
    "    label=model_name,\n",
    "    color=color,\n",
    ")\n",
    "axs[0, 0].set_xticks(np.arange(n_epochs))\n",
    "axs[0, 0].set_xlabel(\"Epochs\")\n",
    "axs[0, 0].set_ylabel(\"Loss\")\n",
    "axs[0, 0].legend()\n",
    "\n",
    "axs[0, 1].plot(\n",
    "    np.arange(n_epochs),\n",
    "    knn_accuracies[:, 0],\n",
    "    label=f\"({knn_accuracies[0, 0]:.3f}, {knn_accuracies[-1, 0]:.3f})\",\n",
    "    color=color,\n",
    ")\n",
    "axs[0, 1].scatter(-0.4, knn_acc_highd[0], c=color, s=5)\n",
    "axs[0, 1].text(\n",
    "    -0.3,\n",
    "    knn_acc_highd[0],\n",
    "    f\"{knn_acc_highd[0]:.3f}\",\n",
    "    fontsize=5,\n",
    "    va=\"center\",\n",
    "    ha=\"left\",\n",
    ")\n",
    "axs[0, 1].set_xticks(np.arange(n_epochs))\n",
    "axs[0, 1].set_ylim(0.2, 0.65)\n",
    "axs[0, 1].set_xlabel(\"Epochs\")\n",
    "axs[0, 1].set_ylabel(\"kNN accuracy [AV]\")\n",
    "axs[0, 1].legend(loc=\"lower right\")\n",
    "\n",
    "axs[1, 0].plot(\n",
    "    np.arange(n_epochs),\n",
    "    knn_accuracies[:, 1],\n",
    "    label=f\"({knn_accuracies[0, 1]:.3f}, {knn_accuracies[-1, 1]:.3f})\",\n",
    "    color=color,\n",
    ")\n",
    "axs[1, 0].scatter(-0.4, knn_acc_highd[1], c=color, s=5)\n",
    "axs[1, 0].set_xticks(np.arange(n_epochs))\n",
    "axs[1, 0].set_ylim(0.2, 0.65)\n",
    "axs[1, 0].set_xlabel(\"Epochs\")\n",
    "axs[1, 0].set_ylabel(\"kNN accuracy [CLS]\")\n",
    "axs[1, 0].legend()\n",
    "\n",
    "axs[1, 1].plot(\n",
    "    np.arange(n_epochs),\n",
    "    knn_accuracies[:, 2],\n",
    "    label=model_name,\n",
    "    color=color,\n",
    ")\n",
    "axs[1, 1].scatter(-0.4, knn_acc_highd[2], c=color, s=5)\n",
    "axs[1, 1].set_xticks(np.arange(n_epochs))\n",
    "axs[1, 1].set_ylim(0.2, 0.65)\n",
    "axs[1, 1].set_xlabel(\"Epochs\")\n",
    "axs[1, 1].set_ylabel(\"kNN accuracy [SEP]\")\n",
    "\n",
    "fig.savefig(figures_path / \"loss_and_knn_accuracy_training_run7_v1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fancy plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"BERT\",\n",
    "    \"MPNet\",\n",
    "    \"SBERT\",\n",
    "    \"SciBERT\",\n",
    "    \"SPECTER\",\n",
    "    \"SciNCL\",\n",
    "    \"SimCSE\",\n",
    "    \"DeCLUTR\",\n",
    "    \"DeCLUTR-sci\",\n",
    "    \"SPECTER2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = plt.get_cmap(\"tab10\")\n",
    "dict_original_colors = dict()\n",
    "for i, model_name in enumerate(model_names):\n",
    "    color = colormap(i)\n",
    "    dict_original_colors[model_name] = colormap(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new order\n",
    "model_names = [\n",
    "    \"MPNet\",\n",
    "    \"BERT\",\n",
    "    \"SciBERT\",\n",
    "    \"SimCSE\",\n",
    "    \"DeCLUTR\",\n",
    "    \"DeCLUTR-sci\",\n",
    "    \"SciNCL\",\n",
    "    \"SPECTER\",\n",
    "    \"SPECTER2\",\n",
    "    \"SBERT\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2, 3), dpi=200)\n",
    "\n",
    "rep = 0  # av\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    # load things\n",
    "    saving_path = Path(\"embeddings_\" + model_name.lower()) / Path(\n",
    "        \"updated_dataset\"\n",
    "    )\n",
    "\n",
    "    losses = np.load(variables_path / saving_path / \"losses_run1.npy\")\n",
    "    knn_accuracies_after = np.load(\n",
    "        variables_path / saving_path / \"knn_accuracies_run1.npy\"\n",
    "    )\n",
    "\n",
    "    saving_name_2 = Path(\"knn_accuracy_\" + model_name.lower() + \".npy\")\n",
    "    knn_accuracies_before = np.load(\n",
    "        variables_path / \"updated_dataset\" / saving_name_2\n",
    "    )\n",
    "    # NEW ACCURACIES WITH PROJECTION HEAD\n",
    "    # if model_name != \"SPECTER2\":\n",
    "    knn_accuracies_run7 = np.load(\n",
    "        variables_path\n",
    "        / saving_path\n",
    "        / \"knn_accuracies_train_with_projector_head_run7.npy\"\n",
    "    )\n",
    "    # knn_accuracies_run7_v2 = np.load(\n",
    "    #     variables_path\n",
    "    #     / saving_path\n",
    "    #     / \"knn_accuracies_train_with_projector_head_run7_v2.npy\"\n",
    "    # )\n",
    "\n",
    "    color = dict_original_colors[model_name]\n",
    "\n",
    "    ax.plot(\n",
    "        np.ones(2) * i,\n",
    "        np.array([knn_accuracies_before[rep], knn_accuracies_after[0, rep]]),\n",
    "        label=f\"{model_name}\",\n",
    "        color=color,\n",
    "        # marker=\".\",\n",
    "        # ms=3,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        i,\n",
    "        knn_accuracies_before[rep],\n",
    "        color=color,\n",
    "        marker=\".\",\n",
    "        s=3,\n",
    "    )\n",
    "    if knn_accuracies_before[rep] <= knn_accuracies_after[0, rep]:\n",
    "        ax.scatter(\n",
    "            i,\n",
    "            knn_accuracies_after[0, rep],\n",
    "            color=color,\n",
    "            marker=\"^\",\n",
    "            s=3,\n",
    "        )\n",
    "    else:\n",
    "        ax.scatter(\n",
    "            i,\n",
    "            knn_accuracies_after[0, rep],\n",
    "            color=color,\n",
    "            marker=\"v\",\n",
    "            s=3,\n",
    "        )\n",
    "    # NEW ACCURACIES WITH PROJECTION HEAD\n",
    "    # if model_name != \"SPECTER2\":\n",
    "    ax.scatter(\n",
    "        i,\n",
    "        knn_accuracies_run7[0, rep],\n",
    "        color=color,\n",
    "        marker=\"x\",\n",
    "        s=3,\n",
    "    )\n",
    "    # ax.scatter(\n",
    "    #     i,\n",
    "    #     knn_accuracies_run7_v2[0, rep],\n",
    "    #     color=color,\n",
    "    #     marker=\"*\",\n",
    "    #     s=3,\n",
    "    # )\n",
    "\n",
    "    ax.set_xticks(np.arange(len(model_names)))\n",
    "    ax.set_xticklabels(model_names, rotation=\"vertical\")\n",
    "    ax.set_ylim(0.2, 0.65)\n",
    "    ax.set_ylabel(\"kNN accuracy [AV]\")\n",
    "    # ax.legend(loc=\"lower right\")\n",
    "    ax.set_title(\"[AV]\")\n",
    "    ax.annotate(\n",
    "        \"Base\",\n",
    "        xy=(1, 0.35),\n",
    "        xytext=(1, 0.32),\n",
    "        xycoords=\"data\",\n",
    "        fontsize=6,\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        bbox=dict(boxstyle=\"square\", fc=\"white\", edgecolor=\"None\"),\n",
    "        arrowprops=dict(\n",
    "            arrowstyle=\"-[, widthB=2.2, lengthB=0.25\", lw=0.35, color=\"k\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ax.annotate(\n",
    "        \"Unsupervised\",\n",
    "        xy=(4, 0.35),\n",
    "        xytext=(4, 0.32),\n",
    "        xycoords=\"data\",\n",
    "        fontsize=6,\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        bbox=dict(boxstyle=\"square\", fc=\"white\", edgecolor=\"None\"),\n",
    "        arrowprops=dict(\n",
    "            arrowstyle=\"-[, widthB=2.2, lengthB=0.25\", lw=0.35, color=\"k\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ax.annotate(\n",
    "        \"Supervised\",\n",
    "        xy=(7.5, 0.35),\n",
    "        xytext=(7.5, 0.32),\n",
    "        xycoords=\"data\",\n",
    "        fontsize=6,\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        bbox=dict(boxstyle=\"square\", fc=\"white\", edgecolor=\"None\"),\n",
    "        arrowprops=dict(\n",
    "            arrowstyle=\"-[, widthB=3.2, lengthB=0.25\", lw=0.35, color=\"k\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "fig.savefig(\n",
    "    figures_path / \"loss_and_knn_accuracy_training_run1_and_run7_v3.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
